{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "854d2dbf-3c89-46b7-a24c-5173ecfc442b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Semesterarbeit Teil2: Textverarbeitung mit Python\n",
    "Michael Bieri\n",
    "\n",
    "Informationen:\n",
    "https://de.wikipedia.org/wiki/Tf-idf-Ma%C3%9F#Vorkommensh%C3%A4ufigkeit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4782642",
   "metadata": {},
   "source": [
    "### 1. Funktion tf_read\n",
    "Funktion soll eine Datei einlesen von einem Pfad und ein Dictionary erstellen. In dem die Keys die Woerter sind die im Dokument vorkommen. Die Values sind die Haeufigkeit wie oft es vorkommt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e4a1a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dicitionary with keys and additional values:\n",
      "Counter({'der': 188, 'die': 165, 'in': 99, 'von': 68, 'ein': 68, 'werden': 67, 'ist': 63, 'und': 60, 'eine': 53, 'des': 49, 'fr': 46, 'convolutional': 45, 'den': 42, 'autoencoder': 40, 'das': 40, 'neural': 38, 'zu': 38, 'sich': 36, 'wird': 36, 'auf': 35, 'als': 34, '1': 33, 'networks': 32, 'im': 32, '3': 31, '0': 31, 'mit': 29, 'abbildung': 28, 'layer': 28, '10': 24, 'elias': 23, 'saalmann': 23, 'zur': 23, 'eines': 22, 'an': 22, 'sind': 21, 'aus': 21, 'einem': 21, 'zb': 20, 'gewichte': 20, 'kann': 19, 'auch': 19, 'oder': 18, 'netz': 18, 'einer': 17, 'ber': 17, '2': 16, 'ergebnis': 16, 'diese': 16, 'bzw': 16, 'of': 16, 'netze': 15, 'einen': 15, 'daten': 15, 'neuronen': 15, 'dass': 15, 'beispiel': 15, 'and': 15, 'neuronale': 14, 'nicht': 14, 'oensichtlich': 14, 'schicht': 14, 'bei': 14, 'denition': 13, 'lernen': 13, 'dieser': 13, 'mittels': 13, 'schichten': 13, 'convolution': 13, 'dann': 12, 'dabei': 12, 'es': 12, 'a': 12, 'eingabe': 12, 'neuronalen': 12, 'um': 12, 'wurde': 12, '30': 12, 'learning': 11, 'l15b': 11, 'funktion': 11, 'dem': 11, 'dies': 11, 'ia': 11, 'the': 11, 'deep': 10, 'lokal': 10, 'bereits': 10, 'parameter': 10, 'knnen': 10, 'zeigt': 10, 'netzen': 10, 'mssen': 10, '101': 10, 'zwei': 9, 'eingabedaten': 9, 'hat': 9, 'berechnet': 9, 'funktionen': 9, 'alle': 9, 'trainiert': 9, 'unter': 9, 'anwendung': 9, 'da': 9, 'anzahl': 9, 'pca': 9, 'cnn': 9, 'faltung': 9, 'verarbeitung': 8, '21': 8, 'wie': 8, 'jedes': 8, 'nun': 8, 'verfahren': 8, 'nur': 8, 'knstlichen': 8, 'hidden': 8, 'also': 8, 'sehr': 8, 'solche': 8, 'so': 8, 'pooling': 8, 'dieses': 7, '22': 7, 'anwendungen': 7, 'durch': 7, 'training': 7, 'dazu': 7, 'zunchst': 7, 'bereich': 7, 'eingaberaum': 7, 'elemente': 7, 'eingaberaums': 7, 'eintrge': 7, 'man': 7, 'keine': 7, 'neuronales': 7, 'sein': 7, 'genannt': 7, 'vollstndig': 7, 'nachfolgend': 7, 'verwendung': 7, 'diesem': 7, 'kodierung': 7, 'nach': 7, 'autoencoders': 7, 'soll': 7, 'gelernt': 7, 'verwendet': 7, '9': 7, 'bildern': 7, 'f': 7, 'on': 7, 'ohne': 6, 'verbundene': 6, 'convolutionoperator': 6, 'arbeit': 6, 'eingabedatum': 6, 'welche': 6, 'vor': 6, 'wahl': 6, 'lineare': 6, 'einfache': 6, 'entspricht': 6, 'besteht': 6, 'art': 6, 'engl': 6, 'nen': 6, 'schichtweise': 6, 'abschnitt': 6, 'genau': 6, 'haben': 6, 'ergibt': 6, 'dimension': 6, 'muss': 6, 'layers': 6, 'datum': 6, 'dimensionsreduzierung': 6, 'jeweils': 6, 'weiterhin': 6, 'bild': 6, 'filtermatrix': 6, 'einfhrung': 5, 'insbesondere': 5, 'einleitung': 5, 'knstliche': 5, '23': 5, '32': 5, '11': 5, '33': 5, 'aufbau': 5, 'deren': 5, 'knn': 5, 'lernens': 5, 'lsst': 5, 'beim': 5, 'liegt': 5, 'wobei': 5, 'werte': 5, 'regression': 5, 'ie': 5, 'ausgabe': 5, 'trainingsdatensatz': 5, 'lsung': 5, 'unberwachten': 5, 'ergebnisse': 5, 'neben': 5, 'verbunden': 5, 'neuron': 5, 'grundlage': 5, 'solcher': 5, 'betrachte': 5, 'folgende': 5, 'konstruktion': 5, 'bezeichnet': 5, 'ausgabeneuronen': 5, 'welches': 5, 'folgenden': 5, 'backpropagation': 5, 'berechnung': 5, 'sei': 5, 'backpropagationalgorithmus': 5, 'initialisierung': 5, 'niedrigdimensionalen': 5, 'kompression': 5, 'eingefhrt': 5, 'deniert': 5, 'lassen': 5, 'ab': 5, 'vorangegangenen': 5, 'hs06': 5, 'wurden': 5, '2dimensionalen': 5, 'deutlich': 5, 'angenommen': 5, 'server': 5, 'cnns': 5, 'verbindungen': 5, 'eingabematrix': 5, 'bildverarbeitung': 5, 'architektur': 5, 'pages': 5, 'sowie': 4, '4': 4, '6': 4, '31': 4, '34': 4, '35': 4, 'mehr': 4, 'spezielle': 4, 'knstlicher': 4, 'erlutert': 4, 'neuronaler': 4, 'berwachten': 4, 'bestimmten': 4, 'mglichst': 4, 'konkrete': 4, 'fehler': 4, 'liefert': 4, 'gradienten': 4, 'form': 4, 'allerdings': 4, 'verarbeitet': 4, 'gestalt': 4, 'fest': 4, 'b': 4, 'dienen': 4, 'mathematische': 4, 'aufgrund': 4, 'dargestellt': 4, 'netzes': 4, 'ausgabeschicht': 4, 'verbundenen': 4, 'einfaches': 4, 'basis': 4, 'gem': 4, 'sollen': 4, 'autoencodern': 4, 'interesse': 4, 'hochdimensionalen': 4, 'datensatz': 4, 'reduzierung': 4, 'kapitel': 4, 'niedrigdimensionale': 4, 'berechnen': 4, 'entsprechen': 4, 'mglich': 4, 'anschlieend': 4, 'verbundenes': 4, 'trainingsdatensatzes': 4, 'ursprnglichen': 4, 'veranschaulichung': 4, 'zeile': 4, 'trainings': 4, 'phase': 4, 'bildet': 4, 'af': 4, 'ergebnismatrix': 4, 'filter': 4, 'network': 4, 'poolinglayer': 4, 'lecun': 4, 'et': 4, 'al': 4, 'for': 4, 'informatik': 3, '5': 3, 'erweiterungen': 3, 'zusammenfassung': 3, '20': 3, 'heutzutage': 3, 'damit': 3, 'letzten': 3, 'maschinellen': 3, 'besprochen': 3, 'l15a': 3, 'sogenannte': 3, 'merkmalen': 3, 'chen': 3, 'klassen': 3, 'klassikation': 3, 'dar': 3, 'raum': 3, 'gut': 3, 'zumeist': 3, 'fehlerfunktion': 3, 'betrachtet': 3, 'j': 3, 'praxis': 3, 'hug': 3, 'dh': 3, 'nden': 3, 'hx': 3, 'be': 3, 'zustzlich': 3, 'sogenannten': 3, 'entsprechend': 3, 'aktivierungsfunktion': 3, 'hintereinanderausfhrung': 3, 'mathematisch': 3, 'c': 3, 'hier': 3, 'enthlt': 3, 'aller': 3, 'zum': 3, 'vorwrtsdurchlauf': 3, 'fehlers': 3, 'berechneten': 3, 'w': 3, 'ter': 3, 'stellt': 3, 'vielversprechend': 3, 'erwiesen': 3, 'zufllige': 3, 'einige': 3, 'visualisierung': 3, 'seien': 3, 'zudem': 3, 'wir': 3, '2b2': 3, 'solches': 3, 'gewhlt': 3, 'hinsichtlich': 3, 'mehrerer': 3, 'feedforwardnetz': 3, 'sondern': 3, 'codierung': 3, 'angelegt': 3, 'jedoch': 3, 'knnte': 3, 'schrittweise': 3, 'erreicht': 3, 'analog': 3, 'verdeutlicht': 3, 'verschiedene': 3, 'handgeschriebenen': 3, 'vektor': 3, 'jeder': 3, 'erkennen': 3, 'gesichter': 3, 'weitere': 3, 'beispielsweise': 3, 'gelernten': 3, 'am': 3, 'erfllen': 3, 'hoher': 3, 'unten': 3, 'solchen': 3, 'gewichten': 3, 'angepasst': 3, 'erhhen': 3, 'signalverarbeitung': 3, 'her05': 3, '1dimensionalen': 3, 'fall': 3, 'filtervektor': 3, '40': 3, 'n': 3, 'convolutionoperators': 3, '601': 3, '201': 3, 'andere': 3, '01': 3, '100': 3, 'gre': 3, 'strided': 3, 'folgt': 3, 'nebenbedingung': 3, 'translationsinvarianz': 3, 'verschiebung': 3, 'whrend': 3, 'lbbh98': 3, 'unsupervised': 3, 'bengio': 3, '2010': 3, 'with': 3, '2015': 3, 'ieee': 3, 'seiner': 2, 'teile': 2, 'jede': 2, 'grenzen': 2, 'gilt': 2, 'literaturverzeichnis': 2, 'maschinelles': 2, 'intelligenz': 2, 'formal': 2, 'ren': 2, 'fokus': 2, 'wirtschaft': 2, 'wissenschaft': 2, 'neuro': 2, 'beruht': 2, 'grundlegend': 2, 'problem': 2, 'bestehend': 2, 'featurevektoren': 2, 'menge': 2, 'vorhandenes': 2, 'wissen': 2, 'wert': 2, 'bestimmtes': 2, 'davon': 2, 'h': 2, 'dierentialrechnung': 2, 'liegen': 2, 'sen': 2, 'innerhalb': 2, 'struktur': 2, 'de': 2, 'lernenden': 2, 'bietet': 2, 'linearen': 2, 'logistische': 2, 'ausdruck': 2, 'entweder': 2, 'liefern': 2, 'weiteren': 2, 'miteinander': 2, 'eingehenden': 2, 'gewichteten': 2, 'hnlich': 2, 'summen': 2, 'sol': 2, 'bersichtlichkeit': 2, '373190221': 2, 'eingabeneuronen': 2, 'eingabeschicht': 2, 'knstliches': 2, 'feedforward': 2, 'darauf': 2, 'mathematischen': 2, 'feedforwardnetzes': 2, 'ersichtlich': 2, 'diejenigen': 2, 'entsprechender': 2, 'algorithmus': 2, 'grob': 2, 'basiert': 2, 'wiederum': 2, 'drei': 2, 'widie': 2, 'gradientenj': 2, 'abschlieend': 2, 'vom': 2, 'rahmen': 2, 'stelle': 2, 'initialisiert': 2, 'wenig': 2, 'wenige': 2, 'merkmale': 2, 'kontext': 2, 'gesucht': 2, 'zwischenraum': 2, 'bedeutet': 2, 'beliebiges': 2, 'element': 2, 'optimalerweise': 2, 'je': 2, 'fundg': 2, 'immer': 2, 'bal12': 2, 'kleiner': 2, 'encoder': 2, 'beginn': 2, 'decoderfunktion': 2, 'codierten': 2, 'enthalten': 2, 'betrachten': 2, 'annahme': 2, 'ersten': 2, 'beiden': 2, 'w1b1w': 2, 'gau77': 2, 'msummationdisplay': 2, 'mp43': 2, 'bentigt': 2, 'deingabe': 2, 'dargestellten': 2, 'unterschiede': 2, 'erfolgen': 2, 'gelegt': 2, 'klassiziert': 2, 'werten': 2, 'selbstverstndlich': 2, 'sodass': 2, 'rung': 2, 'weit': 2, 'pea01': 2, 'hochdimensionale': 2, 'einbettung': 2, '2dimensionale': 2, 'mnist': 2, 'ziern': 2, 'bis': 2, 'lcb10': 2, 'graustufenbild': 2, 'transformiert': 2, 'verschiedenen': 2, 'eindeutige': 2, 'daher': 2, 'qualitt': 2, 'zugrunde': 2, 'erkennung': 2, 'entwickelt': 2, 'rekonstruktionen': 2, 'sk87': 2, 'mittleren': 2, 'bilder': 2, 'database': 2, 'faces': 2, 'att': 2, 'sh94': 2, 'rekonstruktion': 2, 'bessere': 2, 'variante': 2, 'gleich': 2, 'bleibt': 2, 'komprimierung': 2, 'erreichen': 2, 'x1x2x': 2, 'trac': 2, 'sollte': 2, 'entsprechende': 2, 'hinreichend': 2, 'client': 2, 'vorab': 2, 'ihren': 2, 'zweck': 2, 'voll': 2, 'rohdaten': 2, 'tung': 2, 'ebc10': 2, 'kon': 2, '24': 2, 'oberen': 2, 'kleines': 2, 'dessen': 2, 'gewichtsmatrizen': 2, 'konstruiert': 2, 'abb': 2, 'grn': 2, 'umrahmt': 2, 'unberwacht': 2, 'zweiten': 2, 'rot': 2, 'initialen': 2, 'nalen': 2, 'durchgefhrt': 2, 'initialisierungsmethode': 2, 'er': 2, 'nachdem': 2, 'populre': 2, 'kanten': 2, 'habe': 2, 'million': 2, 'erste': 2, 'nachfolgende': 2, 'dimensionen': 2, 'overtting': 2, 'lernender': 2, 'operator': 2, '1dconvolution': 2, 'diskrete': 2, 'seia': 2, 'eingabevektor': 2, '1dimensionale': 2, 'wiefolgt': 2, '60': 2, 'undf': 2, '501': 2, '401': 2, 'stets': 2, 'glttung': 2, 'oben': 2, 'informationen': 2, 'filters': 2, 'i1ksummationdisplay': 2, 'insgesamt': 2, 'ausfhrlich': 2, 'daraus': 2, 'operation': 2, 'kantenerkennung': 2, 'weiterer': 2, 'erzielt': 2, 'gewnscht': 2, 'rand': 2, 'anforderung': 2, 'schrittweite': 2, 'jedem': 2, 'schritt': 2, 'vollstndigverbundener': 2, 'sowohl': 2, 'groen': 2, 'geringer': 2, 'eintrag': 2, 'ob': 2, 'w1': 2, 'ergebnisses': 2, 'gleichheit': 2, 'gekennzeichnet': 2, 'gespeichert': 2, 'gengt': 2, 'gleiche': 2, 'durchschnitt': 2, 'gebildet': 2, 'letzte': 2, 'maximums': 2, 'poolinglayers': 2, 'smb10': 2, 'positionen': 2, 'maps': 2, 'konnten': 2, 'lenet5': 2, '37': 2, 'objekterkennung': 2, '38': 2, 'ksh12': 2, 'processing': 2, 'kim14': 2, 'praktische': 2, 'google': 2, 'implementierung': 2, 'sp15': 2, 'shm16': 2, 'architectures': 2, 'workshop': 2, '2012': 2, 'yoshua': 2, 'digitale': 2, 'georey': 2, 'e': 2, 'hinton': 2, 'classication': 2, 'systems': 2, 'quoc': 2, 'v': 2, 'le': 2, 'tutorial': 2, 'part': 2, 'yann': 2, 'to': 2, 'recognition': 2, 'proceedings': 2, 's': 2, '1994': 2, 'international': 2, 'conference': 2, 'universitt': 1, 'leipzig': 1, 'institut': 1, 'fakultt': 1, 'mathematik': 1, 'abteilung': 1, 'datenbanken': 1, 'seminararbeit': 1, 'seminar': 1, 'vorgelegt': 1, 'matrikelnummer': 1, '3731902': 1, 'betreuer': 1, 'victor': 1, 'christen': 1, '2018': 1, 'werk': 1, 'einschlielich': 1, 'urheberrechtlich': 1, 'geschtzt': 1, 'verwertung': 1, 'auerhalb': 1, 'engen': 1, 'urheberrechtgesetzes': 1, 'zustimmung': 1, 'autors': 1, 'unzulssig': 1, 'strafbar': 1, 'vervielfltigungen': 1, 'bersetzungen': 1, 'mikroverlmungen': 1, 'einspeicherung': 1, 'elektronischen': 1, 'systemeninhaltsverzeichnis': 1, 'inhaltsverzeichnis': 1, '13': 1, '17': 1, '18': 1, '19': 1, '3731902i1': 1, 'stichworte': 1, 'oderknstliche': 1, 'wegzudenken': 1, 'computerprogramme': 1, 'gemeint': 1, 'erfahrungen': 1, 'verbessert': 1, 'jah': 1, 'gerieten': 1, 'gehuft': 1, 'arten': 1, 'naler': 1, 'undconvolutional': 1, 'grundlagen': 1, 'nachfolgenden': 1, 'kapiteln': 1, 'behan': 1, 'delt': 1, 'oderunberwachten': 1, 'einordnen': 1, 'trai': 1, 'ningsdatensatz': 1, 'tupeln': 1, 'xiyivor': 1, 'xiaus': 1, 'x': 1, 'dieyiaus': 1, 'ausgaberaum': 1, 'ystammen': 1, 'datums': 1, 'entspre': 1, 'ausgaberaums': 1, 'zugeordneten': 1, 'wertebereich': 1, 'tupel': 1, 'stellen': 1, 'welchen': 1, 'annimmt': 1, 'klasse': 1, 'ziel': 1, 'generalisierende': 1, 'hxy': 1, 'paramater': 1, 'zusammenhang': 1, 'beschreiben': 1, 'jaufgestellt': 1, 'parameterwahl': 1, 'minimierungsproblem': 1, 'zugleich': 1, 'gesuchte': 1, 'minimierungsprobleme': 1, 'gelst': 1, 'xivor': 1, 'kein': 1, 'wis': 1, 'versucht': 1, 'strukturen': 1, 'clustering': 1, 'hund': 1, 'einhergehende': 1, 'nition': 1, 'nichttriviales': 1, 'funktionale': 1, 'sammenhnge': 1, 'xb': 1, '373190211': 1, 'binres': 1, 'linear': 1, 'trennbares': 1, 'klassizierungsproblem': 1, 'whrt': 1, 'hierbei': 1, 'sigmoidfunktion': 1, 'gin': 1, 'intervall': 1, '01transformiert': 1, 'gxbmitgz': 1, 'expz': 1, 'wiebereitserwhntlassensichdieoptimalenparameter': 1, 'berdierentialrechnungbestimmen': 1, 'explizite': 1, 'numerische': 1, 'annhernde': 1, 'gra': 1, 'dientenverfahren': 1, 'unbekannte': 1, 'nichtlineare': 1, 'zusammenhnge': 1, 'akzeptablen': 1, 'klassi': 1, 'kation': 1, 'nichtlinearen': 1, 'zusammenhngen': 1, 'spezielleren': 1, 'regressionsverfahren': 1, 'support': 1, 'vector': 1, 'machines': 1, 'vielversprechende': 1, 'derartige': 1, 'probleme': 1, 'gegensatz': 1, 'bestehenden': 1, 'statistischen': 1, 'verfah': 1, 'biologischen': 1, 'vorbild': 1, 'ausgerichtet': 1, 'signale': 1, 'summe': 1, 'ausdrucks': 1, 'logistischen': 1, 'ak': 1, 'tivierungsfunktion': 1, 'gweiterverarbeitet': 1, 'bedrfnisse': 1, 'skaliert': 1, 'formalisieren': 1, 'g1g1xb1bracehtipupleftbracehtipdownrightbracehtipdownleftbracehtipupright': 1, '2g2xb2bracehtipupleftbracehtipdownrightbracehtipdownleftbracehtipupright': 1, 'bracehtipupleft': 1, 'bracehtipdownrightbracehtipdownleft': 1, 'bracehtipupright': 1, 'abundcoensichtlich': 1, 'ausdrcke': 1, 'gewichtete': 1, 'vektorschreibweise': 1, 'aundbneuronen': 1, 'cals': 1, 'eignet': 1, 'schreibweise': 1, 'cher': 1, 'netzwerke': 1, 'mangelnder': 1, 'schlecht': 1, 'alternativ': 1, 'zusam': 1, 'menhang': 1, 'gerichteter': 1, 'graph': 1, 'knoten': 1, 'verbindungenalskantengezeichnetwerdekantenknnenmitdemzugehrigenparameterge': 1, 'wicht': 1, 'derart': 1, 'dargestelltes': 1, 'symbolisiert': 1, 'knotenzeichnung': 1, 'beispielhafter': 1, 'eingehende': 1, 'input': 1, 'ausgehende': 1, 'output': 1, 'innere': 1, 'versteckte': 1, 'vielen': 1, 'versteckten': 1, 'alstiefbezeichnet': 1, 'begrndet': 1, 'begri': 1, 'graphreprsentation': 1, 'gerichtet': 1, 'azyklisch': 1, 'kreise': 1, 'allen': 1, 'mglichen': 1, 'spricht': 1, 'beispielhaften': 1, 'beschreibung': 1, 'einfa': 1, 'belegungen': 1, '1212b1b2cgesucht': 1, 'jminimieren': 1, 'verkettete': 1, 'verkompliziert': 1, 'finden': 1, 'korrektes': 1, 'hinblick': 1, 'fang': 1, 'umrissen': 1, 'regel': 1, 'bekannten': 1, 'kettenregel': 1, 'aufbaut': 1, 'aktualisierung': 1, 'erfolgt': 1, 'schritten': 1, 'angelegtes': 1, 'aktuellen': 1, 'abarbeiten': 1, 'rechnet': 1, 'vorgegebenen': 1, 'tupels': 1, 'tatschlich': 1, 'neue': 1, 'parametervernderun': 1, 'genwibestimmt': 1, 'imrckwrtsdurchlauf': 1, 'iunter': 1, 'bestimm': 1, 'rechenvorschriften': 1, 'wij': 1, 'iaktualisiert': 1, 'nutzer': 1, 'denierte': 1, 'lernrate': 1, 'feedforwardnetze': 1, 'anwendbar': 1, 'ntige': 1, 'behandelt': 1, 'tatschlichen': 1, 'frage': 1, '0en': 1, 'scheint': 1, 'sinnvoller': 1, 'nichtsde': 1, 'stotrotz': 1, 'initiale': 1, 'belegung': 1, 'schlechten': 1, 'klassizierungsergebnis': 1, 'fhren': 1, '373190232': 1, 'motivation': 1, 'ko': 1, 'dierung': 1, 'mehreren': 1, 'hundert': 1, 'kombinierte': 1, 'netzengebrachtinabschnitt23werdendaraufhinverschiedeneanwendungenderautoencoder': 1, 'funktionenpaar': 1, 'fgber': 1, 'ddimensionalen': 1, 'gaberaumxrddeniert': 1, 'allgemein': 1, 'blich': 1, 'vektoren': 1, 'features': 1, 'fxfencoderfunktion': 1, 'gfxdecoderfunktion': 1, 'wobeifrpeinpdimensionaler': 1, 'plessmuchdist': 1, 'xxgilt': 1, 'xgfxmin': 1, 'anschaulich': 1, 'forderung': 1, 'gauf': 1, 'fauf': 1, 'xdes': 1, 'ursprngliche': 1, 'xxerhlt': 1, 'komplexitt': 1, 'minimierung': 1, 'zwischenxundxgefordert': 1, 'pdeutlich': 1, 'dsein': 1, 'sie': 1, 'kapitels': 1, 'beschrieben': 1, 'ursprungsdatum': 1, 'umkehrbar': 1, 'verallgemeinernd': 1, 'forderungen': 1, 'endlichen': 1, 'gesuchten': 1, 'trivialerweise': 1, 'denieren': 1, 'fundglineare': 1, 'polynome': 1, 'grades': 1, 'realisierung': 1, 'einzugehen': 1, '373190242': 1, 'xmit': 1, 'ysollen': 1, 'fundgeines': 1, 'fxi': 1, 'w1xib1': 1, 'gxi': 1, 'w2xib2': 1, 'minimieren': 1, 'zuhilfenahme': 1, 'methode': 1, 'kleinsten': 1, 'quadrate': 1, 'erhlt': 1, 'raumxx1xmrdfolgendes': 1, 'optimierungsziel': 1, 'jw1b1w': 1, 'i1xigfxi222': 1, 'i1xiw2w1xib1': 1, 'b2223': 1, 'min': 1, 'einsetzen': 1, 'diesen': 1, 'minimiert': 1, 'erwhnt': 1, 'praktisch': 1, 'autoenco': 1, 'decoderfunktionen': 1, 'xrdjeweils': 1, 'betrachtetmanfunktionenderimvorangegangenbeispieldeniertengestaltergibtsichalsein': 1, 'fachster': 1, 'pneuronen': 1, 'wenn': 1, 'pdie': 1, 'zwischenraums': 1, 'beispielhaft': 1, 'hyperparameter': 1, 'spielt': 1, 'rolle': 1, 'au': 1, 'toencoders': 1, 'bezeichnen': 1, 'zweier': 1, 'linearer': 1, 'mathe': 1, 'matischen': 1, 'vollstndigverbundenen': 1, 'aufweist': 1, '373190252': 1, 'beispielautoencoder': 1, 'selbst': 1, 'reprsentative': 1, 'teilmenge': 1, 'xgenau': 1, 'datumxauch': 1, 'intuitiven': 1, 'stellung': 1, 'vorliegen': 1, 'zustzliches': 1, 'autoen': 1, 'coder': 1, 'einzuordnen': 1, 'trainiertes': 1, 'codie': 1, 'rungeineseingabedatums': 1, 'xberechnetwerdenindemdasdatumandieeingabeschichtangelegt': 1, 'pberechneten': 1, 'aktivierungsfunktionen': 1, 'zbezeichnet': 1, 'teil': 1, 'war': 1, 'optima': 1, 'lerweise': 1, 'nahezu': 1, 'per': 1, 'beschrnkt': 1, 'verringert': 1, 'zieldimension': 1, 'perst': 1, 'einigen': 1, 'bildlich': 1, 'trich': 1, 'darstellen': 1, 'erhhung': 1, 'mehrere': 1, 'hinweg': 1, 'ziehen': 1, 'abschnitten': 1, 'kodie': 1, 'dimensionreduzierung': 1, 'raums': 1, 'bekanntes': 1, 'verbreitetes': 1, 'statistisches': 1, '373190262': 1, 'hauptkomponentenanalyse': 1, 'princi': 1, 'pal': 1, 'component': 1, 'analysis': 1, 'projeziert': 1, 'aufgezeigt': 1, 'denen': 1, 'verglichen': 1, 'einiger': 1, 'gelernte': 1, '3dimensional': 1, 'gegebene': 1, 'r2oderr3visualisiert': 1, 'datensatzes': 1, 'kodierungen': 1, 'amittels': 1, 'bmithilfe': 1, 'stimmt': 1, 'mnistdatensatz': 1, 'verbreiteter': 1, 'klassizierten': 1, 'zahl': 1, 'trag': 1, 'farbwert': 1, 'pixels': 1, 'bildgre': 1, '2828pixeln': 1, '784dimensional': 1, 'vek': 1, 'tor': 1, 'visualisiert': 1, 'zehn': 1, 'kombination': 1, 'symbol': 1, 'farbe': 1, 'separation': 1, 'reprsentationen': 1, 'geringe': 1, 're': 1, 'berlappung': 1, 'deutlichere': 1, 'abgrenzungen': 1, 'reprsentation': 1, 'hherer': 1, 'diejenige': 1, '2dcodierung': 1, 'zahlen': 1, 'dimensionsreduzierungimkontextdergesichtserkennung': 1, 'imvorangegangenbeispielwurde': 1, 'qua': 1, 'litt': 1, 'rckkodierung': 1, 'menschlicher': 1, 'reduziert': 1, 'tatsache': 1, 'charakterisierung': 1, 'gesichtern': 1, 'gengen': 1, 'theorie': 1, 'reprsentatio': 1, 'eigengesichter': 1, 'bekannt': 1, 'geworden': 1, 'obersten': 1, '373190272': 1, 'originalgesichter': 1, 'kodierten': 1, 'rekonstruierten': 1, 'untersten': 1, 'zurckgerechneten': 1, 'diente': 1, 'laboratories': 1, 'cambridge': 1, 'ursprungsdaten': 1, 'schrfer': 1, 'fllt': 1, 'menschen': 1, 'leichter': 1, 'person': 1, 'identizieren': 1, 'gewonnenen': 1, '30dimensionalen': 1, 'dimensions': 1, 'ddimensionales': 1, 'pdimensionales': 1, 'komprimierte': 1, 'anteil': 1, 'vonp': 1, 'ddes': 1, 'speicherbedarfs': 1, 'datentyp': 1, 'opti': 1, 'malerweise': 1, 'verlustfrei': 1, 'wrde': 1, 'auto': 1, 'encoders': 1, 'verlustbehaftete': 1, 'zahlreiche': 1, 'folgendes': 1, 'smartphoneapp': 1, 'mchte': 1, 'dan': 1, 'unntig': 1, 'viel': 1, 'erzeugen': 1, 'prozedur': 1, 'kodiere': 1, 'dauf': 1, 'encoderfunktion': 1, 'zuz1z2z': 1, 'pmitplessmuchd': 1, 'sendez1z2z': 1, 'pan': 1, 'dekodierez1z2z': 1, 'pauf': 1, 'dekoderfunktion': 1, 'x1x2xd': 1, 'hneln': 1, 'xiso': 1, 'xi': 1, 'app': 1, 'etwaiger': 1, 'audiovi': 1, 'suellen': 1, 'gewissem': 1, 'mae': 1, 'hinnehmbar': 1, '373190282': 1, 'knninitialisierung': 1, 'angesprochen': 1, 'bedeu': 1, 'trivial': 1, 'optimal': 1, 'mglichkeit': 1, 'phasen': 1, '1pretraining': 1, 'nacheinander': 1, 'exklusive': 1, 'bearbeitet': 1, 'struiert': 1, 'w1undw2in': 1, 'ers': 1, 'ten': 1, 'temporren': 1, 'ausgabelayer': 1, 'dort': 1, 'trainingsdaten': 1, 'somit': 1, 'w1undwprime': 1, '1gelerntwprime': 1, '1wird': 1, 'ver': 1, 'worfen': 1, 'w1fr': 1, 'kommenden': 1, 'iterationen': 1, 'gesetzt': 1, 'nchste': 1, 'w2der': 1, 'lernt': 1, 'bereinstimmen': 1, '2finetuning': 1, 'gesetzten': 1, 'vorherigen': 1, 'berwacht': 1, '3finetuning': 1, 'eigentliche': 1, 'mithilfe': 1, '373190293': 1, 'natrlichen': 1, 'fotos': 1, 'sprachen': 1, 'freut': 1, 'besonders': 1, 'relevanz': 1, 'automatischen': 1, 'spra': 1, 'cherkennung': 1, 'autonomen': 1, 'fahrens': 1, 'vielversprechender': 1, 'ansatz': 1, 'zugrundeliegende': 1, 'ideen': 1, 'vorangegangen': 1, 'spezieller': 1, 'typ': 1, 'verbunde': 1, 'gefhrt': 1, 'daraufhin': 1, 'beleuchtet': 1, 'abschlieenden': 1, 'verweise': 1, 'cnnanwendungen': 1, 'aufgefhrt': 1, 'natrliche': 1, 'hohe': 1, 'parametern': 1, 'pixel': 1, 'dimensio': 1, '100000': 1, 'ergeben': 1, 'vollstndi': 1, 'ger': 1, 'verbundenheit': 1, '106105': 1, '1011': 1, '100milliarden': 1, 'vielfache': 1, 'hinzuzurechnen': 1, 'trainingsauf': 1, 'wandweiterhinbestehtdiegefahrderberanpassungengl': 1, 'desneuronalennetzes': 1, 'klein': 1, 'schaulich': 1, 'auswendig': 1, 'vorstellen': 1, 'triviale': 1, 'reduzieren': 1, 'weglassen': 1, 'zwischen': 1, 'backpropa': 1, 'gation': 1, 'vorhandene': 1, 'gewichtsmatrix': 1, 'dauerhaft': 1, 'belegt': 1, '3731902103': 1, 'tragen': 1, 'ihrem': 1, 'namen': 1, 'bezeichnung': 1, 'damitistdersogenanntediskretefaltungsoperatorenglconvolutiongemeintwelcherhug': 1, 'veranschaulicht': 1, 'a1a': 1, 'nein': 1, 'sogenannter': 1, 'kernelvektor': 1, 'f1f': 1, 'kundkn': 1, 'afxksummationdisplay': 1, 'i1axi1fimitx1nk': 1, 'beispielvektoren': 1, '50': 1, '7undk': 1, 'ergebnisvektor': 1, 'aundfgenaunk': 1, '73': 1, 'af1': 1, 'af2': 1, 'af3': 1, 'af4': 1, 'af5': 1, 'summationtext3': 1, 'i1a1i1fisummationtext3': 1, 'i1a2i1fisummationtext3': 1, 'i1a3i1fisummationtext3': 1, 'i1a4i1fisummationtext3': 1, 'i1a5i1fi': 1, '301': 1, 'aufdenerstenblickerschlietsichmglicherweisenichtdersinndervorangegangenveranschau': 1, 'lichten': 1, 'convolutionoperation': 1, 'berlappt': 1, 'teilbereich': 1, 'ahingegen': 1, 'messkurve': 1, 'elektri': 1, 'schen': 1, 'signales': 1, 'fzb': 1, 'signals': 1, 'verursachen': 1, 'extreme': 1, 'ausreier': 1, 'fhtte': 1, 'mglicherweise': 1, 'her': 1, 'geltert': 1, 'faltungsoperators': 1, 'eingabesignal': 1, 'huge': 1, 'vorgehensweise': 1, '3731902113': 1, 'erschliet': 1, 'noch': 1, 'anschaulicher': 1, '2dconvolution': 1, 'aijknneine': 1, 'fijkkkmitk': 1, 'neine': 1, 'kernelmatrix': 1, 'afxyksummationdisplay': 1, 'j1axi1yj1fij': 1, 'mitxy1nk': 1, 'beispielmatrizen': 1, '6undk': 1, 'afknk1nk1k44': 1, 'grnden': 1, '16': 1, 'af11ksummationdisplay': 1, 'j1aijfij': 1, 'anderen': 1, 'gesamtergebnis': 1, 'anschaulichwurdediefiltermatrixzurberechnungeinesteilergebnisseshnlichder1dimensio': 1, 'fkanten': 1, 'aerkennt': 1, 'aals': 1, 'matrix': 1, 'pixelfarbwerten': 1, 'interpretiert': 1, 'dunklen': 1, 'hellen': 1, 'farbton': 1, 'reprsentiert': 1, 'kante': 1, '3731902123': 1, 'mitte': 1, 'erkannt': 1, 'worden': 1, 'grundprinzip': 1, '2dimensionaler': 1, 'vielzahl': 1, 'eekte': 1, 'schrfung': 1, 'weichzeichnung': 1, '33filter': 1, 'nn': 1, 'entsteht': 1, 'n2n2matrix': 1, 'umstnden': 1, 'beibehalten': 1, 'padding': 1, 'hinzugefgt': 1, 'tatschliche': 1, 'eingabegre': 1, 'erhht': 1, 'ursprungsab': 1, 'messungen': 1, 'matrixeintrge': 1, 'einnehmen': 1, 'hngt': 1, 'diverse': 1, 'lsungsanstze': 1, 'entgegen': 1, 'gleichbleibender': 1, 'erhalten': 1, 'kleineres': 1, 'summenindex': 1, 'wur': 1, 'ua': 1, 'idee': 1, 'teilweise': 1, 'allermeist': 1, 'zusammengesetzt': 1, 'paarungen': 1, 'wieder': 1, 'mehrfach': 1, 'hintereinander': 1, 'geschaltet': 1, 'teilnetze': 1, 'hlt': 1, 'blicherweise': 1, 'verbundener': 1, 'neuronenverbindungen': 1, 'detaillierter': 1, 'einzelnen': 1, 'eingegangen': 1, '3731902133': 1, 'name': 1, 'sagt': 1, 'fal': 1, 'eingabesignalen': 1, 'angelehnt': 1, 'convolutionoperatorabdhjedesneuronineinemconvolutionallayerberechnetgenaueinen': 1, 'zeption': 1, 'folglich': 1, 'denierten': 1, 'eintrgen': 1, 'hauptidee': 1, 'fnicht': 1, 'feststehen': 1, 'wacht': 1, 'gewnschten': 1, 'trainierten': 1, 'klassizie': 1, 'eingabesignale': 1, 'genutzt': 1, 'aussage': 1, 'treen': 1, 'objekt': 1, 'fmit': 1, 'unbekannten': 1, 'ge': 1, 'wichten': 1, 'w2': 1, 'w3': 1, '1dfaltung': 1, 'eineraktivierungsfunktionverarbeitetoensichtlichhatdasinderabbildungdargestelltelokal': 1, 'fester': 1, 'gelten': 1, 'w1w4w7undw2w5w8undw3w6w9': 1, 'farblich': 1, 'zustzliche': 1, 'bestimmter': 1, 'desweiteren': 1, 'speicherbedarf': 1, 'weniger': 1, 'unterschiedliche': 1, '3731902143': 1, 'gleicher': 1, 'schon': 1, 'minimale': 1, 'nderung': 1, 'wimit': 1, 'konkreten': 1, 'un': 1, 'abhngig': 1, 'manche': 1, 'ihnen': 1, 'rckwrtsdurchlauf': 1, 'iberechnet': 1, 'aktualisieren': 1, 'laut': 1, 'blei': 1, 'ben': 1, 'w1w4w7': 1, 'identisch': 1, 'einuss': 1, 'bercksichtigen': 1, 'betroenen': 1, 'dadurch': 1, 'w1w1j': 1, 'w1j': 1, 'w4j': 1, 'w7': 1, 'subsamplinglayer': 1, 'maximumoperator': 1, 'ihrer': 1, 'eingabewerte': 1, 'ausfhren': 1, 'angewendet': 1, 'funktionsweise': 1, 'operators': 1, 'linke': 1, 'obere': 1, 'maximum': 1, 'vier': 1, 'linken': 1, '1329': 1, 'maxbracketleftbigg': 1, '3bracketrightbigg': 1, 'fortfhrend': 1, 'hinter': 1, 'geschalteten': 1, 'sinne': 1, 'ihres': 1, 'zusammen': 1, 'gefasst': 1, '3731902153': 1, 'intendierten': 1, 'vergrbert': 1, 'berssige': 1, 'op': 1, 'timalerweise': 1, 'verworfen': 1, 'oftmals': 1, 'grobe': 1, 'informati': 1, 'wo': 1, 'feature': 1, 'bendet': 1, 'vielmehr': 1, 'erkenntnis': 1, 'eingegebene': 1, 'links': 1, 'rote': 1, 'ampel': 1, 'verwen': 1, 'dung': 1, 'maximumoperators': 1, 'uert': 1, 'klassizierungsaufgaben': 1, 'statt': 1, 'etwas': 1, 'schlechtere': 1, 'nutzen': 1, 'toleranz': 1, 'gegenber': 1, 'gewissen': 1, 'verschiebungen': 1, 'derselben': 1, 'ton': 1, 'durchaus': 1, 'vorhanden': 1, 'fotograeren': 1, 'leichten': 1, 'zeitlichen': 1, 'versatz': 1, 'tonaufnahmen': 1, 'gegeben': 1, 'einzige': 1, 'rechts': 1, 'verschoben': 1, 'derverwendungdesmaxoperatorsgleichdiegleichheitderwerteistinderabbildungfarblich': 1, 'originale': 1, 'verschobene': 1, 'oensichtliche': 1, 'eigenschaft': 1, 'verbindungengelerntwerdenmssendasneuronfhrtnureineeinfachestatischefunktionaus': 1, 'leicht': 1, 'implementieren': 1, 'beachten': 1, 'vorwrts': 1, 'lauf': 1, 'pfade': 1, 'maximumsoperationen': 1, 'gewonnen': 1, 'pfaden': 1, 'ausfhrung': 1, 'aktualisiert': 1, 'folgen': 1, 'vollstndigverbundene': 1, 'nimmt': 1, 'bentigten': 1, 'ausgabedimension': 1, 'klassische': 1, 'klassizierungschicht': 1, 'nichtlinearer': 1, 'verstehen': 1, 'zuletzt': 1, 'gesamten': 1, '3731902163': 1, 'vollstndigkeit': 1, 'halber': 1, 'abschluss': 1, 'net': 1, 'workszweierweiterungenderbishervorgestelltenarchitekturgenanntwiebereitsimabschnitt': 1, 'durchzufhren': 1, 'berhinaus': 1, 'hinzukommen': 1, 'signal': 1, 'kanle': 1, 'verschiede': 1, 'farbkanle': 1, 'kanal': 1, 'gelb': 1, 'blauwert': 1, 'eingabekanlen': 1, 'ausfhrt': 1, 'nennenswerte': 1, 'herausforderung': 1, '2dimensionales': 1, 'verschiedener': 1, 'eingabekanle': 1, 'abbildungen': 1, 'convolutionoperationen': 1, 'beinhalten': 1, 'weichzeichnen': 1, 'werdendieergebnisseeinersolchenoperationwerdenhugals': 1, 'mapbezeichnetabbildung36': 1, 'faltungsoperationen': 1, 'filtervektoren': 1, '36': 1, 'faltungsoperatoren': 1, '3731902173': 1, 'vorangehend': 1, 'erluterten': 1, 'verschiedensten': 1, 'bereichen': 1, 'hervorragende': 1, 'klassizierungsergebnisse': 1, 'inbesondere': 1, 'handge': 1, 'schriebenen': 1, 'zeichen': 1, 'namens': 1, 'sehrguteergebnisselieferndieshatsmtlichecnnarchitekturenbzwderenerfolgbegrndet': 1, 'tauchen': 1, 'besproche': 1, 'erfolg': 1, 'hoch': 1, 'aufgelsten': 1, 'hilfe': 1, 'krizhevskyetalinksh12erzielendieklassikationzieltedabeiaufdieerkennungbestimm': 1, 'objekte': 1, 'abermals': 1, 'kennengelernten': 1, 'komponenten': 1, 'konzepte': 1, 'wiedernden': 1, 'bilddaten': 1, 'erfreuen': 1, 'spracherken': 1, 'nung': 1, 'natural': 1, 'language': 1, 'beliebtheit': 1, 'natrlichsprachliche': 1, 'stze': 1, 'fand': 1, 'inc': 1, 'anderem': 1, 'smartphone': 1, 'sprachsteuerung': 1, 'sprachrohdaten': 1, 'maschinenfreundliche': 1, 'umgewandelt': 1, 'alphago': 1, '3731902184': 1, 'formen': 1, 'umkehrbare': 1, 'eingabedatums': 1, 'einbettungen': 1, 'vielerlei': 1, 'vergleich': 1, 'darberhinaus': 1, 'erfolgreich': 1, 'allgemeinen': 1, 'bestehen': 1, 'grundstzlich': 1, 'stndig': 1, 'convolutionalschicht': 1, 'faltungsoperator': 1, 'faltungsoperation': 1, 'liegenden': 1, 'state': 1, 'artim': 1, 'klassizierung': 1, 'natrlich': 1, 'objekter': 1, 'kennung': 1, '373190219literaturverzeichnis': 1, 'pierre': 1, 'baldi': 1, 'pro': 1, 'ceedings': 1, 'icml': 1, 'transfer': 1, '3749': 1, 'dumitru': 1, 'erhan': 1, 'aaron': 1, 'courville': 1, 'pierreantoine': 1, 'manzagol': 1, 'pascal': 1, 'vincent': 1, 'samy': 1, 'why': 1, 'does': 1, 'pretraining': 1, 'help': 1, 'journal': 1, 'machine': 1, 'research': 1, '11feb625660': 1, 'carl': 1, 'friedrich': 1, 'gauss': 1, 'theoria': 1, 'motus': 1, 'corporum': 1, 'coelestium': 1, 'sectionibus': 1, 'conicis': 1, 'solem': 1, 'ambientium': 1, 'volume': 1, '7': 1, 'fa': 1, 'perthes': 1, '1877': 1, 'thorsten': 1, 'hermes': 1, 'bildverarbeitungeine': 1, 'prak': 1, 'tische': 1, '134135': 1, '2005': 1, 'ruslan': 1, 'r': 1, 'salakhutdinov': 1, 'reducing': 1, 'dimensionality': 1, 'data': 1, 'science': 1, '3135786504507': 1, '2006': 1, 'yoon': 1, 'kim': 1, 'sentence': 1, 'arxiv': 1, 'preprint': 1, 'arxiv14085882': 1, '2014': 1, 'alex': 1, 'krizhevsky': 1, 'ilya': 1, 'sutskever': 1, 'imagenet': 1, 'advances': 1, 'information': 1, '10971105': 1, 'nonlinear': 1, 'classiers': 1, 'algorithm': 1, 'recurrent': 1, 'brain': 1, 'lon': 1, 'bottou': 1, 'patrick': 1, 'haner': 1, 'gradientbased': 1, 'applied': 1, 'document': 1, '86112278': 1, '2324': 1, '1998': 1, 'corinna': 1, 'cortes': 1, 'cj': 1, 'burges': 1, 'handwritten': 1, 'digit': 1, 'labs': 1, 'online': 1, 'available': 1, 'httpyann': 1, 'comexdbmnist': 1, 'warren': 1, 'mcculloch': 1, 'walter': 1, 'pitts': 1, 'logical': 1, 'calculus': 1, 'ideas': 1, 'immanent': 1, 'nervous': 1, 'activity': 1, 'bulletin': 1, 'mathematical': 1, 'biophysics': 1, '54115133': 1, 'dec': 1, '1943': 1, 'k': 1, 'pearson': 1, 'lines': 1, 'planes': 1, 'closest': 1, 't': 1, 'points': 1, 'space': 1, 'philoso': 1, 'phical': 1, 'magazine2': 1, '559572': 1, '1901': 1, 'ferdinando': 1, 'samaria': 1, 'andy': 1, 'harter': 1, 'parameterisation': 1, 'stochastic': 1, 'model': 1, 'forhumanfaceidentication': 1, 'applications': 1, 'computer': 1, 'vision': 1, 'second': 1, '138142': 1, '373190220literaturverzeichnis': 1, 'david': 1, 'silver': 1, 'aja': 1, 'huang': 1, 'chris': 1, 'maddison': 1, 'arthur': 1, 'guez': 1, 'laurent': 1, 'sifre': 1, 'george': 1, 'van': 1, 'driessche': 1, 'julian': 1, 'schrittwieser': 1, 'ioannis': 1, 'antonoglou': 1, 'veda': 1, 'panneershelvam': 1, 'marc': 1, 'lanctot': 1, 'mastering': 1, 'game': 1, 'go': 1, 'tree': 1, 'searchnature': 1, '5297587484489': 1, '2016': 1, 'lawrence': 1, 'sirovich': 1, 'michael': 1, 'kirby': 1, 'lowdimensional': 1, 'procedure': 1, 'characte': 1, 'rization': 1, 'human': 1, 'josa': 1, '43519524': 1, '1987': 1, 'dominikschererandreasmllerandsvenbehnke': 1, 'evaluationofpoolingoperations': 1, 'object': 1, 'articial': 1, '92101': 1, 'springer': 1, 'tara': 1, 'sainath': 1, 'carolina': 1, 'parada': 1, 'small': 1, 'footprint': 1, 'keyword': 1, 'spotting': 1, 'sixteenth': 1, 'annual': 1, 'speech': 1, 'communication': 1, 'association': 1})\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "from collections import Counter  #Um die Woerter zu zaehlen\n",
    "import re #Ignoriert den Text ohne Woerter\n",
    "\n",
    "def tf_read(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file: #Oeffnen im binaermodus\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "        for page_num in range(num_pages):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def count_words(text):\n",
    "    # Normalizing text by converting to lowercase and removing non-alphanumeric characters\n",
    "    normalized_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower()) #Ersatz spezielle symbole in leerzeichen und umwandlung gross/klein\n",
    "    keys = normalized_text.split() #Text wird in Woerter aufgeteilt und keys der Woerterliste gespeichert\n",
    "    word_count = Counter(keys) #Zaehlung\n",
    "    return word_count\n",
    "\n",
    "def main():\n",
    "    pdf_path = \"Einführung in Autoencoder und Convolutional Neural Networks.pdf\"  # Replace with the path to your PDF file\n",
    "    text = tf_read(pdf_path)\n",
    "    values = count_words(text) #Values die Anzahl wie oft ein Wort vorkommt\n",
    "    print(\"Dicitionary with keys and additional values:\")\n",
    "    print(values)\n",
    "\n",
    "if __name__ == \"__main__\": #Eine bedingte Anweisung, die überprüft, ob das Skript direkt ausgeführt wird.\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf588f4f",
   "metadata": {},
   "source": [
    "### 2. Korpus\n",
    "Ein Korpus ist eine Sammlung von Dictionaries, wie sie in der ersten Aufgabe beschrieben werden.\n",
    "Erzeugen Sie ein Korpus aus mehreren Dokumenten.\n",
    "\n",
    "Td-idf-Mass: Ein statistisches Mass fuer die Beurteilung der Relevanz von Termen in Dokumenten einer Kollektion. Auch Vorkommenshauefigkeit/Termfrequenz. Es wird berechnet wie oft ein Term vorkommt in anbetracht wie hauefig ein Termin ueberhaupt vorkommen kann.\n",
    "Inverse Dokumenthauefigkeit (IDF): Misst die Sepzifitaet eines Terms fuer die Gesamtmenge der betrachteten Dokumente. Wird dieser Wert ueber eine Gesamtmenge aller Dokumente betrachtet (Dokumentkorpus) wird der Begriff \"inverse Dokumentfrequenz\" genannt.\n",
    "Ein Wort das in wenigen Dokumenten hauefig vorkommt ist geeigneter, als ein Wort, das in jedem Dokument vorkommt oder nur gering vorkommt.\n",
    "IDF und Tf-idf-mass zusammen ergeben die Gewichtung von Woertern bei der automatischen indexierung im information retrieval.\n",
    "\n",
    "Quellen:\n",
    "https://de.wikipedia.org/wiki/Tf-idf-Ma%C3%9F#Vorkommensh%C3%A4ufigkeit\n",
    "https://realpython.com/nltk-nlp-python/\n",
    "https://www.mygreatlearning.com/blog/nltk-tutorial-with-python/\n",
    "https://www.nltk.org/howto/corpus.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cd5ab9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Universität Leipzig\\nInstitut für Informatik\\nFakultät für Mathematik und Informatik\\nAbteilung Datenbanken\\nEinführung in Autoencoder und Convolutional Neural Networks\\nSeminararbeit im Seminar: Deep Learning\\nvorgelegt von:\\nElias Saalmann\\nMatrikelnummer:\\n3731902\\nBetreuer:\\nVictor Christen\\n©2018\\nDieses Werk einschließlich seiner Teile ist urheberrechtlich geschützt . Jede Verwertung außerhalb der\\nengen Grenzen des Urheberrechtgesetzes ist ohne Zustimmung des Autors unzulässig und strafbar. Das\\ngilt insbesondere für Vervielfältigungen, Übersetzungen, Mikroverﬁlmungen sowie die Einspeicherung und\\nVerarbeitung in elektronischen Systemen.Inhaltsverzeichnis\\nInhaltsverzeichnis\\n1 Einleitung 1\\n2 Autoencoder 4\\n2.1 Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n2.2 Autoencoder als künstliche neuronale Netze . . . . . . . . . . . . . . . . . . . . . 5\\n2.3 Anwendungen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n3 Convolutional Neural Networks 10\\n3.1 Lokal verbundene neuronale Netze . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n3.2 Der Convolution-Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n3.3 Aufbau von Convolutional Neural Networks . . . . . . . . . . . . . . . . . . . . . 13\\n3.4 Erweiterungen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n3.5 Anwendungen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n4 Zusammenfassung 19\\nLiteraturverzeichnis 20\\nElias Saalmann\\n3731902I1 Einleitung\\n1 Einleitung\\nHeutzutage sind Stichworte wie Maschinelles Lernen oderKünstliche Intelligenz nicht mehr aus\\nder Informatik wegzudenken. Formal sind damit Computerprogramme gemeint, deren Ergebnis\\ndurch die Verarbeitung von Erfahrungen (Training) verbessert werden kann. In den letzten Jah-\\nren gerieten gehäuft Künstliche neuronale Netze (KNN) und Deep Learning in den Fokus von\\nWirtschaft und Wissenschaft. In dieser Arbeit werden zwei spezielle Arten künstlicher neuro-\\nnaler Netze erläutert: Autoencoder undConvolutional Neural Networks . Dazu werden zunächst\\ndie Grundlagen des maschinellen Lernens sowie künstlicher neuronaler Netze besprochen. In den\\nnachfolgenden Kapiteln werden dann Autoencoder und Convolutional Neural Networks behan-\\ndelt. Diese Arbeit beruht grundlegend auf [L+15a] und [L+15b].\\nMaschinelles Lernen Jedes Problem des maschinellen Lernens lässt sich in den Bereich des\\nüberwachten oderunüberwachten Lernens einordnen. Beim überwachten Lernen liegt ein Trai-\\nningsdatensatz bestehend aus Tupeln (x(i),y(i))vor, wobei die x(i)aus einem Eingaberaum X\\nund diey(i)aus einem Ausgaberaum Ystammen. Dabei sind die Elemente des Eingaberaums\\nsogenannte Feature-Vektoren , deren Einträge bestimmten Merkmalen eines Datums entspre-\\nchen. Die Elemente des Ausgaberaums sind die den Eingabedaten zugeordneten Werte, z.B. der\\nWertebereich einer Funktion (Regression) oder eine Menge von Klassen (Klassiﬁkation). Die\\nTupel stellen bereits vorhandenes Wissen über die Eingabedaten dar, i.e. welchen Wert einer\\nFunktion ein bestimmtes Eingabedatum annimmt oder welche Klasse ein Eingabedatum hat.\\nZiel des überwachten Lernens ist es nun, eine möglichst generalisierende Funktion h:X→Y\\nbzw. deren Paramater θzu lernen, die den Zusammenhang von Eingaberaum und Ausgabe-\\nraum möglichst gut beschreiben. Zumeist wird dazu eine Fehlerfunktion Jaufgestellt, die für\\neine konkrete Parameter-Wahl θeinen Fehler auf dem Trainingsdatensatz berechnet. Betrachtet\\nman das Minimierungsproblem über die Fehlerfunktion J, liefert eine Lösung davon zugleich die\\ngesuchte Funktion h. Die Minimierungsprobleme werden in der Praxis häuﬁg über Verfahren\\naus der Diﬀerentialrechnung, i.e. über Gradienten, gelöst. Beim unüberwachten Lernen liegen\\nim Trainingsdatensatz nur Feature-Vektoren x(i)vor, d.h. es liegt kein bereits vorhandenes Wis-\\nsen über die Daten vor. Beim unüberwachten Lernen wird versucht, Strukturen innerhalb des\\nEingaberaums zu ﬁnden, z.B. mittels Clustering.\\nKünstliche neuronale Netze Die Wahl der Struktur von hund die damit einhergehende De-\\nﬁnition der zu lernenden Parameter ist ein nicht-triviales Problem. Für lineare funktionale Zu-\\nsammenhänge bietet sich eine einfache lineare Funktion der Form\\nh(x) =θx+b\\nan. Dies entspricht der linearen Regression .\\nElias Saalmann\\n373190211 Einleitung\\nFür ein binäres linear trennbares Klassiﬁzierungs-Problem hat sich die logistische Regression be-\\nwährt. Hierbei wird der lineare Ausdruck zusätzlich über die Sigmoid-Funktion gin das Intervall\\n[0,1]transformiert:\\nh(x) =g(θx+b)mitg(z) =1\\n1 + exp(−z)\\nWiebereitserwähnt,lassensichdieoptimalenParameter θüberDiﬀerentialrechnungbestimmen\\n- entweder über eine explizite Lösung oder über numerische annähernde Verfahren, z.B. das Gra-\\ndientenverfahren . Für unbekannte nicht-lineare Zusammenhänge liefern lineare bzw. logistische\\nRegression allerdings keine akzeptablen Ergebnisse mehr. Neben weiteren Verfahren zur Klassi-\\nﬁkation von Daten mit nicht-linearen Zusammenhängen, z.B. spezielleren Regressionsverfahren\\noder Support Vector Machines, sind Künstliche neuronale Netze eine vielversprechende Lösung\\nfür derartige Probleme. Dieses Verfahren ist im Gegensatz zu bestehenden statistischen Verfah-\\nren an einem biologischen Vorbild ausgerichtet: Ein neuronales Netz besteht aus Neuronen , die\\nmiteinander verbunden sein können. Dabei verarbeitet jedes Neuron die Signale der eingehenden\\nNeuronen und berechnet eine Ausgabe auf Grundlage einer gewichteten Summe. Das Ergebnis\\ndieses linearen Ausdrucks wird ähnlich der logistischen Regression mittels einer sogenannten Ak-\\ntivierungsfunktion gweiterverarbeitet und entsprechend der Bedürfnisse skaliert. Die konkrete\\nGestalt der Aktivierungsfunktion ist nicht fest. Die Hintereinanderausführung solcher Neuronen\\nlässt sich mathematisch als Funktion von Funktionen formalisieren. Betrachte z.B. die folgende\\nStruktur von h:\\nh(x) =g(ω1·g(θ1x+b1/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\\n(a)) +ω2·g(θ2x+b2/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\\n(b)) +c\\n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\\n(c)) (1.1)\\nDann sind (a),(b)und(c)oﬀensichtlich einfache lineare Ausdrücke bzw. gewichtete Summen\\n(hier in Vektorschreibweise), wobei (a)und(b)Neuronen sind, die dem Neuron (c)als Eingabe\\ndienen.\\nOﬀensichtlich eignet sich die mathematische Schreibweise zur Deﬁnition bzw. Konstruktion sol-\\ncher Netzwerke aufgrund mangelnder Übersichtlichkeit schlecht. Alternativ kann der Zusam-\\nmenhang als gerichteter Graph dargestellt werden: Neuronen können als Knoten und neuronale\\nVerbindungenalsKantengezeichnetwerde.KantenkönnenmitdemzugehörigenParameter(Ge-\\nwicht) bezeichnet werden. Abbildung 1.1 zeigt ein derart dargestelltes neuronales Netz. Dabei\\nsymbolisiert die Art der Knoten-Zeichnung die Wahl der Aktivierungsfunktion.\\nAbbildung 1.1: Beispielhafter Aufbau eines künstlichen neuronalen Netzes [L+15a]\\nElias Saalmann\\n373190221 Einleitung\\nDie Schicht der Eingabeneuronen (ohne eingehende Neuronen) wird auch Eingabeschicht (engl.\\nInput Layer) genannt - die Schicht der Ausgabeneuronen (ohne ausgehende Neuronen) wird als\\nAusgabeschicht (engl. Output Layer) bezeichnet. Innere Schichten werden versteckte Schicht\\n(engl. Hidden Layer) genannt. Ein künstliches neuronales Netz mit vielen versteckten Schichten\\nwird alstiefbezeichnet - dies begründet den Begriﬀ Deep Learning .\\nEin neuronales Netz, welches in seiner Graph-Repräsentation gerichtet und azyklisch ist, also\\nkeine Kreise enthält, wird als Feedforward -Netz bezeichnet. Sind für eine Schicht alle Neuro-\\nnen mit allen möglichen Neuronen der darauf folgenden Schicht verbunden, spricht man von\\nschichtweise vollständig verbundenen Netzen.\\nBackpropagation An der beispielhaften mathematischen Beschreibung (1.1) eines sehr einfa-\\nchen Feedforward-Netzes wird ersichtlich, welche Parameter trainiert werden müssen: Es sind\\ndiejenigen Belegungen für ω1,ω2,θ1,θ2,b1,b2,cgesucht, welche die Fehlerfunktion Jminimieren.\\nAufgrund der Konstruktion über verkettete Funktionen verkompliziert sich das Finden einer sol-\\nchen Lösung bzw. entsprechender Gradienten.\\nEin mathematisch korrektes Verfahren zur Berechnung aller Gradienten zum Training eines\\nFeedforward-Netzes liefert der Backpropagation -Algorithmus. Im Hinblick auf Fokus und Um-\\nfang dieser Arbeit sei dieser nachfolgend nur grob umrissen. Grundlegend basiert das Verfahren\\nauf der sogenannten δ-Regel, welche wiederum auf die aus der Diﬀerentialrechnung bekannten\\nKettenregel aufbaut. Die Aktualisierung der Gewichte eines neuronalen Netzes erfolgt grob in\\nden folgenden drei Schritten:\\n1. Im sogenannten Vorwärtsdurchlauf wird das Ergebnis für ein angelegtes Eingabedatum\\nunter Verwendung der aktuellen Gewichte durch einfaches Abarbeiten der Schichten be-\\nrechnet. Auf Basis des Fehlers des durch den Trainingsdatensatz vorgegebenen Tupels zum\\ntatsächlich berechneten Ergebnis müssen nun neue Parameter bzw. Parameterveränderun-\\ngen∆wibestimmt werden.\\n2. ImRückwärtsdurchlauf werden für alle widie Gradienten∂J\\n∂w iunter Verwendung bestimm-\\nter Rechenvorschriften und des im Vorwärtsdurchlauf bestimmten Fehlers berechnet.\\n3. Abschließend werden die Parameter gemäß ∆wi=α∂J\\n∂w iaktualisiert, wobei αeine vom\\nNutzer deﬁnierte Lernrate ist.\\nDer Backpropagation-Algorithmus ist i.A. nur auf einfache Feedforward-Netze anwendbar - im\\nRahmen dieser Arbeit werden nötige Erweiterungen des Algorithmus an entsprechender Stelle\\nbehandelt. Neben der tatsächlichen Berechnung der Gewichte mit Backpropagation stellt sich\\ndie Frage, wie diese initialisiert werden sollen. Eine Initialisierung mit 0en hat sich als wenig\\nvielversprechend erwiesen - eine zufällige Initialisierung scheint i.A. sinnvoller zu sein. Nichtsde-\\nstotrotz kann auch eine zufällige initiale Belegung zu einem schlechten Klassiﬁzierungsergebnis\\nführen.\\nElias Saalmann\\n373190232 Autoencoder\\n2 Autoencoder\\nAls Motivation zu Autoencodern sei zunächst das Interesse an der niedrig-dimensionalen Ko-\\ndierung von hoch-dimensionalen Daten genannt. Auf einem Datensatz mit mehreren Hundert\\nMerkmalen kann eine Reduzierung auf einige wenige (miteinander kombinierte) Merkmale von\\nInteresse sein, insbesondere zur Kompression oder Visualisierung.\\nIn diesem Kapitel werden Autoencoder als ein spezielle Form von künstlichen neuronalen Netzen\\neingeführt, die eine solche niedrig-dimensionale Kodierung berechnen. In Abschnitt 2.1 wird\\ndiese zunächst mathematisch deﬁniert und in Abschnitt 2.2 in den Kontext von neuronalen\\nNetzengebracht.InAbschnitt2.3werdendaraufhinverschiedeneAnwendungenderAutoencoder\\nbesprochen.\\n2.1 Deﬁnition\\nNachfolgend sei ein Autoencoder ein Funktionenpaar (f,g)über einem d-dimensionalen Ein-\\ngaberaumX⊆Rddeﬁniert. Die Elemente des Eingaberaums seien hier, wie im maschinellen\\nLernen allgemein üblich, Vektoren, deren Elemente i.A. Merkmalen (engl. features) entsprechen.\\nGesucht sind nun Funktionen\\n•f:X→F(Encoder-Funktion)\\n•g:F→X(Decoder-Funktion)\\nwobeiF⊆Rpeinp-dimensionaler Zwischenraum mit p/lessmuchdist und für alle x∈Xgilt:\\n|x−(g◦f)(x)|→min (2.1)\\nAnschaulich bedeutet die Forderung (2.1), dass man bei Anwendung der Funktion gauf das\\nErgebnis der Anwendung von fauf ein beliebiges Element xdes Eingaberaums, optimalerweise\\ngenau das ursprüngliche Element ˆx=xerhält. Da dies je nach Komplexität von fundg\\nnicht immer möglich ist, wird in der Deﬁnition eines Autoencoders die Minimierung des Fehlers\\nzwischenxundˆxgefordert [Bal12]. Da pdeutlich kleiner als dsein soll, berechnet die Encoder-\\nFunktion oﬀensichtlich eine niedrig-dimensionale Kodierung, wie sie zu Beginn des Kapitels\\nbeschrieben wurde. Der Autoencoder liefert zudem die Decoder-Funktion, über die aus einem\\nbereits codierten Datensatz das Ursprungsdatum berechnet werden kann - die Kodierung ist\\nalso umkehrbar.\\nEine solche Deﬁnition eines Autoencoders ist sehr verallgemeinernd, da keine Forderungen an\\ndie konkrete Gestalt der Funktionen enthalten ist. Für einen endlichen Eingaberaum lassen sich\\ndie gesuchten Funktionen formal in trivialerweise deﬁnieren.\\nIm Folgenden betrachten wir ein Beispiel unter der Annahme, dass fundglineare Funktionen\\nbzw. Polynome ersten Grades sind, um anschließend auf die Realisierung mittels künstlicher\\nneuronaler Netze einzugehen.\\nElias Saalmann\\n373190242 Autoencoder\\nBeispiel Für einen einen Eingaberaum Xmit Zwischenraum Ysollen die beiden Funktionen\\nfundgeines Autoencoders in diesem Beispiel folgende Gestalt haben:\\n•f(x(i)) =W1x(i)+b1\\n•g(x(i)) =W2x(i)+b2\\nGesucht sind nun also die Parameter W1,b1,W 2,b2, die für jedes Eingabedatum den Fehler (2.1)\\nminimieren.\\nUnter Zuhilfenahme der Methode der kleinsten Quadrate [Gau77] erhält man für den Eingabe-\\nraumX={x(1),...,x(m)}⊂Rdfolgendes Optimierungsziel J:\\nJ(W1,b1,W 2,b2) =m/summationdisplay\\ni=1(x(i)−(g◦f)(x(i)))2(2.2)\\n=m/summationdisplay\\ni=1(x(i)−(W2(W1x(i)+b1) +b2))2(2.3)\\n→min\\nDabei ergibt sich (2.3) durch einfaches Einsetzen der Hintereinanderausführung von fundg\\nin (2.2). Eine Lösung für W1,b1,W 2,b2, die diesen Ausdruck minimiert, ist ein Autoencoder.\\nNachfolgend wird die Verwendung von künstlichen neuronalen Netzen für das Lernen dieser\\nParameter erläutert.\\n2.2 Autoencoder als künstliche neuronale Netze\\nWie bereits in der Einführung erwähnt, ist es möglich, mit einem künstlichen neuronalen Netz\\npraktisch jede Funktion zu berechnen [MP43]. Entsprechend lassen sich auch ein Autoenco-\\nder bzw. die Encoder- und Decoder-Funktionen über neuronale Netze berechnen. Oﬀensichtlich\\nbenötigt der Autoencoder über einem Eingaberaum X⊆Rdjeweils genau dEingabe- und\\nAusgabe-Neuronen.\\nBetrachtetmanFunktionenderimvorangegangenBeispieldeﬁniertenGestalt,ergibtsichalsein-\\nfachster Autoencoder ein schichtweise vollständig verbundenes neuronales Netz mit dEingabe-\\nund Ausgabe-Neuronen sowie einem Hidden Layer mit pNeuronen, wenn pdie Dimension des\\nZwischenraums ist. Ein solches Netz ist in Abbildung 2.1 dargestellt.\\nIn dem dargestellten Netz wurde beispielhaft eine lineare Aktivierungsfunktion gewählt - die\\nkonkrete Wahl der Hyperparameter spielt i.A. keine Rolle hinsichtlich der Deﬁnition eines Au-\\ntoencoders. Ab nun bezeichnen wir mit Autoencoder ein künstliches neuronales Netz, welches\\neine Hintereinanderausführung (zweier oder mehrerer) linearer Funktionen gemäß der mathe-\\nmatischen Deﬁnition berechnet bzw. gelernt hat.\\nUm diese Parameter zu lernen, muss das neuronale Netz trainiert werden. Dazu kann der\\nBackpropagation-Algorithmus verwendet werden, da ein Autoencoder keine Unterschiede zu\\neinem schichtweise vollständig-verbundenen Feedforward-Netz aufweist. Der Trainingsdatensatz\\nElias Saalmann\\n373190252 Autoencoder\\nAbbildung 2.1: Beispiel-Autoencoder KNN [L+15b]\\nist dabei entweder der Eingaberaum selbst oder eine (möglichst repräsentative) Teilmenge des\\nEingaberaums. Das Training hat so zu erfolgen, dass für jedes Eingabedatum xgenau dieses\\nDatumxauch an die Ausgabeschicht gelegt wird. Dies entspricht genau der intuitiven Vor-\\nstellung der Funktion, die der Autoencoder lernen soll. Da die Eingabedaten nicht klassiﬁziert\\nvorliegen müssen, sondern ohne zusätzliches Wissen verarbeitet werden, ist ein solcher Autoen-\\ncoder bzw. das Training und Lernen der Parameter in den Bereich des unüberwachten Lernens\\neinzuordnen.\\nLiegt ein als Autoencoder trainiertes neuronale Netz vor, kann die niedrig-dimensionale Codie-\\nrungeinesEingabedatums xberechnetwerden,indemdasDatumandieEingabeschichtangelegt\\nwird. Die Codierung entspricht dann den pberechneten Werten der Aktivierungsfunktionen für\\njedes Neuron des Hidden Layers (in der Abbildung mit zbezeichnet). Es kann selbstverständlich\\nauch ein Eingabedatum angelegt werden, das nicht Teil des Trainingsdatensatzes war - optima-\\nlerweise ist die Kodierung jedoch so gut gelernt, dass auch für ein solches Datum die Ausgabe\\nder ursprünglichen Eingabe (nahezu) entspricht.\\nDie Anzahl der Hidden Layer eines Autoencoders ist nicht per Deﬁnition beschränkt. So könnte\\ndie Dimension von Schicht zu Schicht schrittweise verringert werden, sodass die Zieldimension\\nperst nach einigen Schichten erreicht wird. Bildlich lässt sich so ein Autoencoder als Trich-\\nter darstellen. Analog kann sich die Erhöhung der Dimension über mehrere Schichten hinweg\\nziehen.\\n2.3 Anwendungen\\nIn den vorangegangenen Abschnitten wurde bereits verdeutlicht, dass ein Autoencoder zur\\nniedrig-dimensionalen Kodierung der Daten eines Eingaberaums verwendet wird. Diese Kodie-\\nrung aller Elemente des Eingaberaums kann auch als Dimensionreduzierung des Raums bzw.\\nder Elemente im Raum bezeichnet werden. Ein bekanntes und weit verbreitetes statistisches\\nElias Saalmann\\n373190262 Autoencoder\\nVerfahren zur Dimensionsreduzierung ist die Hauptkomponentenanalyse (PCA, engl. Princi-\\npal Component Analysis ) [Pea01]. Dabei werden, ähnlich zu Autoencodern, Elemente aus dem\\nEingaberaum in einen niedrig-dimensionalen Raum projeziert. In [HS06] werden verschiedene\\nAnwendungen von Dimensionsreduzierung aufgezeigt und die Ergebnisse von Autoencodern mit\\ndenen einer PCA verglichen. Diese sollen hier als Veranschaulichung einiger Anwendungen von\\nDimensionsreduzierung dienen.\\nDimensionsreduzierung zur Visualisierung Ist die gelernte Codierung 2- oder 3-dimensional,\\nkönnen hochdimensionale Daten durch die Codierung und damit gegebene Einbettung in den\\nR2oderR3visualisiert werden. Abbildung 2.2 zeigt die 2-dimensionale Einbettung des MNIST-\\nDatensatzes, wobei die Kodierungen Amittels einer PCA und Bmithilfe eines Autoencoders be-\\nstimmt wurden. Der MNIST-Datensatz ist ein weit verbreiteter Datensatz, der aus klassiﬁzierten\\nhandgeschriebenen Ziﬀern (0 bis 9) besteht [LCB10]. Eine handgeschriebenen Zahl entspricht\\ndabei einem Graustufen-Bild. Das Graustufen-Bild wird als Vektor dargestellt, in dem jeder Ein-\\ntrag einem Farbwert eines bestimmten Pixels entspricht. Bei einer Bildgröße von 28×28Pixeln\\nist der Vektor also 784-dimensional. Jeder solche Vektor wurde in einen 2-dimensionalen Vek-\\ntor transformiert und entsprechend visualisiert. Die zehn verschiedenen Ziﬀern haben jeweils\\neine eindeutige Kombination aus Symbol und Farbe, sodass sich in Abbildung 2.2 eindeutige\\nUnterschiede hinsichtlich der Separation der Klassen erkennen lassen. Oﬀensichtlich haben die\\n2-dimensionalen Repräsentationen, die mittels eines Autoencoder gelernt wurden ( B), geringe-\\nre Überlappung und deutlichere Abgrenzungen - die Repräsentation ist daher i.A. von höherer\\nQualität als diejenige, die mittels PCA berechnet wurde.\\nAbbildung 2.2: 2D-Codierung von handgeschriebenen Zahlen [HS06]\\nDimensionsreduzierungimKontextderGesichtserkennung ImvorangegangenBeispielwurde\\ndie Qualität der niedrig-dimensionalen Kodierung betrachtet. Von Interesse ist zudem die Qua-\\nlität der Rückkodierung, i.e. der Decoder-Funktion. In [HS06] wurde dazu die Dimension von\\nBildern menschlicher Gesichter reduziert. Dem zugrunde liegt die Tatsache, dass einige wenige\\nMerkmale zur Charakterisierung bzw. Erkennung von Gesichtern genügen. Diese Theorie wurde\\nauf Basis von PCA entwickelt - die Rekonstruktionen der niedrig-dimensionalen Repräsentatio-\\nnen sind als Eigengesichter bekannt geworden [SK87]. Abbildung 2.3 zeigt in der obersten Zeile\\nElias Saalmann\\n373190272 Autoencoder\\ndie Originalgesichter, in der mittleren Zeile die mittels Autoencoder kodierten und anschließend\\nrekonstruierten Bilder und in der untersten Zeile die mittels PCA zurückgerechneten Gesichter.\\nAls Datensatz diente The Database of Faces von den AT&T Laboratories Cambridge [SH94].\\nOﬀensichtlich liefert der Autoencoder auch hinsichtlich der Rekonstruktion der Ursprungsdaten\\nbessere Ergebnisse als eine PCA: Die Gesichter in der mittleren Zeile sind deutlich schärfer\\nund es fällt einem Menschen deutlich leichter, eine Person zu identiﬁzieren. Die mittels PCA\\ngewonnenen Rekonstruktionen lassen nur wenig erkennen.\\nAbbildung 2.3: Rekonstruktion von codierten (30-dimensionalen) Bildern [HS06]\\nDimensionsreduzierung zur Kompression Eine weitere einfache Anwendung von Dimensions-\\nreduzierung ist die Kompression: Wird ein d-dimensionales Datum in ein p-dimensionales Datum\\ntransformiert, benötigt die komprimierte Variante nur einen Anteil vonp\\nddes ursprünglichen\\nSpeicherbedarfs (unter der Annahme, dass der Datentyp einer Dimension gleich bleibt). Opti-\\nmalerweise ist die Komprimierung zusätzlich verlustfrei - dies würde nach Deﬁnition des Auto-\\nencoders einem Fehler von 0 für jedes Datum entsprechen und ist daher i.A. nicht zu erreichen.\\nFür verlustbehaftete Komprimierung ﬁnden sich in der Praxis zahlreiche Anwendung. Betrachte\\nbeispielsweise folgendes einfaches Beispiel:\\nAngenommen, eine Smartphone-App möchte das Datum {x1,x2,...,x d}an einen Server sen-\\nden, allerdings ohne unnötig viel Traﬃc zu erzeugen. Dazu sollte zunächst ein Autoencoder für\\ndie entsprechende Art von Daten trainiert werden. Angenommen ein solcher Autoencoder liegt\\nhinreichend gut trainiert auf Client und Server vor, bietet sich folgende Prozedur zur Traﬃc-\\nReduzierung an:\\n1. Kodiere das Datum {x1,x2,...,x d}auf dem Client mittels der gelernten Encoder-Funktion\\nzu{z1,z2,...,z p}mitp/lessmuchd.\\n2. Sende{z1,z2,...,z p}an den Server\\n3. Dekodiere{z1,z2,...,z p}auf dem Server mittels der vorab gelernten Dekoder-Funktion zu\\n{˜x1,˜x2,...,˜xd}\\nOptimalerweise ähneln sich dann die am Server berechneten ˜xiso sehr den ursprünglichen xi,\\ndass die App ihren Zweck voll erfüllen kann. Ein etwaiger Fehler ist insbesondere bei audiovi-\\nsuellen Rohdaten in gewissem Maße hinnehmbar.\\nElias Saalmann\\n373190282 Autoencoder\\nAutoencoder zur KNN-Initialisierung In der Einführung wurde bereits angesprochen, dass die\\nInitialisierung der Gewichte in neuronalen Netzen zu Beginn des Trainings von hoher Bedeu-\\ntung und nicht trivial ist. Insbesondere ist eine zufällige Initialisierung nicht immer optimal.\\nEine Möglichkeit, die sich als vielversprechend erwiesen hat, beruht auf einer Initialisierung mit\\nAutoencodern [EBC+10].\\nDas Verfahren besteht aus folgenden drei Phasen:\\n1.Pre-Training: In der ersten Phase werden nacheinander alle Schichten (exklusive der\\nAusgabe-Schicht) bearbeitet und jeweils ein Autoencoder mit einem Hidden Layer kon-\\nstruiert. Abbildung 2.4 zeigt im oberen Bereich ein kleines Feed-Forward-Netz, dessen\\nGewichts-Matrizen W1undW2in dieser Phase initialisiert werden sollen. Dazu wird\\nzunächst ein Autoencoder konstruiert, der aus den beiden Eingabe-Neuronen, dem ers-\\nten Hidden Layer und einem temporären Ausgabe-Layer mit zwei Neuronen besteht (in\\nder Abb. unten grün umrahmt). Dort werden nun alle Trainingsdaten an Eingabe- und\\nAusgabe-Neuronen (unüberwacht) angelegt und somit W1undW/prime\\n1gelernt.W/prime\\n1wird ver-\\nworfen und W1für die kommenden Iterationen fest gesetzt. Anschließend wird der nächste\\nAutoencoder konstruiert, der die Gewichte W2der zweiten Schicht unüberwacht lernt (in\\nder Abb. unten rot umrahmt). Die Anzahl an Eingabe- und Ausgabeneuronen muss in\\neinem solchen unüberwachten Autoencoder selbstverständlich übereinstimmen.\\n2.Fine-Tuning 1: In der zweiten Phase werden die Gewichte des letzten Layers trainiert,\\nallerdings ohne einen unüberwachten Autoencoder. Die Gewichte werden (mit den fest\\ngesetzten bereits gelernten initialen Gewichten der vorherigen Schichten) überwacht auf\\nBasis das Trainingsdatensatzes gelernt.\\n3.Fine-Tuning 2: In der ﬁnalen Phase werden das eigentliche Training des Netzes mithilfe\\nvon Backpropagation durchgeführt und die initialen Gewichte schrittweise angepasst.\\nAbbildung 2.4: Autoencoder als Initialisierungsmethode [L+15b]\\nElias Saalmann\\n373190293 Convolutional Neural Networks\\n3 Convolutional Neural Networks\\nDie Verarbeitung von hochdimensionalen natürlichen Daten, wie z.B. Fotos oder Sprachen, er-\\nfreut sich heutzutage in Wirtschaft und Wissenschaft besonders hoher Relevanz. Insbesondere\\nin Anwendungen der künstlichen Intelligenz müssen solche Daten z.B. zur automatischen Spra-\\ncherkennung oder als Grundlage autonomen Fahrens verarbeitet werden. Ein vielversprechender\\nAnsatz für die Verarbeitung solcher Daten sind die Convolutional Neural Networks (CNN) -\\neine spezielle Form von künstlichen neuronale Netzen, deren zugrundeliegende Ideen in diesem\\nKapitel besprochen werden.\\nNachdem im vorangegangen Kapitel ein spezieller Typ von schichtweise vollständig verbunde-\\nnen Netzen eingeführt wurde, werden in diesem Kapitel zunächst lokal verbundene Netze ein-\\ngeführt. Weiterhin wird in Abschnitt 3.2 der mathematische Convolution-Operator eingeführt,\\nder die Grundlage für CNNs bildet. Daraufhin werden Aufbau und Funktion der verschiedenen\\nSchichten eines CNN beleuchtet. Im abschließenden Abschnitt werden Verweise auf populäre\\nCNN-Anwendungen aufgeführt.\\n3.1 Lokal verbundene neuronale Netze\\nSollen hochdimensionale natürliche Daten mit einem künstlichen neuronalen Netz verarbeitet\\nwerden, ergibt sich bei schichtweise vollständig verbundenen Netzen eine sehr hohe Anzahl an\\nKanten bzw. Parametern: Angenommen ein Bild habe 1 Million Pixel, also 1 Million Dimensio-\\nnen. Besteht weiterhin der erste Hidden Layer aus 100.000 Neuronen, ergeben sich bei vollständi-\\nger Verbundenheit 106·105= 1011= 100Milliarden Kanten bzw. Gewichte, die trainiert werden\\nmüssen. Im Kontext von Deep Learning sind weitere Vielfache durch nachfolgende Hidden Layer\\nhinzuzurechnen. Gewichts-Matrizen in solchen Dimensionen erhöhen also i.A. den Trainingsauf-\\nwand.WeiterhinbestehtdieGefahrderÜberanpassung(engl. Overﬁtting )desneuronalenNetzes\\nan den Trainingsdatensatz - ist dieser hinreichend klein, kann man sich dieses Overﬁtting an-\\nschaulich als „auswendig lernen“ vorstellen. Eine triviale Art, die Anzahl zu lernender Parameter\\nzu reduzieren, ist das einfache Weglassen von Verbindungen zwischen Neuronen. Solche Netze\\nwerden als lokal verbundene neuronale Netze bezeichnet [L+15b]. Abbildung 3.1 zeigt ein solches\\nNetz. Ein lokal verbundenes Netz ist weiterhin ein Feedforward-Netz, das mittels Backpropa-\\ngation trainiert werden kann: Nicht vorhandene Verbindungen werden in der Gewichtsmatrix\\ndauerhaft mit 0 belegt.\\nAbbildung 3.1: Lokal verbundenes KNN [L+15b]\\nElias Saalmann\\n3731902103 Convolutional Neural Networks\\n3.2 Der Convolution-Operator\\nConvolutional Neural Networks tragen bereits in ihrem Namen die Bezeichnung Convolution .\\nDamitistdersogenanntediskreteFaltungs-Operator(engl.Convolution)gemeint,welcherhäuﬁg\\nin der Bild- und Signalverarbeitung verwendet wird [Her05]. Da dieser Operator die Grundlage\\nfür CNNs bildet, wird er nachfolgend jeweils für den 1-dimensionalen und 2-dimensionalen Fall\\ndeﬁniert und veranschaulicht.\\n1D-Convolution (Diskrete Faltung) SeiA= (a1,...,a n)ein Eingabevektor. Sei weiterhin F\\nein sogenannter Filtervektor (auch Kernelvektor genannt) mit F= (f1,...,f k)undk<n. Dann\\nist der 1-dimensionale Convolution-Operator ∗wiefolgt deﬁniert:\\nA∗F:= (a∗f)x=k/summationdisplay\\ni=1ax+i−1·fimitx∈{1,...,n−k+ 1}\\nBetrachte zur Veranschaulichung folgende Beispielvektoren für A und F:\\nA=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f010\\n50\\n60\\n10\\n20\\n40\\n30\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fbundF=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01\\n3\\n1\\n3\\n1\\n3\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nOﬀensichtlich ist n= 7undk= 3. Dann hat der Ergebnis-Vektor nach Anwendung des\\nConvolution-Operators auf AundFgenaun−k+ 1 = 7−3 + 1 = 5 Einträge. Gemäß der\\nDeﬁnition ergibt sich:\\nA∗F=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0(a∗f)1\\n(a∗f)2\\n(a∗f)3\\n(a∗f)4\\n(a∗f)5\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0/summationtext3\\ni=1a1+i−1·fi/summationtext3\\ni=1a2+i−1·fi/summationtext3\\ni=1a3+i−1·fi/summationtext3\\ni=1a4+i−1·fi/summationtext3\\ni=1a5+i−1·fi\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f010·1\\n3+ 50·1\\n3+ 60·1\\n3\\n50·1\\n3+ 60·1\\n3+ 10·1\\n3\\n60·1\\n3+ 10·1\\n3+ 20·1\\n3\\n10·1\\n3+ 20·1\\n3+ 40·1\\n3\\n20·1\\n3+ 40·1\\n3+ 30·1\\n3\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f040\\n40\\n30\\n20\\n30\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nAufdenerstenBlickerschließtsichmöglicherweisenichtderSinndervorangegangenveranschau-\\nlichten Convolution-Operation. Oﬀensichtlich überlappt der Filter-Vektor stets einen Teilbereich\\nder Eingabe. Stellt man sich den Eingabevektor Ahingegen z.B. als Messkurve eines elektri-\\nschen Signales vor, könnte das Ergebnis der Faltung bei dieser Wahl von Fz.B. eine Glättung\\ndes Signals verursachen. Extreme Ausreißer nach oben (60) oder unten (10) sind im Ergebnis\\nnicht mehr enthalten. Eine andere Wahl von Fhätte möglicherweise andere Informationen „her-\\naus geﬁltert“. Die Anwendung des Faltungs-Operators auf ein Eingabesignal unter Verwendung\\neines bestimmten Filters ist eine häuﬁge Vorgehensweise in der Signalverarbeitung.\\nElias Saalmann\\n3731902113 Convolutional Neural Networks\\nIm 2-dimensionalen Fall, der nachfolgend deﬁniert und erläutert wird, erschließt sich diese Art\\nvon Verarbeitung noch anschaulicher.\\n2D-Convolution (Diskrete Faltung) SeiA= (aij)∈Kn×neine Eingabematrix. Sei weiterhin\\nF= (fij)∈Kk×kmitk <neine sogenannte Filtermatrix (auch Kernelmatrix genannt). Dann\\nist der 2-dimensionale Convolution-Operator ∗wiefolgt deﬁniert:\\nA∗F:= (a∗f)xy=k/summationdisplay\\ni=1k/summationdisplay\\nj=1a(x+i−1)(y+j−1)·fij\\nmitx,y∈{1,...,n−k+ 1}\\nBetrachte zur Veranschaulichung folgende Beispielmatrizen für A und F:\\nA=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f010 10 10 0 0 0\\n10 10 10 0 0 0\\n10 10 10 0 0 0\\n10 10 10 0 0 0\\n10 10 10 0 0 0\\n10 10 10 0 0 0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fbundF=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01 0−1\\n1 0−1\\n1 0−1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nOﬀensichtlich ist n= 6undk= 3. Dann gilt für das Ergebnis:\\nA∗F∈K(n−k+1)×(n+k+1)=K4×4\\nAus Gründen der Übersichtlichkeit seien nachfolgend nur der erste der insgesamt 16 Einträge\\nder Ergebnismatrix ausführlich berechnet:\\n(a∗f)11=k/summationdisplay\\ni=1k/summationdisplay\\nj=1aij·fij\\n= 10·1 + 10·0 + 10·−1 + 10·1 + 10·0 + 10·−1 + 10·1 + 10·0 + 10·−1\\n= 0\\nAnalog werden die anderen Einträge berechnet - daraus ergibt sich das Gesamtergebnis:\\nA∗F=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f00 30 30 0\\n0 30 30 0\\n0 30 30 0\\n0 30 30 0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nAnschaulichwurdedieFiltermatrixzurBerechnungeinesTeilergebnissesähnlichder1-dimensio-\\nnalen Convolution über Teile der Eingabematrix gelegt. Im vorangegangenen Beispiel wurde F\\nso gewählt, dass die Operation mit FKanten in Aerkennt. Dabei sollte Aals eine Matrix\\nbestehend aus Pixelfarbwerten interpretiert werden, wobei 10 z.B. einen sehr dunklen und 0\\neinen sehr hellen Farbton repräsentiert. Oﬀensichtlich ist in der Ergebnismatrix die Kante in der\\nElias Saalmann\\n3731902123 Convolutional Neural Networks\\nMitte des „Bilder“ erkannt und entsprechen dargestellt worden (betrachte die Einträge mit Wert\\n30). Dies verdeutlicht das Grundprinzip von Bildverarbeitung mittels 2-dimensionaler Faltung.\\nJe nach Wahl der Filtermatrix können neben der Kantenerkennung eine Vielzahl weiterer Eﬀekte\\nerzielt werden, z.B. Glättung, Schärfung oder Weichzeichnung [Her05].\\nIm vorangegangenen Beispiel wurde ein 3×3-Filter gewählt. Aufgrund der Konstruktion des\\nConvolution-Operators ist das Ergebnis der Anwendung eine Filters solcher Größe auf eine n×n-\\nEingabematrix stets kleiner - es entsteht eine (n−2)×(n−2)-Matrix. Unter Umständen kann\\njedoch ein Beibehalten der ursprünglichen Größe gewünscht sein. Um dies zu erreichen, kann\\nein Rand (engl. Padding) um die Eingabematrix hinzugefügt, also die tatsächliche Eingabegröße\\num 1 in jeder Dimension erhöht werden. Das Ergebnis hat dann wiederum die Ursprungsab-\\nmessungen. Welche Werte die Matrixeinträge am Rand einnehmen soll, hängt vom Filter ab -\\ndiverse Lösungsansätze wurden z.B. in der Bildverarbeitung entwickelt.\\nEntgegen der Anforderung, ein Ergebnis mit gleichbleibender Größe zu erhalten, kann auch\\nein deutlich kleineres Ergebnis gewünscht sein. Dies kann beispielsweise der Kompression oder\\nReduzierung der Anzahl zu lernender Parameter dienen. Eine einfache Art, diese Anforderung\\nzu erfüllen, ist die Schrittweite in der Deﬁnition der Faltung zu erhöhen, i.e. den Summenindex\\nin jedem Schritt um mehr als 1 zu erhöhen. Eine solche Art der Faltung wird auch Strided\\nConvolution genannt.\\n3.3 Aufbau von Convolutional Neural Networks\\nNachdem im vorangegangenen Abschnitt der Convolution-Operator ausführlich eingeführt wur-\\nde, wird nun der Aufbau von Convolutional Neural Networks u.a. unter Verwendung von lokal\\nverbundenen neuronalen Netzen und der Idee des Convolution-Operators erläutert.\\nEin Convolutional Neural Network ist ein teilweise lokal verbundenes neuronales Feedforward-\\nNetz, welches zu allermeist aus folgenden Schichten zusammengesetzt ist:\\n1. Convolutional Layer\\n2. Pooling Layer\\n3. Vollständig-verbundener Layer\\nDabei folgt ein Pooling Layer i.A. auf einen Convolution Layer - diese Paarungen sind wieder-\\num mehrfach hintereinander geschaltet. Sowohl Pooling Layer als auch Convolution Layer sind\\nlokal verbundene Teilnetze, d.h. die Anzahl an zu lernenden Gewichten hält sich auch bei sehr\\ngroßen Eingabedaten (z.B. Bildern) in Grenzen. Üblicherweise folgt abschließend ein vollständig-\\nverbundener Layer, dessen Anzahl an eingehenden Neuronen-Verbindungen durch entsprechende\\nKonstruktion der vorangegangenen Layer jedoch deutlich geringer als die Anzahl an Eingabe-\\nNeuronen in der Eingabeschicht ist. Nachfolgend wird detaillierter auf die einzelnen Schichten\\neingegangen.\\nElias Saalmann\\n3731902133 Convolutional Neural Networks\\nConvolutional Layer Wie der Name bereits sagt, ist diese Schicht an die mathematische Fal-\\ntung (Convolution) von Eingabesignalen angelehnt. Der Convolutional Layer bildet dabei den\\nConvolution-Operatorab,d.h.jedesNeuronineinemConvolutionalLayerberechnetgenaueinen\\nEintrag der Ergebnis-Matrix. Dies ist möglich, da die Berechnung der Faltung analog zur Kon-\\nzeption von neuronalen Netzen auf gewichteten Summen basiert. Soll die Filter-Matrix 3×3 = 9\\nEinträge habe, muss folglich jedes der Neuronen im Convolutional Layer mit genau 9 Eingabe-\\nneuronen verbunden sein. Daraus folgt, dass die Schicht nicht vollständig zur Eingabe, sondern\\nlokal verbunden ist. Angenommen die Berechnung an einem Neuron soll auf Grundlage einer\\nfest deﬁnierten Filter-Matrix erfolgen - dann entsprechen die Gewichte der Verbindungen zu\\nder Eingabe genau den Einträgen der Filter-Matrix. Die Hauptidee von Convolutional Neural\\nNetworks ist es nun, dass die Einträge der Filter-Matrix Fnicht vorab feststehen, sondern über-\\nwacht unter Verwendung eines Trainingsdatensatzes für den gewünschten Zweck gelernt werden .\\nDie trainierten Filter bzw. Convolutional Neural Networks können anschließend zur Klassiﬁzie-\\nrung der Eingabesignale genutzt werden, z.B. um für ein beliebiges Bild die Aussage treﬀen zu\\nkönnen, ob ein bestimmtes Objekt zu erkennen ist.\\nBetrachten wir zur Veranschaulichung einen Filter-Vektor Fmit 3 zunächst unbekannten Ge-\\nwichten, der für 1D-Convolution verwendet werden soll:\\nF=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0w1\\nw2\\nw3\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nAngenommen es soll Strided Convolution mit einer Schrittweite von 2 durchgeführt werden.\\nDann lässt sich die Convolution mittels des in Abbildung 3.2 dargestellten Convolutional Layers\\nberechnen.\\nAbbildung 3.2: 1D-Faltung als KNN [L+15b]\\nZur weiteren Verarbeitung wird das Ergebnis der Convolution an jedem Neuron zusätzlich von\\neinerAktivierungsfunktionverarbeitet.OﬀensichtlichhatdasinderAbbildungdargestelltelokal\\nverbundene Netz insgesamt 9 neuronale Verbindungen. Gemäß der Deﬁnition der Convolution\\nwird für die Berechnung des Ergebnisses ein fester Filter verwendet, d.h. für die Gewichte in\\ndiesem Beispiel muss gelten:\\nw1=w4=w7undw2=w5=w8undw3=w6=w9\\nDie Gleichheit der Gewichte ist in der Abbildung farblich gekennzeichnet. Oﬀensichtlich ergibt\\nsich eine zusätzliche Nebenbedingung für das Training von Convolutional Neural Networks: die\\nGleichheit bestimmter Gewichte. Desweiteren ist der Speicherbedarf i.A. geringer, da weniger\\n(unterschiedliche) Gewichte gespeichert werden müssen.\\nElias Saalmann\\n3731902143 Convolutional Neural Networks\\nUm die Nebenbedingung gleicher Gewichte zu erfüllen, genügt schon eine minimale Änderung\\nam Backpropagation-Algorithmus:\\n1. Im Vorwärtsdurchlauf können alle wimit ihren konkreten Werten betrachtet werden, un-\\nabhängig davon, ob manche von ihnen gleiche Werte haben, oder nicht.\\n2. Im Rückwärtsdurchlauf können weiterhin für alle widie Gradienten∂J\\n∂w iberechnet werden.\\n3. Beim Aktualisieren der Gewichte werden diejenigen, die laut Nebenbedingung gleich blei-\\nben müssen (z.B. w1=w4=w7), identisch angepasst. Um den Einﬂuss jedes der Gewichte\\nzu berücksichtigen, wird dazu der Durchschnitt aller betroﬀenen Gradienten gebildet. Es\\nergibt sich dadurch z.B. für w1:\\nw1=w1−α(∂J\\n∂w1+∂J\\n∂w4+∂J\\n∂w7)\\nOﬀensichtlich muss für das Training eines Convolutional Layers nur der letzte Schritt des\\nBackpropagation-Algorithmus angepasst werden.\\nPooling Layer Der Pooling-Layer (auch Subsampling-Layer genannt) besteht aus Neuronen,\\ndie zumeist den Maximum-Operator auf der Menge ihrer Eingabewerte ausführen. Der Operator\\nin dieser Schicht wird i.A. auf das Ergebnis (die Ergebnis-Matrix) eines Convolutional Layers\\nangewendet. Das nachfolgende Beispiel verdeutlicht die Funktionsweise dieses Operators bzw.\\ndieser Schicht: Oﬀensichtlich ist der linke obere Eintrag des Ergebnisses ( 9) das Maximum der\\nvier linken oberen Werte ( {1,3,2,9}) der Eingabe-Matrix.\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f01 3 2 1\\n2 9 1 1\\n1 3 2 3\\n5 6 1 2\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb→MAX/bracketleftBigg\\n9 2\\n6 3/bracketrightBigg\\nDas Beispiel aus dem Abschnitt über den Convolutional Layer fortführend, zeigt Abbildung\\n3.3 einen hinter den Convolutional Layer geschalteten Pooling Layer. Dabei werden aus dem\\n1-dimensionalen Ergebnis der Faltung jeweils zwei Werte im Sinne ihres Maximums zusammen-\\ngefasst.\\nAbbildung 3.3: Beispiel Pooling-Layer [L+15b]\\nElias Saalmann\\n3731902153 Convolutional Neural Networks\\nAn diesem Beispiel wird bereits eine der intendierten Funktionen des Pooling-Layers deutlich:\\nDas Ergebnis einer Convolution wird vergröbert und überﬂüssige Informationen werden op-\\ntimalerweise verworfen. So genügt bei hochdimensionalen Daten oftmals die grobe Informati-\\non, wo genau sich in den Daten ein Feature beﬁndet. Vielmehr ist häuﬁg eine Erkenntnis der\\nForm „Das eingegebene Bild enthält links oben eine rote Ampel“ von Interesse. Die Verwen-\\ndung des Maximum-Operators hat sich in der Praxis als äußert vielversprechend für solche\\nKlassiﬁzierungs-Aufgaben erwiesen. Statt des Maximums kann auch der Durchschnitt gebildet\\nwerden, diese Variante erreicht i.A. allerdings etwas schlechtere Ergebnisse [SMB10].\\nEin weiterer Nutzen des Pooling-Layers ist die Toleranz gegenüber gewissen Verschiebungen in\\nden Eingabedaten - die sogenannte Translationsinvarianz . Die Translationsinvarianz bedeutet,\\ndass bei einer Verschiebung derselben Eingabedaten möglichst die gleiche Ausgabe (im Rahmen\\neiner Klassiﬁkation) erreicht wird. Diese Verschiebung ist im Bereich von Rohdaten wie Ton\\noder Bild durchaus vorhanden - man stelle sich verschiedene Positionen beim Fotograﬁeren oder\\nleichten zeitlichen Versatz bei Tonaufnahmen vor. Ein sehr einfaches Beispiel zur Verschiebung\\nvon Eingabedaten ist in Abbildung 3.4 gegeben: Die einzige 1 in den Eingabedaten (a) wird um\\nzwei Positionen nach rechts verschoben (b) - die Ausgabe (in diesem Fall die 1) bleibt aufgrund\\nderVerwendungdesMax-Operatorsgleich.DieGleichheitderWerteistinderAbbildungfarblich\\n(grün) gekennzeichnet.\\n(a) Originale Eingabe\\n (b) Verschobene Eingabe\\nAbbildung 3.4: Beispiel Translationsinvarianz im Pooling-Layer [L+15b]\\nEine oﬀensichtliche Eigenschaft des Pooling Layers ist, dass keine Gewichte für die neuronalen\\nVerbindungengelerntwerdenmüssen.DasNeuronführtnureineeinfachestatischeFunktionaus,\\ndie sich leicht implementieren lässt. Während des Trainings ist zu beachten, dass im Vorwärts-\\nlauf gespeichert werden muss, welche Pfade im Netz die Maximums-Operationen „gewonnen“\\nhaben. Nur die Gewichte neuronaler Verbindungen, die auf solchen Pfaden liegen, müssen in der\\nAusführung des Backpropagation-Algorithmus aktualisiert werden.\\nVollständig-verbundener Layer Auf den letzten Pooling-Layer in einem Convolutional Neural\\nNetwork folgen häuﬁg einige schichtweise vollständig-verbundene Layer. Zumeist nimmt dann\\ndie Anzahl an Neuronen schrittweise bis zur benötigten Ausgabedimension ab. Diese Schichten\\nsind als klassische Klassiﬁzierung-Schicht nicht-linearer Daten zu verstehen - nicht zuletzt da\\ndie letzte solche Schicht die Ausgabeschicht des gesamten Netzes ist. An diese werden während\\ndes überwachten Trainings die Klassen des Trainingsdatensatzes angelegt.\\nElias Saalmann\\n3731902163 Convolutional Neural Networks\\n3.4 Erweiterungen\\nDer Vollständigkeit halber seien zum Abschluss der Einführung in Convolutional Neural Net-\\nworkszweiErweiterungenderbishervorgestelltenArchitekturgenannt.WiebereitsimAbschnitt\\nüber die mathematische Deﬁnition von Convolution ersichtlich wurde, ist es möglich, sowohl auf\\n1-dimensionalen als auch auf 2-dimensionalen Eingabedaten eine Faltung durchzuführen. Dar-\\nüberhinaus können auch weitere Dimensionen hinzukommen - z.B. im Bereich der Signal- oder\\nBildverarbeitung verschiedene Kanäle. In der Bildverarbeitung können dies z.B. die verschiede-\\nnen Farbkanäle sein, i.e. jeweils ein Kanal für den Rot-, Gelb- und Blauwert. Abbildung 3.5 zeigt\\nein kleines Convolutional Neural Network, welches 1-dimensionale Strided Convolution auf zwei\\nEingabekanälen ausführt. Oﬀensichtlich stellt dies keine nennenswerte Herausforderung bei der\\nKonstruktion eines CNN dar.\\nAbbildung 3.5: Beispiel 2-dimensionales CNN [L+15b]\\nNeben der Verarbeitung verschiedener Eingabekanäle kann ein Convolutional Neural Network\\nauch Abbildungen mehrerer Convolution-Operationen innerhalb einer Schicht beinhalten. So\\nkönnte z.B. eine Operation auf Kantenerkennung und eine andere auf Weichzeichnen trainiert\\nwerden.DieErgebnisseeinersolchenOperationwerdenhäuﬁgals Mapbezeichnet.Abbildung3.6\\nzeigt ein CNN, welches im Convolutional Layer zwei Faltungs-Operationen enthält. Es müssen\\nalso zwei Filter-Vektoren mit jeweils 3 Gewichten gelernt werden.\\nAbbildung 3.6: Beispiel CNN mit zwei Faltungs-Operatoren (Maps) [L+15b]\\nElias Saalmann\\n3731902173 Convolutional Neural Networks\\n3.5 Anwendungen\\nConvolutional Neural Networks der vorangehend erläuterten Gestalt liefern in verschiedensten\\nBereichen hervorragende Klassiﬁzierungs-Ergebnisse. Inbesondere zur Erkennung von handge-\\nschriebenen Zeichen konnten LeCun et. al. in [LBBH98] mit einer Architektur namens LeNet-5\\nsehrguteErgebnisseliefern.DieshatsämtlicheCNN-Architekturenbzw.derenErfolgbegründet.\\nAbbildung 3.7 zeigt diese Architektur. Oﬀensichtlich tauchen alle in dieser Arbeit besproche-\\nnen Schichten (Convolutional, Pooling, vollständig verbunden) sowie die Verwendung mehrerer\\nMaps auf.\\nAbbildung 3.7: Architektur von LeNet-5 [LBBH98]\\nGroßen Erfolg bei der Klassiﬁkation von hoch aufgelösten Bildern mit Hilfe eines CNN konnten\\nKrizhevskyet.al.in[KSH12]erzielen.DieKlassiﬁkationzieltedabeiaufdieErkennungbestimm-\\nter Objekte auf den Bildern ab (Objekterkennung). Die Architektur dieses CNN zeigt Abbildung\\n3.8 - abermals lassen sich alle kennengelernten Komponenten und Konzepte wiederﬁnden.\\nAbbildung 3.8: Architektur Objekterkennung in Bildern [KSH12]\\nNeben der Verarbeitung von Bilddaten erfreuen sich CNN auch im Bereich der Spracherken-\\nnung (Natural Language Processing) hoher Beliebtheit. So werden beispielsweise in [Kim14]\\nnatürlichsprachliche Sätze klassiﬁziert. Populäre praktische Anwendung fand Google Inc. für\\nCNNs unter anderem bei der Implementierung der Smartphone Sprachsteuerung [SP15], bei der\\nSprach-Roh-Daten in maschinenfreundliche Daten umgewandelt werden müssen. Zudem wurden\\nCNNs in der Implementierung von AlphaGo [SHM+16] verwendet.\\nElias Saalmann\\n3731902184 Zusammenfassung\\n4 Zusammenfassung\\nIn dieser Arbeit wurden zwei spezielle Formen künstlicher neuronaler Netze eingeführt.\\nAutoencoder dienen der Dimensionsreduzierung und werden darauf trainiert, eine umkehrbare\\nniedrig-dimensionale Kodierung eines Eingabedatums zu lernen. Die berechneten Einbettungen\\nﬁnden vielerlei praktische Anwendung, z.B. Kompression oder Visualisierung. Im Vergleich zur\\nPCA haben Autoencoder auf Basis von KNN bessere Ergebnisse erzielt. Darüberhinaus werden\\nAutoencoder erfolgreich als Initialisierungsmethode für Gewichte von allgemeinen künstlichen\\nneuronalen Netzen verwendet.\\nConvolutional Neural Networks bestehen grundsätzlich aus Convolutional-, Pooling- und voll-\\nständig verbundenen Schichten. Die Convolutional-Schicht bildet dabei den mathematischen\\nFaltungs-Operator aus der Bild- und Signalverarbeitung ab. Während des Trainings werden die\\nder Faltungs-Operation zugrunde liegenden Filter gelernt. CNNs sind heutzutage state of the\\nartim Bereich der Klassiﬁzierung von hoch-dimensionalen natürlich Daten, z.B. der Objekter-\\nkennung in Bildern.\\nElias Saalmann\\n373190219Literaturverzeichnis\\nLiteraturverzeichnis\\n[Bal12] Pierre Baldi. Autoencoders, unsupervised learning, and deep architectures. In Pro-\\nceedings of ICML Workshop on Unsupervised and Transfer Learning , pages 37–49,\\n2012.\\n[EBC+10] Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal\\nVincent, and Samy Bengio. Why does unsupervised pre-training help deep learning?\\nJournal of Machine Learning Research , 11(Feb):625–660, 2010.\\n[Gau77] Carl Friedrich Gauss. Theoria motus corporum coelestium in sectionibus conicis\\nsolem ambientium , volume 7. FA Perthes, 1877.\\n[Her05] Thorsten Hermes. Digitale bildverarbeitung. Digitale Bildverarbeitung—Eine prak-\\ntische Einführung , pages 134–135, 2005.\\n[HS06] Geoﬀrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data\\nwith neural networks. science, 313(5786):504–507, 2006.\\n[Kim14] Yoon Kim. Convolutional neural networks for sentence classiﬁcation. arXiv preprint\\narXiv:1408.5882 , 2014.\\n[KSH12] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with\\ndeep convolutional neural networks. In Advances in neural information processing\\nsystems, pages 1097–1105, 2012.\\n[L+15a] Quoc V Le et al. A tutorial on deep learning part 1: Nonlinear classiﬁers and the\\nbackpropagation algorithm, 2015.\\n[L+15b] Quoc V Le et al. A tutorial on deep learning part 2: Autoencoders, convolutional\\nneural networks and recurrent neural networks. Google Brain , 2015.\\n[LBBH98] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haﬀner. Gradient-based\\nlearning applied to document recognition. Proceedings of the IEEE , 86(11):2278–\\n2324, 1998.\\n[LCB10] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database.\\nAT&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist , 2, 2010.\\n[MP43] Warren S. McCulloch and Walter Pitts. A logical calculus of the ideas immanent in\\nnervous activity. The bulletin of mathematical biophysics , 5(4):115–133, Dec 1943.\\n[Pea01] K Pearson. On lines and planes of closest ﬁt to systems of points in space, philoso-\\nphical magazine2 (6): 559–572, 1901.\\n[SH94] Ferdinando S Samaria and Andy C Harter. Parameterisation of a stochastic model\\nforhumanfaceidentiﬁcation. In Applications of Computer Vision, 1994., Proceedings\\nof the Second IEEE Workshop on , pages 138–142. IEEE, 1994.\\nElias Saalmann\\n373190220Literaturverzeichnis\\n[SHM+16] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George\\nVan Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam,\\nMarc Lanctot, et al. Mastering the game of go with deep neural networks and tree\\nsearch.nature, 529(7587):484–489, 2016.\\n[SK87] Lawrence Sirovich and Michael Kirby. Low-dimensional procedure for the characte-\\nrization of human faces. Josa a, 4(3):519–524, 1987.\\n[SMB10] DominikScherer,AndreasMüller,andSvenBehnke. Evaluationofpoolingoperations\\nin convolutional architectures for object recognition. In International conference on\\nartiﬁcial neural networks , pages 92–101. Springer, 2010.\\n[SP15] Tara N Sainath and Carolina Parada. Convolutional neural networks for small-\\nfootprint keyword spotting. In Sixteenth Annual Conference of the International\\nSpeech Communication Association , 2015.\\nElias Saalmann\\n373190221', ' \\n \\nFunktionsweise künstlicher neuronaler  \\nNetzwerke: Grundlagen und Programmierung  \\neines Beispiels  \\n \\n \\n \\nElias Krainer  \\n8. Klasse  \\n \\n \\n \\n \\n \\n \\n \\n \\nBetreuer: Mag. Gernot Schatzdorfer  \\nModellschule  Graz , Fröbelgasse 28 , 8010 Graz  \\nFebru ar 2020  \\n 2 \\n Abstract  \\nKünstliche neuronale Netzwerk e sind eine mathematische Nachbildung der menschlichen \\nbeziehungsweise tierischen Reizverarbeitung im Nervensystem. Sie sind selbstlernende \\nAlgorithmen und dadurch ein Zweig der künstlichen Intelligenz, ein Teilbereich der Infor-\\nmatik. Sie haben eine Vielzahl von Anwendungsgebieten . Diese reichen von der Ge-\\nsichtserkennung bis zur Fahrzeugsteuerung.  \\nDie vorliegende Arbeit gibt einen Einblick in den Aufbau und die Funktionsweise von \\nkünstlichen neuronalen Netzen. Anfangs  wird der A lgorithmus theoretisch beleuchtet , \\ndas künstliche neuronale Netzwerk wird aus dem biologischen Vorbild modelliert und ma-\\nthematisch, in Bezug auf Aufbau und Verwendung, betrachtet. Danach  wird ein vielver-\\nwendetes Lernverfahren, das Gradientenabstiegsverfahr en, hergeleitet. In einem weite-\\nren Teil dieser Arbeit wird das künstliche neuronale Netz von der praktischen Seite be-\\nschrieben , indem es in ein Computersystem implementiert wird. Das Programm lernt  er-\\nfolgreich , handgeschriebene Ziffern selbstständig zu klassifizieren. Abschließend werden \\ngesellschaftliche und ethische Probleme, welche mit der Verwendung künstlicher neuro-\\nnaler Netze einher gehen, untersucht.  \\n  3 \\n  \\nInhalt  \\nAbstract  ................................ ................................ ................................ ................................ . 2 \\n1. Einleitung ................................ ................................ ................................ ...........................  5 \\n2. Das Neuron  ................................ ................................ ................................ ........................  6 \\n2.1. Das biologische Neuron  ................................ ................................ ................................ ........  6 \\n2.2. Mathematische Umsetzung  ................................ ................................ ................................ .. 7 \\n2.3. Das Neuron als Klassifizierer  ................................ ................................ ................................ . 9 \\n3. Das künstliche neuronale Netz ................................ ................................ ..........................  13 \\n3.1. Parallele Neuronen  ................................ ................................ ................................ .............  13 \\n3.1.1. Grundlagen  ................................ ................................ ................................ ...................  13 \\n3.1.2. Mathemati sche Definition  ................................ ................................ ...........................  14 \\n3.1.3. Grenzen einlagiger neuronaler Netze  ................................ ................................ ..........  14 \\n3.2. Mehrlagige Neuronen  ................................ ................................ ................................ .........  15 \\n3.2.1. Grundlagen  ................................ ................................ ................................ ...................  15 \\n3.2.2. Wieso ist  eine Aktivierungsfunktion bei mehreren Lagen zwingend notwendig?  ...... 16 \\n3.3. Der Fehler  ................................ ................................ ................................ ............................  16 \\n4. Wie lernt ein künstliches neuronales Netz?  ................................ ................................ .......  18 \\n4.1. Grundlagen  ................................ ................................ ................................ ..........................  18 \\n4.2. Backpropagati on ................................ ................................ ................................ .................  20 \\n5. Simulation am Computer  ................................ ................................ ................................ .. 26 \\n5.1. Wieso Python?  ................................ ................................ ................................ ....................  26 \\n5.2. Wieso die Erkennung handgeschriebener Zahlen?  ................................ .............................  26 \\n5.3. Das Programm  ................................ ................................ ................................ .....................  27 \\n5.3.1. Wichtig e Begriffe  ................................ ................................ ................................ ..........  27 \\n5.3.2. Das Programm und ich  ................................ ................................ ................................ . 27 \\n5.3.3. Struktur und Aufbau  ................................ ................................ ................................ ..... 27 \\n5.4. O ptimierung der Parameter  ................................ ................................ ................................  28 \\n5.4.1. Lernrate  ................................ ................................ ................................ ........................  28 \\n5.4.2. Epochen  ................................ ................................ ................................ ........................  29 \\n5.5. Rückwärtsabfrage  ................................ ................................ ................................ ...............  30 \\n5.6. Datenvervielfältigung  ................................ ................................ ................................ ..........  32 \\n6. Eth ische und gesellschaftliche Probleme künstlicher  neuronaler Netze  .............................  34 \\n6.1. Fehlschlüsse künstlicher neuronaler Netzwerke  ................................ ................................  34 \\n6.1.1. Voreingenommene Daten  ................................ ................................ ............................  34 4 \\n 6.1.2. Ungewohnte Daten  ................................ ................................ ................................ ...... 35 \\n6.2. Artificial Ethics  ................................ ................................ ................................ .....................  35 \\n6.2.1. Grundlagen  ................................ ................................ ................................ ...................  35 \\n6.2.2. Das Trolley Problem  ................................ ................................ ................................ ..... 36 \\n6.2.3. Probleme der Deklaration  ................................ ................................ ............................  37 \\n6.3. Responsible Research and Innovation  ................................ ................................ ................  37 \\n6.4. Zusammenfassung  ................................ ................................ ................................ ..............  38 \\n7. Resümee  ................................ ................................ ................................ ..........................  39 \\nLiteraturverzeichnis  ................................ ................................ ................................ .............  40 \\nAbbildungsve rzeichnis  ................................ ................................ ................................ .........  42 \\nAnhang  ................................ ................................ ................................ ................................  43 \\nMatrizen  ................................ ................................ ................................ ................................ ..... 43 \\nDefinition  ................................ ................................ ................................ ................................  43 \\nSkalare Multiplikation  ................................ ................................ ................................ ............  43 \\nMatrix -Vektor-Multiplikation  ................................ ................................ ................................ . 43 \\nTransponierte Matrix  ................................ ................................ ................................ .............  43 \\nWeiterführende Rechenregeln für Vektoren  ................................ ................................ .............  43 \\nElementweise Multiplikation  ................................ ................................ ................................ . 43 \\nDyadisches Produkt  ................................ ................................ ................................ ................  44 \\nQuellcode  ................................ ................................ ................................ ................................ ... 44 \\n \\n \\n  5 \\n 1. Einleitung  \\nEin Computer ist eine automatisierte Rechenmaschine. Er kann innerhalb von Millisekun-\\nden vielstellige  Zahlen multiplizieren. Im Rechnen und allen Aufgaben, welche sich durch \\nRechenoperationen zusammensetzen lassen, ist er uns Menschen unbestreitbar überle-\\ngen. Doch gibt es eine Vielzahl von Problemstellungen, in welchen uns ein Computer nur \\nschwer bis gar nicht das Wasser reichen kann. Diese Aufgaben reichen von der Gesichtser-\\nkennung bis zur Fahrzeugsteuerung.  \\nUm Computern diese Art der Aufgabenstellungen zugänglich zu machen, wurden künstli-\\nche neuronale Netze entwickelt. Diese sind eine mathematische Nachb ildung der neuro-\\nnalen Reizverarbeitung im Gehirn. Auch wenn sie ein Teilgebiet der künstlichen Intelligenz \\nsind, muss hier klar zu „Intelligenz“ im allgemeinen Sinn differenziert werden. Hier geht \\nes nicht um denkende Maschinen, welche womöglich sogar Gefü hle entwickeln. Nein, \\nkünstliche neuronale Netzwerke sind rein selbständig lernende Algorithmen.  \\nSie sind ein Werkzeug für eine unglaubliche Vielzahl und Diversität an Anwendungsgebie-\\nten. Sie suchen aus, welche Werbung geschaltet wird und handeln an der Bö rse. Sie wer-\\nden in der Überwachung sowie für die Steuerung autonomer Fahrzeuge verwendet.  \\nDas Ziel dieser Arbeit ist, die Funktionsweise von künstlichen neuronalen Netzwerken zu \\nbeschreiben. Diese werden einerseits von ihrer theoretischen Seite beleuchtet , so befasst \\nsich ein wesentlicher Teil dieser Arbeit mit der Modellbildung von biologischen Neuronen, \\nwelche im folgendem optimiert und zu einem neuronalen Netz zusammengefügt werden, \\nsowie mit dem Lernverfahren dieser Netze. Andererseits wird die praktis che Seite eines \\nkünstlichen neuronalen Netzes anhand eines Beispiels dargelegt. Im Rahmen dieser Ar-\\nbeit wird ein Netz lernen, selbstständig handgeschriebene Ziffern zu klassifizieren. Ab-\\nschließend beschäftigt sich diese Arbeit mit gesellschaftlichen und et hischen Problemen, \\nwelche mit der Verwendung dieser Algorithmen einhergehen.   6 \\n 2. Das Neuron  \\n2.1. Das biologische Neuron  \\nKünstliche neuronale Netze sind dem Gehirn von Mensch und Tier nachempfunden. Ihr \\nbiologisches Vorbild definiert sich durch ein Netzwerk  aus Nervenzellen, den Neuronen. \\nAllein ist ein Neuron lediglich ein simples Rechenelement, doch den Netzwerken, welche \\nsie bilden, wird der Grund für die menschliche Intelligenz zugesprochen (vgl. Kinnebrock, \\n1992, S. 11).  \\n \\nAbbildung 1: biologische Neuronen, Quelle: https://de.wikipedia.org/wiki/Datei:Neurons_uni_bi_multi_pseudouni.svg , \\nCC BY -SA 3.0, in Inkscape bearbeitet  \\nEs gibt zwar verschiedene Arten von Neuronen, doch sie funktionieren alle nach demsel-\\nben Prinzip: Elektris che Signale (Reize) werden ausgehend von Rezeptoren (biologische \\nSensoren) oder anderen Neuronen über die Dendriten in das Neuron geleitet. Zwischen \\nden Dendriten des Neurons und den Axonterminalen der zuvor geschalteten Nervenzel-\\nlen befindet sich ein klei ner Spalt, über welchen die Signale chemisch übertragen werden. \\nDiese Verbindung wird als Synapse bezeichnet. Sie hemmt oder stärkt eingehende Sig-\\nnale. Da die Wirkung der Synapse auf eintreffende Reize veränderbar ist, ist die Synapse \\nist bedeutend für das  Lernverhalten eines neuronalen Netzes (vgl. Kinnebrock, 1992, S. \\n15). \\nDas Neuron sammelt alle eintreffen den Reize in einem gemeinsamen Speicher. Überstei-\\ngen diese einen Schwellenwert, schickt es über seine Terminale ein Signal an die folgen-\\nden Neuronen. B iologen sagen dazu: „das Neuron feuert“  \\n7 \\n 2.2. Mathematische Umsetzung  \\nDie Modellierung eines neuronalen Netzes hilft dabei, die menschliche Informationsver-\\narbeitung auf Computern nachzubilden. Allererst werden die einzelnen Elemente eines \\nneuronalen Netzes mathematisch nachgebildet: Die Neuronen.  \\nIn dem mathematischen Modell werden Reize (Signale) durch reelle Zahlenwerte darge-\\nstellt  (vgl. Rashid, 2017, S. 32) . Somit besteht der Eingang des Neurons aus mehreren Zah-\\nlenwerten 𝑎, welche entweder von der Umgebung (dem Input) oder von vorgeschalteten \\nNeuronen kommen.  \\nDie Synapsen, welche die Eingangssignale verstärken bzw. schwächen werden ebenfalls \\ndurch reelle Zahlenwerte modelliert , den Gewichten 𝑤 (vgl. Kinnebrock, 1992, S. 15). \\nDiese werden mit den ei ngehenden Signalen multipliziert. Daraus folgt, dass ein großes \\nGewicht das eingehende Signal vergrößert (stärkt) und ein kleines Gewicht das einge-\\nhende Signal verkleinert (schwächt), wobei ein Gewicht natürlich auch negativ sein kann.  \\nBei diesem mathemati schen Modell eines neuronalen Netzes wird davon ausgegangen, \\ndass alle eingehenden Signale zur selben Zeit eintreffen. Dies erlaubt , den vorhin erwähn-\\nten Speicher, welcher alle eingehende Reize zusammen speichert, als Summe der gewich-\\nteten Eingangssignale zu schreiben (vgl. Kinnebrock, 1992, S. 16). Also werden alle einge-\\nhenden Signale, nachdem sie durch die Synapsen verstärkt oder geschwächt werden, \\nsummiert.  \\nDas momentane Modell lässt sich in folgender Abbildung übersichtlich ablesen, wobei 𝑦 \\nfür die Ausgabe des Neurons steht . \\n \\nAbbildung 2: Vorläufiges Modell einer Nervenzelle, in Inkscape erstellt  \\nDieses lässt sich durch die Summ enfunktion verallgemeinern, wobei 𝑛 für die Anzahl der \\nEingangssignale steht:  \\n8 \\n 𝑦=∑𝑎𝑖𝑤𝑖𝑛\\n𝑖=1 \\nDer Schwellenwert wird mit einer reellen Zahl beschrieben, welcher von der gewichteten \\nSumme der Eingangssignale subtrahiert wird. Somit ist der Output 𝑦 des Neurons erst \\ndann positiv, wenn die eintreffenden Signale größer als der Schwellenwert sind. Jedoch \\nspricht man anstatt vom Schwellenwert meist vom Bias 𝑏, welcher das Negative vom \\nSchwellenwert ist (vgl. Ertel, 2008, S: 244). Also sieht die Formel , welche ein Neuron si-\\nmuliert , so aus:  \\n𝑦=∑𝑎𝑖𝑛\\n𝑖=1𝑤𝑖+𝑏 \\nDie Formel lässt sich auch mithilfe von Vektoren nachbilden, wobei die Summe als Skalar-\\nprodukt eines Vektors aller Eingangssignale 𝑎⃗ und eines Vektors aller Gewichte 𝑤⃗⃗⃗ ge-\\nschrieben wird. Die Beschre ibung eines Neurons durch Vektoren wird bei der Program-\\nmierung noch nützlich sein, da die Eingaben meist in vektorähnlicher Form vorliegen.  \\n𝑎⃗=\\n(  𝑎1\\n𝑎2\\n𝑎3\\n⋮\\n𝑎𝑛)      𝑤⃗⃗⃗=\\n(  𝑤1\\n𝑤2\\n𝑤3\\n⋮\\n𝑤𝑛)       𝑎⃗⋅𝑤⃗⃗⃗=𝑎1𝑤1+𝑎2𝑤2+𝑎3𝑤3+⋯+𝑎𝑛𝑤𝑛  \\n𝑦=𝑎⃗⋅𝑤⃗⃗⃗+𝑏 \\nDiese Formel entspricht der Gleichung einer Geraden mit der Steigung 𝑤 und der Ver-\\nschiebung  𝑏, bei einer einzelnen Eingabe. Gibt es mehrere Eingaben, wird  das Neuron zur \\n(Hyper -) Ebene (vgl. Alpaydin, 2019, S. 280) . Was diese über die Funktion eines Neur ons \\naussagt, wird im nächsten Unterkapitel geklärt . \\nDiese Formel kann weiter vereinfacht werden, indem der Bias zum Skalarprodukt hinzu-\\ngefügt wird. So wird der Bias zum nullten Element des Gewichtsvektors und eine Eins am \\nAnfang des Vektors der Eingangssig nale angehängt. Dies erspart eine separate Behand-\\nlung des Bias für spätere Rechenschritte.  9 \\n 𝑎⃗=\\n(   1\\n𝑎1\\n𝑎2\\n𝑎3\\n⋮\\n𝑎𝑛)   \\n    𝑤⃗⃗⃗=\\n(   𝑏\\n𝑤1\\n𝑤2\\n𝑤3\\n⋮\\n𝑤𝑛)   \\n     𝑎⃗∙𝑤⃗⃗⃗=1𝑏+𝑎1𝑤1+𝑎2𝑤2+⋯+𝑎𝑛𝑤𝑛 \\n𝑦=𝑎⃗⋅𝑤⃗⃗⃗ \\nWie in einem späteren Kapitel noch geklärt wird, darf ein Neuron für mehrla gige neuro-\\nnale Netze nicht linear sein. Deshalb verwendet man meist eine sogenannte Aktivierungs-\\nfunktion, welche die bisherige Formel umschließt. Im Falle einzelner Neuronen wird sie \\nmeist verwendet, um die Ausgabe zu formatieren. Hier wird sie mit 𝑓 beze ichnet.  \\n𝑦=𝑓(𝑎⃗⋅𝑤⃗⃗⃗) \\nEin weit verbreitetes Beispiel für solch eine Aktivierungsfunktion ist die Stufenfunktion. \\nDiese gibt für alle negativen Werte des Neurons 0 aus und für alle positiven 1 (vgl. Rashid, \\n2017, S. 33).  \\n𝑠(𝑥)={1;wenn 𝑥>0   \\n0;andernfalls      \\nIn folgender Grafik wird abschließend das Verhalten eines Neurons bei drei Eingaben \\nübersichtlich zusammengefasst:  \\n \\nAbbildung 3: mathematisches Neuron , in Inkscape erstellt.  \\n2.3. Das Neuron als Klassifizierer  \\nDie Anwendung mathematischer Neuronen besteht meistens in der Lösung sogenannter \\nKlassifizierungsprobleme (vgl. Rashid, 2017, S. 10). Bei Klassifizierungsproblemen handelt \\nes sich um die Unterscheidung verschiedener Klassen. In diesem Zusammenhang wird \\neine Klasse als  Zusammenfassung diverser Objekte bezeichnet (vgl. Alpaydin, 2019, S. 24). \\nZum Beispiel lässt sich eine Klasse Marienkäfer definieren, welche alle Marienkäfer trotz \\n10 \\n unterschiedlicher Größe und Aussehen zusammenfasst. Klassifizierungsprobleme können \\nso einfach sein wie Raupen und Ma rienkäfer voneinander zu unterscheiden, aber auch \\ndie Erkennung handgeschriebener Ziffern oder unterschiedlicher Gesichter können als \\nKlassifizierungsprobleme beschrieben  werden.  \\nUm die Rolle von Neuronen in Klassifizierungsproblemen besser verstehen zu kö nnen, \\nwird folgendes Beispiel angeführt: Angenommen , es gibt einen Roboter , dessen Aufgabe \\nes ist , Raupen von Marienkäfern zu unterscheiden, wobei der Roboter lediglich die Breite \\nund die Länge der gefragten Insekten messen kann. Des Weiteren wird vorausge setzt, \\ndass Raupen sehr lang und schmal sind während Marienkäfer sehr kurz und breit sind. Im \\nfolgenden Diagramm sind unterschiedliche Raupen und Marienkäfer nach ihrer Länge und \\nBreite eingezeichnet. Natürlich sind die Maße der Verständlichkeit halber ein  wenig ver-\\nzerrt dargestellt.  \\n \\nAbbildung 4: Raupen und Marienkäfer, in Inkscape erstellt  \\nIm Folgende n wird die Länge und Breite der Insekten als die Eingaben eines Neurons be-\\ntrachtet. Da zum besseren Verständnis die Aktivierungsfunktion weggelassen wird, defi-\\nniert das Neuron, wie bereits erwähnt, eine Hyperebene. Eine Ebene teilt den Eingabe-\\nraum in zwei T eile. Jenen, bei dem das Neuron ein positives Ergebnis erzeugt, sowie den \\nTeil, für den das Neuron negativ ist. In folgender Abbildung ist dieser Sachverhalt visuali-\\nsiert.  \\n11 \\n  \\nAbbildung 5: Länge und Breite als Eingabe eines Neuron s, in Inkscape erstellt  \\nDurch geschickte Anpassung der Gewichte sowie des Bias lässt sich die durch das Neuron \\ngebildete Grenzgerade so einstellen, dass sie die Größenverhältnisse der Marienkäfer von \\njenen der Raupe trennt. Somit erzeugt das Neuron beispie lsweise für alle Raupen ein po-\\nsitives und für alle Marienkäfer ein negatives Ergebnis. Natürlich sollte der Computer \\nschlussendlich selbst diese Gerade anpassen, jedoch wird das Lernverfahren erst in einem \\nspäteren Kapitel behandelt.  \\n \\nAbbildung 6: Raupen  und Marienkäfer durch ein Neuron klassifiziert, in Inkscape erstellt  \\nIst die Unterscheidung zwischen positiv und negativ immer noch zu unspezifisch, lässt sich \\ndas Ergebnis durch eine Aktivierungsfunktion spezifizieren. Bei Ver wendung der Stufen-\\nfunktion werden die Raupen als Eins und die Marienkäfer als Null gekennzeichnet. \\n12 \\n Möchte man jedoch die Wahrscheinlichkeit haben, dass ein Insekt mit einem gewissen \\nGrößenverhältnis für eine Raupe steht, verwendet man als Aktivierungsfunkt ion die Sig-\\nmoidfunktion, welche eine geglättete Version der Stufenfunktion ist (vgl. Alpaydin, 2019, \\nS. 281). Diese Funktion hat auch noch einen weiteren Vorteil gegenüber der Stufenfunk-\\ntion: Sie ist differenzierbar, das ist für den Lernprozess essenziell.  \\nσ(𝑥)=1\\n1+𝑒−𝑥 \\n \\nAbbildung 7: Sigmoid und Stufenfunktion, in Geogebra erstellt  \\n \\n \\n \\n  \\nKernideen : \\n• Neuronale Netze bestehen aus miteinander verschal-\\nteten Neuronen.  \\n• Künstliche Neuronen werden über eine Hyperebene \\ndefiniert, welche durch eine Aktivierungsfunktion \\n„verzerrt“ wird.  \\n• Ein einzelnes Neuron kann als lineare Trennebene ge-\\nnutzt werden, um zwei Klassen voneinander zu tren-\\nnen.  13 \\n 3. Das künstliche neuronale Netz  \\n3.1. Parallele Neuronen  \\n3.1.1 . Grundlagen  \\nParallele Neuronen werden auch als ein einlagiges künstliches neuronales Netz bezeich-\\nnet. Das sind mehrere Neuronen, welche den Input parallel und unabhängig voneinander \\nverarbeiten. (vgl. Alpaydin, 2019, S. 281) Folgende Grafik verdeutlicht die parallele Daten-\\nverarbeitung der Neuronen.  \\n \\nAbbildung 8: Parallele Neuronen, in Inkscape erstellt  \\nIhre Anwendung besteht in Klassifizierungsproblemen mit mehr als zwei Klassen. Dann \\nwird für jede Klasse ein Neuron verwendet, welches die O bjekte der jeweiligen Klasse von \\ndenen der anderen trennt  (vgl. Alpaydin, 2019, S. 281) . So ist zum Beispiel das Neuron, \\nwelches einer Klasse zugehörig ist, bei Objekten anderer Klassen immer negativ. Der be-\\nschriebene Sachverhalt ist auf folgendem Diagramm  visualisiert.  \\n \\nAbbildung 9: Klassifizierung bei drei Klassen, in Inkscape erstellt  \\n14 \\n 3.1.2 . Mathematische Definition  \\nJedes dieser Neuronen liefert als Output eine explizite Zahl. Diese Zahlen können für eine \\neinfachere Weiterver arbeitung als Vektor zusammengefasst werden. So wäre es doch \\nhilfreich , eine einzige Berechnungsformel für diese Reihe an parallelen Neuronen zu ha-\\nben, welche gleich einen Vektor ausgibt. Diese ist im folgendem dargestellt, wobei 𝑚 die \\nAnzahl der parallelen Neuronen ist.  \\n𝑦⃗=\\n(  f(𝑎⃗⋅𝑤1⃗⃗⃗⃗⃗)\\nf(𝑎⃗⋅𝑤2⃗⃗⃗⃗⃗⃗)\\nf(𝑎⃗⋅𝑤3⃗⃗⃗⃗⃗⃗)\\n⋮\\nf(𝑎⃗⋅𝑤⃗⃗⃗𝑚))   \\nDiese Formel lässt sich weiter vereinfachen. Allererst wird die Aktivierungsfunktion her-\\nausgehoben.  \\n𝑦⃗=f(\\n(  𝑎⃗⋅𝑤1⃗⃗⃗⃗⃗\\n𝑎⃗⋅𝑤2⃗⃗⃗⃗⃗⃗\\n𝑎⃗⋅𝑤3⃗⃗⃗⃗⃗⃗\\n⋮\\n𝑎⃗⋅𝑤⃗⃗⃗𝑚)  ) \\nDas kann man auch als Matrix -Vektor -Multiplikati on schreiben.  Weiterführende Informa-\\ntionen zur Matrix -Vektor -Multiplikation  beziehungsweise Matrizen finden Sie im Anhang.  \\n𝑦⃗=f(W⋅𝑎⃗) \\n3.1.3 . Grenzen einlagiger neuronaler Netze  \\nEinfache sowie parallele Neuronen haben den erheblichen Nachteil, dass sie auss chließ-\\nlich linear trennbare Probleme lösen können. Somit wären folgende Verteilungen nicht \\neindeutig lösbar.  15 \\n  \\nAbbildung 10: Liner unklassifizierbare Klassen, in Inkscape erstellt  \\nDieses Problem lässt sich beheben, indem das Ne tz um mehrere Lagen erweitert wird (vgl. \\nAlpaydin, 2019, S. 288). Somit werden die Ausgaben der ersten Schicht zu den Eingaben \\nder zweiten Schicht. Wie dies im Detail aussieht , wird im folgendem Unterkapitel geklärt.  \\n3.2. Mehrlagige Neuronen  \\n3.2.1 . Grundlagen  \\nMehrlagige künstliche neuronale Netzwerke sind, wie bereits im vorigen Unterkapitel an-\\ngeschnitten, mehrere hintereinandergeschaltete Schichten paralleler Neuronen, welche \\nverknüpft werden, indem der Output der  vorgeschalteten Schicht zum Input  der nächsten \\nSchicht wird (vgl. Alpaydin, 2019, S. 288) . Die zusätzlichen Schichten werden verwendet, \\num nichtlineare Trennflächen  zu schaffen. Somit sind einlagig unlösbare Probleme durch \\nmehrlagige neuronale Netze ohne Weiteres zu lösen. Ein Beispiel fü r solch ein mehrlagi-\\nges neuronales Netz wird in folgender Abbildung visualisiert.  \\n \\nAbbildung 11: Mehrlagiges neuronales Netz, in Inkscape erstellt  \\n16 \\n Bei Verwendung der bisherigen Formeln für die einzelnen Schichten des Netzwerkes lässt \\nsich das oben gezeigte Netz auch folgendermaßen beschreiben, wobei die tiefgestellten \\nZahlen für die Schicht stehen, der die Parameter zugehörig sind:  \\n𝑎⃗2=𝑓(𝑊1⋅𝑎⃗1) \\n𝑎⃗3=𝑓(𝑊2⋅𝑎⃗2) \\n𝑎⃗4=𝑓(𝑊3⋅𝑎⃗3) \\n3.2.2 . Wieso ist eine Aktivierungsfunkti on bei mehreren Lagen zwingend notwendig?  \\nAngenommen es gäbe keine solche Funktion. Ein Neuron ohne Aktivierungsfunktion \\nmacht nichts anderes als eine gewichtete Summe der Eingangswerte. Ist das Neuron in \\nzweiter Schicht, verhalten sich die Eingaben, die es von den vorgeschalteten Neuronen \\nbekommt, linear zum Input des Netzes. Die gewichtete Summe, die das Neuron macht, ist \\njedoch nicht in der Lage diese Linearität zu brechen. Somit verhält sich die Ausgabe des \\nNeurons in zweiter Schicht wiederum wie eine Hyperebene. Damit wäre die weitere \\nSchicht obsolet, da sowieso schon alle möglich en Hyperebenen durch eine Schicht be-\\nschrieben werden können.  \\n3.3. Der Fehler  \\nDer Fehler 𝐸 ist eine Zahl, welche darüber Auskunft gibt, wie inkorrekt ein neuronales \\nNetz eine  Aufgabe löst (vgl. Kinnebrock, 1992, S. 40). Je kleiner der Fehler ist, desto rich-\\ntiger hat das Netz das Problem gelöst. Der Fehler wird durch die Fehlerfunktion 𝐶(𝑎𝐿)𝑡 \\nbeschrieben. Diese vergleicht die Ausgabe des Netzes 𝑎𝐿 mit ihren Soll -Werten  𝑡.  \\n𝐸=𝐶(𝑎𝐿)𝑡 \\nAls Fehlerfunktion wird meist die Summe aus 1\\n2(ist−soll)2 verwendet (vgl. Rashid, 2017, \\nS. 79). Diese Funktion ist stetig und leicht abzuleiten, was für den Lernalgorithmus von \\nVorteil ist.  \\n𝐸=∑1\\n2(𝑎𝑖𝐿−𝑡𝑖)2\\n𝑖 \\n 17 \\n   Kern ideen:  \\n• Parallele Neuronen werden verwendet, um mehr als \\nzwei Klassen voneinander zu trennen.  \\n• Mehrlagige neuronale Netze können auch nicht -line-\\nare Trennflächen erzeugen und somit auch Klassifizie-\\nrungsprobleme lösen, welche linear nicht eindeutig \\nlösbar sind . \\n• Der Fehler ist eine Zahl, welche beschreibt, wie \\n„schlecht“ ein künstliches neuronales Netz ein Prob-\\nlem löst.  18 \\n 4. Wie lernt ein künstliches neuronales Netz?  \\n4.1. Grundlagen  \\nDieses Kapitel beschäftigt sich mit den Lernverfahren von künstlichen neuronalen  Netzen. \\nGrundsätzlich ist vorwegzunehmen, dass man unter „lernen“ lediglich die Optimierung \\nder Parameter (der Gewichte und der Biases) versteht, um ein Problem bestmöglich lösen \\nzu können, also mit minimalem Fehler. In folgendem Kapitel wird der wohl mei st verwen-\\ndete Lernalgorithmus erläutert: Das Gradientenabstiegsverfahren auf Basis von Backpro-\\npagation.  \\nVorweg wird das gesamte  neuronale Netz inklusive  Fehlerfunktion in einer Formel zusam-\\nmengefasst. Das Ergebnis ist  eine Formel, welche  den Fehler des Netzes in Abhängigkeit \\nzum  Input zeigt.  \\n \\nAbbildung 12: Zusammenfassung zu einer Formel, in Incskape und Word erstellt  \\n \\nWobei  𝑊1, 𝑊2, …, 𝑊𝐿 und 𝑦⃗ als Parameter dieser Funktion gesehen werden  \\nDa aber für das Lernverfahren von Bedeutung ist, wie sich der Fehler in Abhängigkeit von \\nden Gewichte n verhält, wird das Ganze umgedreht. Die Gewichte werden zur unabhängi-\\ngen Variablen und der Input wird ein Parameter der Funktion. Formelmäßig ändert sich \\nhierbei nichts, es ist lediglich eine Sache der mathematischen Betrachtung, was als Vari-\\nable und was als Parameter gesehen wird.  \\n𝐸=𝐴𝑁𝑁(𝑊1,𝑊2,…,𝑊𝐿) \\n  \\n19 \\n  \\nIn folgendem Diagramm ist diese Formel schemahaft illustriert. Es zeigt wie sich der Fehler \\nE zu zwei Gewichten, also zwei Elemente aus den Gewichtsmatrizen verhält.  \\n \\nAbbildung 13: Fehler in Abbhängigkeit zu zwei Gewichten, Quelle: https://de.wikipedia.org/wiki/Datei:2D_Wavefunc-\\ntion_(2,1)_Surface_Plot.png , in Inkscape bearbeite t \\nDie Täler dieser Funktion sind Stellen, an denen der Fehler minimal ist. Also die entspre-\\nchenden Werte der Gewichte, an denen das Netz ein Problem optimal lösen kann. Beim \\nLernverfahren eines künstlichen neuronalen Netzes geht es darum, diese Täler zu fi nden. \\nBekanntermaßen  sind die Täler einer zweidimensionalen Funktion einfach zu ermitteln. \\nDoch gestaltet sich dies hier aufgrund der Vielzahl von Dimensionen, die ein neuronales \\nNetz hat, deutlich schwerer. Beim Gradientenabstiegsverfahren nähert man sich  dem Tal \\nSchritt für Schritt (vgl. Rashid, 2017, S. 73). Man startet also an irgendeinen Punkt und \\nändert seine Position in kleinen Schritten talabwärts. Man kann sich das wie bei einem \\nBall vorstellen, der in einer hügeligen Landschaft bergab rollt.  \\nMathe matisch gesehen werden die einzelnen Gewichte mit zufälligen Zahlenwerten be-\\nsetzt. Daraufhin wird jeder dieser Werte so angepasst, dass sich die Gewichte Schritt für \\nSchritt dem Minimum des Fehlers nähern. In folgender Formel ist dieser Sachverhalt für \\nein einzelnes Gewicht mathematisch dargestellt, wobei 𝑢 die Richtung des Tals ist und 𝑙 \\ndie Lernrate, eine Konstante, durch welche sich die Schrittweite einstellen lässt (vgl. \\nRashid, 2017, S. 73).  \\n𝑤𝑛𝑒𝑢=𝑤𝑎𝑙𝑡+𝑢∗𝑙 \\n20 \\n Die Ableitung einer Funktion ist für Abschnitte, in denen die Funktion steigt, positiv und \\nfür jene, in denen die Funktion fällt, negativ. So lässt sich 𝑢 als die negative Ableitung des \\nneuronalen Netzes and Stelle des Gewichtes und in Abhängigkeit zum Fe hler schreiben. \\nSo besteht der finale Schritt darin, die Ableitung des neuronalen Netzes zu finden. Dafür \\nwird der Backpropagation Algorithmus verwendet (vgl. Goodfellow u.a., 2018, S. 225).  \\n4.2. Backpropagation  \\nAls Backpropagation wird der Algorithmus zur  Berechnung der Ableitung eines künstli-\\nchen neuronalen Netzes bezeichnet. Die Formeln hinter dem Algorithmus lassen sich \\ndurch Ableiten über Differenzierregeln herleiten.  \\nBegonnen wird mit der Ableitung an Stelle eines Gewichts in der letzten Schicht: Im F ol-\\ngende n sind die Formel n zur Berechnung der Ausgabe sowie des Fehlers dargestellt, wo-\\nbei diese in drei Teile aufgeteilt ist. Des Weiteren werden hier keine Vektoren oder Mat-\\nrizen verwendet, da diese das Ableiten erschweren. Achtung, hier ist die Benennung  der \\nParameter leicht unterschiedlich: Die Ausgabe des Neurons wird als 𝑎𝑙 bezeichnet. Wobei \\n𝑙 für die Schicht steht, der das Neuron zugehörig ist und 𝐿 steht für die letzte Schicht. \\nSomit ist die Eingabe des Neurons  𝑎𝑙−1, da die Eingabe die Ausgabe der vorgeschalten \\nSchicht ist.  \\n𝐸=1\\n2(𝑎𝐿−𝑡)2+⋯ \\n𝑎𝐿=𝑓(𝑧) \\n𝑧=𝑎𝐿−1𝑤+⋯+𝑏 \\nNun ist die Ableitung aufgrund der Kettenregel einfach zu bilden. Die Ableitung von jedem \\ndieser Terme ist ein Faktor der Gesamtableitung. Das ist in der folgenden Formel darge-\\nstellt. \\n𝑑𝐸\\n𝑑𝑤=𝑑𝐸\\n𝑑𝑎𝐿𝑑𝑎𝐿\\n𝑑𝑧𝑑𝑧\\n𝑑𝑤 \\nDer erste Teil (𝑑𝐸\\n𝑑𝑎𝐿) ist durch die Potenzregel unkompliziert zu differenzieren. Hierbei sieht \\nman, wieso das 1\\n2 in der Kostenfunktion verwendet wird: Es hebt sich mit der zweiten Po-\\ntenz in der Kostenfunktion  auf, welche beim Ableiten heruntergeschrieben wird . Dadurch 21 \\n bleibt  ein konstanter Faktor  erspart . Der folgende Term beschreibt die Ableitung an Stelle \\neines Ne urons und wird auch als Teilfehler bezeichnet.  \\n𝑑𝐸\\n𝑑𝑎=( 1\\n2(𝑎𝐿−𝑡)2+⋯ )′\\n=(𝑎−𝑡) \\nDer zweite Teil ist lediglich die Ableitung der Aktivierungsfunktion. In dieser Arbeit wird \\ndie Sigmoidfunktion verwendet 𝑓(𝑥)= 𝜎(𝑥), da sie einfach zu differenzieren ist, im G e-\\ngensatz zur Stufenfunktion, die zum beispielsweise keine Ableitung hat. Für die Ableitung \\nSigmoid gilt: 𝜎′(𝑧)=𝜎(𝑧)∗(1−𝜎(𝑧)) und da in unserem Fall σ(z)=a ist, kann man \\ndie Ableitung folgendermaßen beschreiben:  \\n𝑑𝑎\\n𝑑𝑧=𝜎′(𝑧)= 𝑎∗(1−𝑎) \\nNun fehlt nur noch der letzte Teil : \\n𝑑𝑧\\n𝑑𝑤=(𝑎𝐿−1𝑤+⋯+𝑏)′ =𝑎𝑙−1 \\nWenn all diese Teile wieder zusammengefügt werden, erhält man die vollständige Ablei-\\ntung.  \\n𝑑𝐸\\n𝑑𝑤=(𝑎−𝑡)∗𝑎∗(1−𝑎)∗𝑎𝑙−1 \\nHierbei darf nicht vergessen werden, dass es sich bei folgender Formel lediglic h um das \\nDifferential eines Gewichts in der letzten Schicht handelt. Für ein Gewicht in einer ande-\\nren Schicht muss ein Ersatz für den Term 𝑑𝐸\\n𝑑𝑎 gefunden werden, da der Fehler nicht mehr \\ndirekt vom Output des Neurons abhängig ist. Ein Beispiel für ein G ewicht, welches sich \\nnicht in letzter Schicht befindet, ist folgendem dreilagigen neuronalen Netz abgebildet. \\nAchtung, hier wird die Schicht, in der sich das Neuron befindet, hochgestellt geschrieben, \\nda die Neuronen einer Schicht tiefgestellt nummeriert w erden.  22 \\n  \\nAbbildung 14: Gewicht in vorletzter Schicht, in Inkscape erstellt  \\nHier sind die Formeln zur Berechnung des Fehlers in Abhängigkeit des Neurons in vorletz-\\nter Schicht. Diese werden gebraucht, um die Ableitung des Netzes, an Stelle der Ausgabe \\ndes Neurons in vorletzter Schicht zu finden (𝑑𝐸\\n𝑑𝑎𝑙). Diese Ableitung wird auch als Teilfehler \\nbezeichnet.  \\n𝐸=1\\n2(𝑎1𝑙+1−𝑡1)2+1\\n2(𝑎2𝑙+1−𝑡2)2 \\n𝑎1𝑙+1=𝑓(𝑧1𝑙+1)       𝑎2𝑙+1=𝑓(𝑧2𝑙+1) \\n𝑧1=𝑎𝑙𝑤1𝑙+1 +𝑏1𝑙+1     𝑧1=𝑎𝑙𝑤2𝑙+1 +𝑏1𝑙+1 \\nDaraus lässt sich folgende Formel für den gesuchten Teilfehler aufstellen:  \\n𝑑𝐸\\n𝑑𝑎𝑙=𝑑𝐸\\n𝑑𝑎1𝑙+1𝑑𝑎1𝑙+1\\n𝑑𝑎𝑙 + 𝑑𝐸\\n𝑑𝑎2𝑙+1𝑑𝑎2𝑙+1\\n𝑑𝑎𝑙 \\nVerallgemeinert lässt sich dies folgendermaßen ausdrücken:  \\n𝑑𝐸\\n𝑑𝑎𝑙=∑𝑑𝐸\\n𝑑𝑎𝑘𝑙+1𝑑𝑎𝑘𝑙+1\\n𝑑𝑎𝑙\\n𝑘  \\nEin zweiter Blick auf diese Formel lohnt sich. Hier steht wie man den Teilfehler eines be-\\nliebigen Neurons (mit Ausnahme jener in letzter Schicht) von de n Teilfehlern der folgen-\\nden Schicht bildet. Das ist der letzte fehlende Puzzlestein zu ein em lernenden neuronalen \\nNetz. Im weiteren Teil wird der Teilfehler als 𝜀𝑙 und nicht mehr als Ableitung (𝑑𝐸\\n𝑑𝑎𝑙) bezeich-\\nnet. \\n23 \\n Als Erstes wird der Term 𝑑𝑎𝑘𝑙+1\\n𝑑𝑎𝑙 im Summenausdruck nach bekannten Mustern aufgelöst.  \\n𝑑𝑎𝑘𝑙+1\\n𝑑𝑎𝑙=𝑑𝑎𝑘𝑙+1\\n𝑑𝑧𝑘𝑙+1𝑑𝑧𝑘𝑙+1\\n𝑑𝑎𝑙 \\n𝑑𝑎𝑘𝑙+1\\n𝑑𝑧𝑘𝑙+1= 𝜎′(𝑧𝑘𝑙+1)=𝑎𝑘𝑙+1∗(1−𝑎𝑘𝑙+1) \\n𝑑𝑧𝑘𝑙+1\\n𝑑𝑎𝑙=(𝑎𝑙𝑤k𝑙+1 +𝑏k𝑙+1)′=𝑤𝑘𝑙+1  \\n 𝑑𝑎𝑘𝑙+1\\n𝑑𝑎𝑙=𝑎𝑘𝑙+1∗(1−𝑎𝑘𝑙+1)∗𝑤𝑘𝑙+1  \\nNun können die Formeln für die Ableitung eines neuronalen Netzes erstmals vollständig \\nformuliert werden.  \\n𝑑𝐸\\n𝑑𝑤𝑙=𝜀𝑙∗𝑎𝑙∗(1−𝑎𝑙)∗𝑎𝑙−1 \\nWenn sich das Neuron in letzter Schicht 𝐿 befindet:  \\n𝜀𝑘𝐿=𝑎𝑘𝐿−𝑦𝑘 \\nSonst:  \\n𝜀𝑙=∑𝜀𝑘𝑙+1∗𝑎𝑘𝑙+1∗(1−𝑎𝑘𝑙+1)∗𝑤𝑘𝑙+1 \\n𝑘 \\nDiese Formeln werden mithilfe von Vektoren  und Matrizen  nachgebildet. Weiterführende \\nInformationen zum Thema Matrizen und Rechenregeln für Matrizen und Vektoren finden \\nSie im Anhang.  \\nBegonnen wird mit der Formel  zur Berechnung des Teilfehlers, der letzten Schicht.  \\n𝜀⃗𝐿=(𝑎1𝐿−𝑦1\\n𝑎2𝐿−𝑦2\\n𝑎3𝐿−𝑦3\\n⋮) =𝑎⃗𝐿−𝑦⃗𝐿 \\nDie Formel zur Berechnung des Teilfehlers der anderen Schichten lässt sich durch die Ver-\\nwendung der elementweisen Vektormultiplikation sowie dem Skalarprodukt folgender-\\nmaßen vereinfachen. 𝑊:𝑘  steht für alle Elemente der Gewichtmatrix in Spal te 𝑘 und ist \\nsomit als Vektor zu sehen. Mit anderen Worten ist der Teilfehler eines Neurons nur von \\nden Gewichten abhängig, die direkt die Ausgabe des Neurons beeinflussen . 24 \\n 𝜀⃗𝑙=\\n(    𝑊:1𝑙+1 ∙[𝜀⃗𝑙+1 ⨀ 𝑎⃗𝑙+1 ⨀ ((1\\n1\\n⋮)−𝑎⃗𝑙+1)]\\n𝑊:2𝑙+1 ∙[𝜀⃗𝑙+1 ⨀ 𝑎⃗𝑙+1 ⨀ ((1\\n1\\n⋮)−𝑎⃗𝑙+1)]\\n⋮ )    \\n \\nWird die Gewichtsmatrix transponiert, lässt sich diese Formel auch als Matrix -Vektor -Mul-\\ntiplikation schreiben.  \\n𝜀⃗𝑙=𝑊𝑙+1𝑇∙[𝜀⃗𝑙+1 ⨀ 𝑎⃗𝑙+1 ⨀ ((1\\n1\\n⋮)−𝑎⃗𝑙+1)] \\nJedes Gewicht in einer Schicht hat eine eigene Kombination von einer Eingabe, die es ver-\\nstärkt beziehungsweise schwächt , sowie einem Neuron, dem es zugehörig ist. Da bei der \\nFormel zur Aktualisierung der Gewichte 𝑎𝑘𝑙−1  für die Eingabe steht und 𝜀𝑙∗𝑎𝑙∗(1−𝑎𝑙)  \\nfür das zugehörige Neuron, wird jedes Gewicht in der Gewichtsmatrix über eine  eigene \\nKombination von diesen Werten aktualisiert. Diese Matrix zum Aktualisieren kann somit \\ndurch die Verwendung des dyadischen Produkts  berechnet werden.  \\n𝑑𝐸\\n𝑑𝑊𝑙=(𝜀1𝑙∗𝑎1𝑙∗(1−𝑎1𝑙)∗𝑎1𝑙−1𝜀1𝑙∗𝑎1𝑙∗(1−𝑎1𝑙)∗𝑎2𝑙−1⋯\\n𝜀2𝑙∗𝑎2𝑙∗(1−𝑎2𝑙)∗𝑎1𝑙−1𝜀2𝑙∗𝑎2𝑙∗(1−𝑎2𝑙)∗𝑎2𝑙−1⋯\\n⋮ ⋮ ⋱)\\n=𝜀⃗𝑙 ⨀ 𝑎⃗𝑙 ⨀ ((1\\n1\\n⋮)−𝑎⃗𝑙) ⨂ 𝑎⃗𝑙−1 \\nDiese Formeln lassen sich noch etwas effizienter gestalten. Beispielsweise wird der Teil-\\nfehler dazu verwendet, die Aktualisierungen der zugehörigen Schicht , sowie den Teilfeh-\\nler der vorgeschalt eten Schicht , zu berechnen. In beiden Fällen wird der Teilfehler noch-\\nmals mit der Ableitung der Sigmoidfunktion multipliziert. Multipliziert man diese gleich \\nzum Teilfehler , erspart man sich einen Rechenschritt pro Schicht. Daraus ergeben sich \\nfolgende akt ualisierten Formeln:  \\n𝑑𝐸\\n𝑑𝑊𝑙=𝜀⃗𝑙 ⨂ 𝑎⃗𝑙−1 \\n𝜀⃗𝐿=(𝑎⃗𝐿−𝑦⃗𝐿) ⨀ 𝑎⃗𝐿 ⨀ ((1\\n1\\n⋮)−𝑎⃗𝐿)  25 \\n 𝜀⃗𝑙=𝑊𝑙+1T∙𝜀⃗𝑙+1 ⨀ 𝑎⃗𝑙 ⨀ ((1\\n1\\n⋮)−𝑎⃗𝑙) \\nIm Wesentlichen beschreiben diese drei Formeln die Ableitung des neuronalen Netzes an \\nStelle der Gewichte in Abhängigkeit zum Fehler des Netzes. Sie stellen den Backpropaga-\\ntion- Algorithmus da, den Kern des Lernverhalten eines neuronalen Netzes. Wie sieht der \\nEinsatz dieser Formeln in der Praxis aus? Allererst rechnet der Roboter ein Trainingsbei-\\nspiel, ganz normal als Input und speichert alle Ausgabewerte jeder Schicht zwischen. Ist \\ndies fertig gerechnet, startet der Roboter den Backpropagation Algorithmus:  Anfangs be-\\nrechnet er den Teilfehler der letzten Schicht. Dieser wird für die Aktualisierung der Ge-\\nwichte in der letzten Schicht verwendet, aber auch um den Teilfehler der vorgeschalteten \\nSchicht zu berechnen.  Der hierbei berechnete Teilfehler wird wieder um verwendet, um \\ndie Gewichte der vorletzten Schicht anzupassen sowie den Teilfehler der vorgeschalteten \\nSchicht zu berechnen. Deshalb wird dieser Algorithmus auf Deutsch auch oft Fehlerrück-\\nführungsmethode  genannt (vgl. Kinnebrock, 1992, S. 41).  \\n \\n \\n Kerni deen:  \\n• Beim Lernverfahren eines künstlichen neuronalen \\nNetzes geht es darum, den Fehler durch Anpassen der \\nGewichte zu minimieren. Also mit anderen Worten \\ngeht es um das Finden des Minimums der Fehlerfunk-\\ntion.  \\n• Dafür wird das Gradientenabstiegsverfahren verw en-\\ndet. Hier nähert man sich dem Tal Schritt für Schritt.  \\n• Die Richtung des Tales gibt die negative Ableitung an. \\nUm diese zu ermitteln wird der Backpropagation Al-\\ngorithmus verwendet.  26 \\n 5. Simulation am Computer  \\nDas folgende Kapitel widmet sich der Implementierung eines neuronalen Netzes in ein \\nComputersystem. Dieses Netz wird im Folgenden darauf trainiert, handgeschriebene Zif-\\nfern richtig zu klassifizieren.  \\n5.1. Wieso Python ? \\nUm das neuronale Netz auf dem Computer umsetzen zu können, wird in dieser Arbeit die \\nProgrammiersprache Python verwendet. Python hat den Vorteil, dass es einfach erwei-\\nterbar ist. So gibt es eine Vielzahl von Modulen, welche zum Beispiel das Rechnen mit \\nMatrizen oder das Erstellen von Graphen erheblich vereinfachen. Es hat  auch eine einfa-\\nche sowie übersichtliche Syntax und ist kostenfrei.  \\n5.2. Wieso die Erkennung handgeschriebener Zahlen?  \\nDie Aufgabe des Netzes ist es , handgeschriebene Ziffern zu erkennen. Das Problem ist \\nunscharf und komplex genug, dass es ein Computer nic ht auf einem herkömmlichen Re-\\nchenweg lösen kann. Hierfür gibt es eine Datenbank, die oft von Forschern im Bereich der \\nkünstlichen Intelligenz verwendet wird, um ihre neusten Ideen auszuprobieren und zu \\nvergleichen. Es gibt eine Liste,  auf der die Ergebniss e von Experten eingetragen sind. So \\nist es immer möglich , die Ergebnisse des in dieser Arbeit erstellten Netze s mit denen von \\nExperten zu vergleichen. Die Datenbank heißt MNIST und beinhaltet einen Trainingsda-\\ntensatz von 60.000 Bildern handgeschriebener Zi ffern, sowie einen Testdatensatz mit \\n10.000 Bildern. All diese sind für Trainingszwecke bzw. Überprüfungszweck e bereits mit \\nder korrespondierenden Ziffer beschriftet (vgl. LeCun u.a., 2013).  Im Folgenden sind drei \\nBilder aus der Datenbank dargestellt.  \\n \\nAbbildung 15: Beispielziffern der MNIST Datenbank, Quelle: http://yann.lecun.com/exdb/mnist/ , in Python visualisiert  \\n27 \\n 5.3. Das Programm  \\n5.3.1 . Wichtige Begriffe  \\nBevor das Programm selbst erklärt werden kann, müssen ein paar zentral e Begriffe klar-\\ngelegt werden.  \\nVariablen  stehen in der Informatik für kleine Speicherplätze, in die man Objekte wie unter \\nanderem eine Zahl, ein Text oder einen Vektor speichern kann.  \\nFunktionen  sind in der Informatik ähnlich wie in der Mathematik: Für Eingaben wird eine \\nAusgabe erzeugt. Nur, dass in der Informatik Funktionen nicht nur über mathematische \\nAusdrücke definiert werden können, sondern auch über jegliche andere Programmab-\\nschnitte. Diese werden beim Aufrufen der Funktion ausgefü hrt. \\nEine Klasse  ist der „Bauplan“ eines Objektes. So lassen sich zum Beispiel aus der Klasse \\n„Hund“ unterschiedliche Hunde (Objekte) ableiten. Die Hunde haben zwar unterschiedli-\\nche Eigenschaften, aber die gleichen grundlegenden Fähigkeiten (bellen, laufen , etc.). In \\nder Informatik werden die Eigenschaften durch Variablen beschrieben und heißen „Attri-\\nbute“. Die Fähigkeiten werden durch Funktionen dargestellt und heißen Methoden.  \\n5.3.2 . Das Programm und ich  \\nIch bin bei dem Erstellen des Programms anfangs uns trukturiert vorgegangen. Ich begann \\nmit einer Hauptklasse, welcher ich nach und nach um die Attribute  und Methoden  eines \\nkünstlichen neuronalen Netzes erweiterte. Durch den unstrukturierten Programmierpro-\\nzess und stundenlangen Fehlersuchen ist ein großes C haos entstanden, so dass ich ent-\\nschied das Ganze in einer neuen Datei noch einmal schön und strukturiert zu schreiben. \\nIn diesem Schritt wurde herausgestrichen, zusammengefasst, umbenannt und aufgespal-\\nten. Der Informatiker würde dazu sagen: „Das Programm w urde aufgeräumt.“ Zum \\nSchluss wurde im Programm selbst, das Programm beschrieben (es wurde kommentiert), \\ndass nicht nur ich selbst wieder verstehe was ich da geschrieben habe, sondern auch jeder \\nandere Informatikaffine Mensch soll das Programm verwenden un d begreifen können.  \\n5.3.3 . Struktur und Aufbau  \\nWie beschrieben besteht das Programm hauptsächlich aus einer Klasse. Diese heißt ANN , \\ndas ist kurz für „artifical neural network“. Im Folgenden sind die Attribute und Methoden \\ndieser Klasse aufgelistet.  28 \\n • Attrib ute: \\no weights : Eine Liste aller Gewichtsmatrizen des Netzes  \\n• Methoden:  \\no __init__ : Wird beim Ableiten eines Objektes aufgerufen, hier werden die \\nGewichstmatrizen deklariert.  \\no run: Lässt das Netz für einen Eingabewert durchlaufen  \\no learn : Algorithmus für das Lernverfahren des Netzes  \\no eval_accuracy : Berechnet die Genauigkeit des Netzes.  \\no feed_backward : Rückw ärtsabfrage (wird später erklärt)  \\no sigmoid : Sigmoidfunktion  \\no backprob : Backpropagation Algorithmus  \\nDie ganzen Funktionen arbeiten hauptsächlich mit Vektoren und Matrizen als Variablen-\\ntypen. Dies erlaubt ein sehr reduziertes Programm sowie die unveränderte Verwendung \\nder hergeleiteten Formeln. Da Python von selbst keine Vektoren - beziehungsweise Mat-\\nrizen rechnung unterstützt, wird das Modul Numerical Python verwendet.  \\nZudem gibt es im Programm, neben der Klasse KNN , noch zwei weitere Funktionen. Ei-\\nnerseits format_mnist  und andererseits main . format_mnist wird verwendet, um die \\nMNIST Datensätze zu importier en und in das erforderliche Format zu bringen. Da aber \\nKlassen und Funktionen irgendwo aufgerufen werden müssen, gibt es die Funktion main . \\nDiese startet die beschriebene Klasse und Funktion unter deklarierten Parametern. Die \\nFunktion main  startet von alle in, wenn das Programm aufgerufen wird.  \\n5.4. Optimierung der Parameter  \\nDer Lernalgorithmus verwendet verschiedene Parameter, welche im Vorhinein zu \\ndeklarieren sind. Dieses Unterkapitel beschäftigt sich mit dem Finden der optimalen \\nWerte für diese Parameter . \\n5.4.1 . Lernrate  \\nIn folgendem Diagramm ist die Trefferquote des selbstprogrammierten Netzes für \\nunterschiedliche Lernraten abgebildet. Man kann erkennen, dass zu kleine oder zu große 29 \\n Lernraten zu schlechten Ergebnissen führen. Das ist auch logisch, da man  durch kleine \\nLernraten zu kleine Schritte macht und damit die Geschwindigkeit des Gradienten -\\nabstieges begrenzt. Auf der anderen Seite führen zu große Lernraten zu einem Pendeln \\num das Minimum, da die Schritte immer über das Minimum hinausschießen. Die op timale \\nTrefferquote für das beschriebene Problem erreicht eine Lernrate von 0.3.  \\n \\nAbbildung 16: Genauigkeit und Lernrate, in Word erstellt  \\n5.4.2 . Epochen  \\nEine bessere Genauigkeit kann erzielt werden, wenn der Lernprozess mehrmals wieder-\\nholt wird. Die Anzahl dieser Wiederholungen nennt man Epochen. In folgenden Diagram-\\nmen sieht man, dass die Trefferquote anfangs beachtlich steigt , aber dann konstant \\nbleibt. Der bleibende Fehler kommt von Ungenauigkeiten wie ein Pendeln um das Mini-\\nmum, uneindeutigen Testbildern oder durch schlicht eine zu kleine Anzahl an Trainings-\\nbeispielen . 0,940,9450,950,9550,960,965\\n0,05 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9Genauigkeit\\nLernrateGenauigkeit und Lernrate30 \\n  \\nAbbildung 17: Genauigkeit und Epochen, in Word erstellt  \\n5.5. Rückw ärtsabfrage  \\nWas passiert eigentlich in einem  neuronalen Netz? Es lernt , selbstständig große Datens-\\nätze richtig zu klassifizieren, doch die selbstgebildeten Regeln, nach denen das Netz vor-\\ngeht, sind von außen nicht einsehbar. Das ist eigentlich kein Problem, wenn man lediglich \\nan Antworten interessie rt ist, doch ist es beispielsweise  für Überprüfungszwecke vorteil-\\nhaft, einen Einblick in das Gedächtnis eines neuronalen Netzes zu bekommen.  \\nHier kommt die Rückw ärtsabfrage ins Spiel. Die Idee ist, dass man, anstatt eine Eingabe \\nvon vorne nach hinten durch zugeben, eine gewünschte Ausgabe von hinten nach vorne \\ndurch das Netz schleust  (vgl. Rashid, 2017, S. 170) . Die Ausgabe kann zum Beispiel für die \\nZiffer Null stehen . Leitet man diese von hinten nach vorne durch das Netz , erhält man \\nfolgendes Bild.  0,9450,950,9550,960,9650,970,9750,98\\n1 2 3 4 5 6 7 8 9Genauigkeit\\nAnzahl der EpochenGenauigkeit und Epochen31 \\n  \\nAbbildung 18: Rückw ärtsabfrage der Ziffer Null, in Python visualisiert  \\nDieses Bild ist das Bild , welches das Netz bestmöglich als Null klassifizieren kann. Also \\nsozusagen für das Netz das Idealbild einer Null. Daraus lässt sich so einiges ablesen:  \\n• Deckt die Spur des Stiftes am Eingabebild , die hellen Stellen in diesem Bild ab , so \\nkann das Netz das Bild bestmöglich als Null klassifizieren. Das ist ja auch ganz lo-\\ngisch , da die hellen  Stellen im Kreis einer Null verlaufen.  \\n• Die dunklen  Stellen müssen an einem Eingabebild freibleiben , um bestmöglich als \\nNull erkannt zu werden. Diese befinden sich hauptsächlich in der Mitte des Ringes, \\nwo bei einer Null natürlich nichts ist.  \\n• Das Netz steht den grauen  stellen gleichgültig gegenüber . Diese beeinflussen die \\nAusgabe kaum.  \\nIm Folgenden sind auch die Bilder aller anderen Ziffern ebenfalls über die Rückw ärtsab-\\nfrage konstruiert.  \\n32 \\n  \\nAbbildung 19: Rückw ärtsabfrage aller Ziffern, in Python visualisiert  \\nAuf den Bilde rn mancher Ziffern lassen sich die ursprünglichen Ziffern über helle Bereiche \\nnoch erkennen. Während sich das Netz bei den anderen mehr auf die Bereiche \\nkonzentriert hat, die frei bleiben müssen. Das ist auch in Ordnung, schließlich funktioniert \\nes und das  Netz erkennt die Ziffern mit einer Genauigkeit von über 95%  \\n5.6. Datenvervielfältigung  \\nDie zuvor generierten Bilder sind das Gedächtnis des neuronalen Netzes. Sie sind aus dem \\nLernen von 60 000 Trainingsdaten entstandenen. Erst eine große Anzahl an Daten \\nermöglich t dem Netz, Bilder mit solch einer Genauigkeit zu erkennen (vgl. Rashid, 2017, \\nS. 174). Will man die Genauigkeit erhöhen, gibt es vielerlei Tricks, die einerseits eine \\ninterne Ungenauigkeit des Netzes ausgleichen, wie die Anpassung der Lernrate, oder die \\nTrainingsdaten besser ausnützen, wie das Lernen in Epochen. Doch meist hilft kaum etwas \\nmehr als das Lernen mit mehr Trainingsdaten.  \\nDoch woher bekommt man diese Daten? Selbst Zehntausende von Ziffern zu schreiben \\nund zu scannen ist sehr mühsam und zeitaufwe ndig. Da wäre es doch praktisch, wenn es \\neinen Weg gäbe, aus den bereits vorhandenen Daten neue zu generieren. Den gibt es. Ein \\nAnsatz dafür ist einfach das Drehen der Trainingsdaten um einen kleinen Winkel nach vor \\nsowie zurück (vgl. Rashid, 2017, S. 174) . Dadurch bleibt die Zahl im Wesentlichen gleich, \\njedoch ändern sich die Pixeldaten für den Computer.  \\nDie Bilder werden 1 0° nach links sowie 1 0° nach rechts gedreht. Dadurch hat man \\ninsgesamt die dreifache Menge an Trainingsdaten , und dank der vielen Erwe iterungen \\n33 \\n von Python ist das sehr einfach und schnell getan.  Als Beispiel ist hier das Bild einer Eins \\ninklusive der gedrehten Variationen.  \\n \\nAbbildung 20: Eine Eins der MNIST Datenbank inklusive gedrehter Variationen, Quelle: \\nhttp://yann.lecun.com/exdb/mnist/ , in Python bearbeitet  \\nDas Ergebnis kann sich sehen lassen. Das trainierte Netz erreicht eine Genauigkeit von \\n98,89% bei 30 Epochen. Das Netz hatte drei Schichten: Zwei mit 500 Neuronen und eine \\nAusgabeschicht mit 10 Neuron en. Es kann mit den Ergebnissen von Experten mithalten, \\ndessen dreilagige neuronale Netze auf der Liste der MNIST Datenbank auf ähnliche \\nGenauigkeiten kommen (vgl. LeCun u.a., 2013).  \\n \\n \\n  \\nKernideen:  \\n• Das Programm besteht aus einer Hauptklasse, welche \\num die Attribute und Methoden eines künstlichen \\nneuronalen Netzes erweitert wurde.  \\n• Um einen kleinen Einblick in das Gedächtnis eines \\nneuronalen Netzes zu bekommen, lässt sich über die \\nRückwärtsabfrage berechnen, welche Eingaben das \\nNetz für ein bestimmtes Erg ebnis negativ bzw. positiv \\nbeeinflussen.  \\n• Durch Rotieren der Bilder lassen sich die Trainingsda-\\nten vervielfachen, was zu einer besseren Genauigkeit \\ndes Netzes führt.  \\n 34 \\n 6. Ethische und gesellschaftliche Probleme künstlicher  \\nneuronaler Netze  \\nKünstliche neuronale Netze werden in vielen Bereichen eingesetzt, welche eine direkte \\nAuswirkung auf Menschen haben. Zum Beispiel entscheiden sie , ob Kredite vergeben wer-\\nden und beurt eilen die Angeklagte beziehungsweise den Angeklagten vor Gericht  (vgl. \\nKrumay, 2018, S. 142) . Auch wenn sie offiziell meist nur als „unverbindliche Unterstüt-\\nzung“ eingesetzt werden, vertrauen viele Menschen diesen Maschinen  (vgl. Spiekermann, \\n2016) . Das Pr oblem ist, dass die eingesetzten Systeme meist eine „Blackbox“ sind  (vgl. \\nCastelvecchi, 2016) . So weiß der Anwender und auch oft der Programmierer nicht , aus \\nwelchen Kriterien das Netz seine Schlüsse gezogen hat. So ist es oftmals schwer heraus-\\nzufinden, ob  das Netz falsche Schlüsse zieht oder bestimmte Personengruppen bevorzugt. \\nDes Weiteren ist es kaum nachvollziehbar, ob und wie sich ein künstliches neuronales \\nNetz über bestimmte Eingaben gezielt manipulieren lässt  (vgl. Castelvecchi, 2016) . Wie \\nsich küns tliche neuronale Netze irren können, ist in folgendem Unterkapitel angeschnit-\\nten. \\n6.1. Fehlschlüsse künstlicher neuronaler Netzwerke  \\n6.1.1 . Voreingenommene Daten  \\nWissenschaftler der Universität von Washington wollten herausfinden, wieso ihr künstli-\\nches neu ronales Netz ein Bild eines Huskys als Wolf erkennt. Sie konnten rekonstruieren , \\nnach welchen Teilen des Bildes das System geschlossen hat, als es das Bild klassifizierte , \\nund kamen zum Schluss, dass das Netz ausschließlich nach dem Schnee im Hintergrund \\ndes Bildes geurteilt hat. Das Problem war, dass fast alle Bilder von Wölfen, nach denen \\ndas Netz gelernt hat, in einer verschneiten Umgebung aufgenommen wurden. So hat das \\nNetz von Schnee im Hintergrund auf einen Wolf geschlossen.  (vgl. Ribeiro  u.a., 2016)  Ein \\nähnliches Problem gibt es in den Gesichtserkennungssystemen von Microsoft und IBM. \\nDiese erkennen die Gesichter von Männern mit niedrigerer Fehlerquote als die von \\nFrauen , und Personen mit heller Hautfarbe werden am besten erkannt  (vgl. Tilly, 2018, S.  \\n139) . Dies kommt  höchstwahrscheinlich daher, dass mit einem Datensatz gelernt wurde, \\nin dem viele männliche Gesichter mit heller Haut waren. Ein Problem wird dies aber dann, \\nwenn neuronale Netze über eine Kreditvergabe entscheiden oder Angeklagte in Geric hts-\\nprozessen beurteilen. Zum Beispiel wird in amerikanischen Gerichten die Software 35 \\n COMPAS verwendet. Diese beurteilt wie wahrscheinlich es ist, ob Verurteilte wieder eine \\nStraftat begehen. Eine Studie fand heraus, dass diese doppelt so wahrscheinlich fäls chli-\\ncherweise Menschen mit dunkler Hautfarbe als risikoreicher einstuft  (vgl. Angwin u.a., \\n2016 ). Solch Fehlverhalten wurde nicht absichtlich von böswilligen Programmierern pro-\\nvoziert. Das Problem ist, dass künstliche neuronale Netze zum Lernen sehr viele Daten \\nbrauchen So ist es beinahe unmöglich, einen Überblick über ihre Qualität zu behalten.  \\n6.1.2. Ungewohnte Daten  \\nFast alle Modelle neuronaler Netze sind sehr instabil gegenüber ungewohnten Mustern \\noder Rauschen  (vgl. Tilly, 2018, S. 139) . Wird zum Beispiel über ein Bild ein bestimmtes \\nRauschen gelegt, so wird es komplett falsch klassifiziert , obwohl ein Mensch beinahe kei-\\nnen Unterschied merken würde. Des Weiteren  versuchen selbst hochentwickelte Sys-\\nteme in eine Eingabe eine Antwort zu interpretieren. So wird eine schwarz/grau gemus-\\nterte Fläche mit hoher Sicherheit als Königspinguin klassifiziert und eine rot/weiß ge-\\nstreifte als elektrische Gitarre  (vgl. Nguyen u.a. , 2015) . Das mag vielleicht nach harmlosen \\nVerwec hslungen klingen, doch lassen sich diese Schwachstellen auch manipulativ für ge-\\nzielte Missinterpretationen einsetzen. Ein Beispiel hierfür wäre, dass Computerwissen-\\nschaftler ein  unhörbares Rauschen über eine  Audiobotschaft gelegt haben , wodurch \\nSpracherken nungssysteme diese Botschaft ganz anders auffassen als Menschen  (vgl. \\nCisse u.a., 2017) . So ist es möglich , bei jemanden, der einen Sprachassistenten besitzt, \\nanzurufen und eine Botschaft auf den Anrufbeantworter zu sprechen. Für den Menschen \\nmöge sich die se wie ein Gruß anhören, aber sie ist so manipuliert, dass der Sprachassis-\\ntent diese als Bestellung interpretiert.  \\n6.2. Artificial Ethics  \\n6.2.1 . Grundlagen  \\nSelbstlernende Maschinen handeln immer mehr selbstständig und unabhängig von Men-\\nschen. Vor allem bei  der autonomen Fahrzeugsteuerung kann es zu Situationen kommen, \\nin denen ein Auto Entscheidungen um Leben und Tod treffen muss. Man hofft dabei auf \\ndie Entwicklung eines allgemeinen Moralkodexes für Maschinen  (vgl. Krumay, 2018, S. \\n142) . In diesem Zusammen hang werden oftmals die Asimovschen Gesetze zitiert. Diese \\nwurden von einem Romanautor erstellt und sind selbstverständlich nicht für die Grund-\\nlage einer Roboterethik gedacht. Dennoch werden sie oft als dessen Basis gesehen.  36 \\n 0. Ein Roboter darf der Mensch heit keinen Schaden zufügen oder durch Untä-\\ntigkeit zulassen, dass der Menschheit Schaden zugefügt wird.  \\n1. Ein Roboter darf einem menschlichen Wesen keinen Schaden zufügen oder \\ndurch Untätigkeit zulassen, dass einem menschlichen Wesen Schaden zu-\\ngefügt wird . \\n2. Ein Roboter muss Befehlen gehorchen, die ihm von Menschen erteilt wer-\\nden, es sei denn, dies würde gegen das erste Gesetz verstoßen.  \\n3. Ein Roboter muss seine eigene Existenz schützen, solange solch ein Schutz \\nnicht gegen das erste oder zweite Gebot ve rstößt.  (Scholtysek , 2015)  \\nDiese Regeln sind jedoch viel zu unzureichend und eindimensional, um die menschliche \\nMoral abzubilden. Im Folgenden wird ein weit verbreitetes Beispiel präsentiert, welches \\nkomplexere Regeln verlangt.  \\n6.2.2 . Das Trolley Problem  \\nSchon in den 1930er Jahren skizzierte Philippa Foot das „Trolley Problem“. Es ist ein Ge-\\ndankenexperiment, bei dem eine unhaltbare Straßenbahn auf eine Gruppe von Menschen \\nzurast, jedoch kann der Weichensteller die Weiche umlegen, sodass die Straßenbahn der \\nMenschenmenge ausweicht. Doch auf der Ausweichstrecke befindet sich auch eine Per-\\nson, die in diesem Fall sterben würde. Dieses Problem kann als „moralisches Dilemma“ \\nbezeichnet werden, da in beiden Fällen, die Weiche wird umgelegt oder nicht, eine Regel \\naus dem Moralkodex gebrochen wird. Einerseits lässt man die Menschen durch Nichtstun \\nsterben und andererseits bringt man den anderen Menschen aktiv um. Dieses Gedanken-\\nexperiment lässt sich auch auf intelligente Maschinen neu definieren im Kontext des au-\\ntomat isierten Fahrens. Das Grundproblem ist folgendes: Ein selbstfahrendes Auto rast \\nunhaltbar auf eine Betonbarriere zu. Im Falle des Nichtstuns würde der Fahrzeuginsasse \\nums Leben kommen. Es kann ihr aber auch ausweichen, jedoch würde das Fahrzeug dabei \\nMensc hen überfahren, welche die Straße auf einem Zebrastreifen überqueren. Das MIT \\nuntersucht dieses Problem, in dem sie die Daten von Menschen sammelt, welche auf der \\nWebsite der sogenannten „Moral Machine“ selbst urteilen können  (vgl. Krumay, 2018, S. \\n144). \\nEs gibt verschiedene Lösungsansätze zum Trolley Problem von selbstfahrenden Fahrzeu-\\ngen, welche sich ebenfalls über Regeln in eine Maschine implementieren lassen können. 37 \\n Ein weit umstrittener Ansatz ist , immer den Fahrzeuginsassen zu schützen. Diese Forde-\\nrung steht im Gegensatz zu dem generellen Schutz der Unschuldigen. Weitere Diskussio-\\nnen gibt es über eine Entscheidung je nach Wert der Personen für die Gesellschaft.  (vgl. \\nKrumay, 2018, S. 14 5) \\n6.2.3 . Probleme der Deklaration  \\nHier stellt sich die Frage, ob sich unsere Moral eindeutig in Regeln ableiten lässt und ob \\nes damit einen Moralkodex für Maschinen überhaupt gibt, denn dieser muss auf einem \\nallgemein anerkannten Moralkodex basieren  (vgl. Krumay, 2018, S. 14 5). Wir Menschen \\nleiten unsere Moral im Laufe unseres Lebens von der umgebenden Gesellschaft ab. Doch \\nwäre dies für Maschinen nicht denkbar, da man kaum einen Einfluss auf die Qualität der \\nQuellen, aus denen die Maschine lernt, hat. Des Weiteren kann man den  Lernprozess ei-\\nner Maschine nicht mit dem eines Kindes vergleichen. Einerseits lernen Maschinen unre-\\nflektiert, was zu lernen vorgesehen ist, und andererseits geht von ihnen eine ganz andere \\nGefahr aus. So stellen Sie sich eine Drohne oder ein Auto vor, das  erst lernen muss, was \\nrichtig oder falsch ist.  \\nHier muss meines Erachtens noch etwas eingeschoben werden. In diesem Unterkapitel \\nwurde über einen Moralkodex für Maschinen gesprochen. Dieser darf aber nicht mit ei-\\nnem „Moralempfinden“ verwechselt werden. Ak tuelle selbstlernende Systeme sind nicht \\nintelligent, sie handeln lediglich auf Basis von Daten, durch welche sie optimiert wurden. \\nEin Moralkodex kann entweder ein Datensatz von Situationen sein, aus denen das System \\nlernt, oder klar formulierte Regeln, n ach denen es handelt.  \\n6.3. Responsible Research and Innovation  \\nKünstliche neuronale  Netze (und im weiteren Sinn „künstliche Intelligenz“ ) ist ein Bereich \\nmit viel Zukunft und Potenzial. Um die Forschung verantwortungsvoll zu betreiben, hat \\ndie Europäische  Union einen Standard veröffentlicht, nach dem sich Universitäten in ganz \\nEuropa richten. Das Ziel ist es Forschung frei zugänglich zu machen und Fehlentwicklun-\\ngen vorzubeugen.  Das Modell heißt Responsible Research and Innovation oder kurz RRI.  \\n(vgl. Kruma y, 2018, S. 14 5) Der Ansatz gliedert sich in vier Prozessdimensionen:  \\n1. Vielfältig und integrierend  \\n2. Antizipativ und reflektiv  38 \\n 3. Offen und transparent  \\n4. Reaktionsfähig und adaptiv bei Veränderungen  (Krumay, 2018, S. 145)  \\nIn der ersten Prozessdimension  geht es darum möglichst viele Steakholder in den For-\\nschungsprozess einzubinden. Diese bringen nicht nur unterschiedliche Bedürfnisse, son-\\ndern auch unterschiedliches Wissen mit. Dadurch soll ein gemeinsamer Entwicklungspro-\\nzess entstehen.  (vgl. ebd. , S. 14 5) \\nDie zweite Prozessdimension soll gesellschaftliche Katastrophen, wie  beispielsweise  \\ndurch Nobel oder Oppenheimer verursacht, verhindern. In dieser Dimension geht es um \\nein selbstkritisches Hinterfragen der Innovation, sowie um ein Reflektieren und Abwäge n \\nüber mögliche Auswirkungen.  (vgl. ebd. , S. 14 5) \\nÜber die dritte Prozessdimension soll der Gesellschaft die Forschung zugänglich gemacht \\nwerden und ihnen somit die Verunsicherung gegenüber den neuen Entwicklungen neh-\\nmen.  (vgl. ebd. , S. 14 6) \\n Die letzte Prozessdimension verlangt die Bereitschaft von Personen und Organisationen, \\nsich veränderten Situationen anpassen zu können.  (vgl. ebd. , S. 14 6) \\n6.4. Zusammenfassung  \\nKünstliche neuronale Netzwerke stellen eine beispiellose Entwicklung dar und w erfen so-\\nmit einige ethische und gesellschaftliche Probleme und Fragen auf, welche in dieser Pra-\\nxisnähe noch nie gestellt wurden. Ebenso haben auch diese neuen Techniken ihre eigenen \\nGefahren und Risiken, denen man sich unvoreingenommen stellen muss, ohne d abei an \\neine wirkliche Intelligenz zu glauben. Auch ist es wichtig, in einem Gebiet mit so viel Po-\\ntentia l die Forschung gewissenhaft und rücksichtsvoll voranzubringen, um nicht zuletzt \\nnegativen Auswirkungen auf die Gesellschaft vorzubeugen.  \\n  39 \\n 7. Resümee  \\nKünstliche neuronale Netze setzen sich aus parallel und seriell geschalteten künstlichen \\nNeuronen zusammen. Diese sind ein mathematisches Modell biologischer Nervenzellen. \\nVergleichbar mit biologischen Reizen, werden im Modell Zahlenwerte von hinten nach \\nvorne durch das Netz gereicht. Synapsen und Schwellenwerte, modelliert als Gewichte \\nund Biases, bestimmen , wie diese Reize verarbeitet werden.  \\nÜber eine sogenannte „Fehlerfunktion“ kann der Fehler eines künstlichen neuronalen \\nNetzes ermittelt werden, dieser beschr eibt wie inkorrekt das Netzwerk eine Aufgabe löst. \\nDas Lernverfahren sucht die Gewichts - und Biaswerte am  Minimum der  Fehlerfunktion.  \\nIn dieser Arbeit wurde das Gradientenabstiegsverfahren als Lernalgorithmus beschrieben. \\nÜber dieses werden die Param eter Schritt für Schritt Richtung Minimum des Fehlers op-\\ntimiert. Die Richtung des Tales gibt die negative Ableitung an.  \\nDie beschriebe Theorie wurde in einem selbstgeschriebenen Programm angewandt. Das \\nZiel, handgeschriebene Ziffern zu klassifizieren, konn te mit einer beachtlichen Genauig-\\nkeit von 98,89% umgesetzt werden. Das war nicht zuletzt dem Vervielfachen der Trai-\\nningsdaten zu verdanken. Diese Daten, welche aus Bildern handgeschriebener Ziffern be-\\nstehen, wurden durch das Erstellen leicht gedrehter Vari ationen verdreifacht.  \\nDas Hauptproblem künstlicher neuronaler Netzwerke liegt in deren Intransparenz. So ist \\nes für den Benutzer eines selbstlernenden Systems meist nicht einsehbar, worauf basie-\\nrend die Maschine ihre Entscheidungen trifft. Maschinen überne hmen  aufgrund maschi-\\nnellen Lernens  auch  immer mehr verantwortungsvolle Aufgaben, welche bis jetzt aus-\\nschließlich Menschen vorbehalten waren. Die Frage, ob Maschinen ethisch handeln kön-\\nnen, bleibt jedoch bestehen.  \\nDas in dieser Arbeit behandelte Modell eine s neuronalen Netzes ist die Basis für zahlrei-\\nche Weiterentwicklungen und Optimierungen. Sie stellen einen bedeutenden Bestandteil \\ndes Teilgebietes „künstlicher Intelligenz“ in der Informatik da. Diese Arbeit zeigt, dass \\nkünstliche Intelligenz wohl doch noc h mehr „künstlich“ als „intelligent“ ist. Und doch ha-\\nben künstliche neuronale Netze ihre eigenen Gefahren, denen man sich unvoreingenom-\\nmen stellen muss, ohne dabei von weit futuristischeren Problemen, welche die Filmin-\\ndustrie aufwirft, abgelenkt zu sein.  40 \\n Literaturverzeichnis  \\nAlpaydin, Ethem: Maschinelles Lernen. 2. Auflage. Berlin/Boston: Walter de Gruyter \\nGmbH , 2019 . \\nAngwin, Julia u.a.: Machine Bias. 23.5.2016. https://www.propublica.org/article/ma-\\nchine -bias-risk-assessments -in-criminal -sentencing [Zugrif f. 2.2.2020].  \\nBeutelsbacher, Albrecht: Lineare Algebra. Eine Einführung in die Wissenschaft der Vek-\\ntoren, Abbildungen und Matrizen. 7. Auflage. Wiesbaden: Vieweg+Teubner | GWV Fach-\\nverlage GMBH , 2010 . \\nCastellvecci, Davide: Eine tückische Blackbox. 16.11.2016. https://www.spekt-\\nrum.de/news/eine -tueckische -blackbox/1429906 [Zugriff: 2.2.2020].  \\nCisse, Moustapha u.a.: Houdini: Fooling Deep Structured Prediction Models. 17.7.2017. \\nhttps://arxiv.org/abs/1707.05373 [Zugriff: 2.2.2020].  \\nErtel Wolfgang: Grund kurs Künstliche Intelligenz. Eine praxisorientierte Einführung.  \\nWiesbaden: Vieweg & Sohn Verlag | GWV Fachverlage GmbH , 2008 . \\nGoodfellow, Ian u.a.: Deep Learning. Das umfassende Handbuch. Frechen: mitp Verlags \\nGmbH & Co. KG , 2018 . \\nJänich, Klaus: Lineare Al gebra. 6. Auflage. Berlin/Heidelberg: Springer -Verlag , 1996 . \\nKinnebrock, Werner: Neuronale Netze. Grundlagen, Anwendungen, Beispiele. München: \\nR. Oldenbourg Verlag GmbH , 1992 . \\nKrumay, Barbara: Zum Nutzen der Menschheit. Ethische Aspekte und Responsible Res e-\\narch. In: iX Developer. Machine Learning. 6.12.2018, S. 142 -146. \\nLeCun, Yann u. a.: The MNIST database of handwritten digits. 14.5.2013. \\nhttp://yann.lecun.com/exdb/mnist/ [Zugriff: 25.1.2020].  \\nRashid, Tariq: Neuronale Netze selbst programmieren. Ein verst ändlicher Einstieg mit \\nPython. Heidelberg : O’Reilly Media Inc. , 2017 . \\nRibeiro u.a.: “Why Should I Trust You?”: Explaining the Predictions of Any Classifier. \\n16.2.2016. https://arxiv.org/abs/1602.04938 [Zugriff. 2.2.2020].  41 \\n Scholtysek, Sebastian: Die Roboter gesetze von Isaac Asimov. 9.2.2015. http://www.ro-\\nboterwelt.de/magazin/die -robotergesetze -von-isaac -asimov/ [Zugriff: 2.2.2020].  \\nSpiekermann, Sara: Künstliche Intelligenz – mehr Fluch als Segen. 28.11.2016. \\nhttps://www.derstandard.at/story/2000048339674/kue nstliche -intelligenz -mehr -fluch -\\nals-segen [Zugriff: 2.2.2020].  \\nWeisstein Eric W.: Vector Direct Product. 2.1.2020. http://mathworld.wolfram.com/Vec-\\ntorDirectProduct.html [Zugriff: 3.2.2020].  \\nTilly, Marcel: Handwerk hat eine Zukunft. Künstliche Intelligenz – zwischen Hype und Re-\\nalität. In: iX Developer. Machine Learning. 6.12.2018, S. 138 -140.  42 \\n Abbildungsverzeichnis  \\nAbbildung 1: biologische Neuronen, Quelle: \\nhttps://de.wikipedia.org/wiki/Datei:Neurons_uni_bi_multi_pseudouni.svg, CC BY -SA \\n3.0, in Inkscape bearbeitet  ................................ ................................ ................................ ..6 \\nAbbildung 2: Vorläufig es Modell einer Nervenzelle, in Inkscape erstellt ............................ 7 \\nAbbildung 3: mathematisches Neuron, in Inkscape erstellt.  ................................ ............... 9 \\nAbbildung 4: Raupen und Marienkäfer, in Inkscape erstellt  ................................ ............. 10 \\nAbbildung 5: Länge und Breite als Eingabe eines Neurons, in Inkscape erstellt  ............... 11 \\nAbbildung 6: Raupen  und Marienkäfer durch ein Neuron klassifiziert, in Inkscape erstellt\\n ................................ ................................ ................................ ................................ ........... 11 \\nAbbildung 7: Sigmoid und Stufenfunktion, in Geogebra erstellt ................................ ....... 12 \\nAbbildung 8: Parallele Neuronen, in Inkscape erstellt  ................................ ...................... 13 \\nAbbildung 9: Klassifizierung bei drei Klassen, in Inkscape erstellt  ................................ ....13 \\nAbbildung 10: Liner unklassifizierbare Klassen, in Inkscape erstellt  ................................ .15 \\nAbbildung 11: Mehrlagiges neuronales Netz, in Inkscape erstellt  ................................ ....15 \\nAbbildung 12: Zusammenfassung zu ein er Formel, in Incskape und Word erstellt  .......... 18 \\nAbbildung 13: Fehler in Abbhängigkeit zu zwei Gewichten, Quelle: \\nhttps://de.wikipedia.or g/wiki/Datei:2D_Wavefunction_(2,1)_Surface_Plot.png, in \\nInkscape bearbeitet  ................................ ................................ ................................ ........... 19 \\nAbbildung 14: Gewicht in vorletzter Schicht, in Inkscape erstellt  ................................ .....22 \\nAbbildung 15: Beispielziffern der MNIST Datenbank, Quelle: \\nhttp://yann.lecun.com/exdb/mnist/, in Python visualis iert ................................ ............. 26 \\nAbbildung 16: Genauigkeit und Lernrate, in Word erstellt  ................................ ............... 29 \\nAbbildung 17: Genauigkeit und Epochen, in Word erstellt  ................................ ............... 30 \\nAbbildung 18: Rückwärtsabfrage der Ziffer Null, in Python visualisiert  ............................ 31 \\nAbbildung 19: Rückwärtsabfrage aller Ziffern, in Python visualisiert  ............................... 32 \\nAbbildung 20: Eine Eins der MNIST Datenbank inklusive gedrehter Variationen, Quelle: \\nhttp://yann.lecun.com/exdb/mnist/, in Python bearbeitet  ................................ .............. 33 \\n 43 \\n Anhang  \\nMatrizen  \\nDefinition  \\nEine Matrix ist eine Rechteckige Anordnung von Elementen nach folgendem Schema (vgl. \\nBeutelsbacher, 2010, S. 50f.):  \\n (𝑎11𝑎12⋯𝑎1𝑛\\n𝑎21𝑎22⋯𝑎2𝑛\\n⋮⋮ ⋮\\n𝑎𝑚1𝑎𝑚2⋯𝑎𝑚𝑛) \\nSkalare Multiplikation  \\nDie skalare Multiplikation  von Matrizen  funktioniert ähnlich wie bei Vektoren (vgl. Beu-\\ntelsbacher, 2010, S. 51).  \\n𝑘∗(𝑎11𝑎12\\n𝑎21𝑎22\\n𝑎31𝑎32)=(𝑘∗𝑎11𝑘∗𝑎12\\n𝑘∗𝑎21𝑘∗𝑎22\\n𝑘∗𝑎31𝑘∗𝑎32) \\nMatrix -Vektor -Multiplikation  \\nDas Produkt einer Matrix -Vektor -Multiplikation ist ein Vektor. Dieser beinhaltet die Ska-\\nlarprodukte des Vektors mit jeder Zeile der Matrix. (Jänich, 1996, S. 89f.)  \\n(𝑎11𝑎12𝑎13\\n𝑎21𝑎22𝑎23)⋅(𝑥1\\n𝑥2\\n𝑥3)= (𝑎11𝑥1+𝑎12𝑥2+𝑎13𝑥3\\n𝑎21𝑥1+𝑎22𝑥2+𝑎23𝑥3) \\nTransponierte Matrix  \\nBeim Transponieren einer Matrix werden die Spalten mit den Zeilen getauscht (vgl. Beu-\\ntelsbacher, 2010, S. 191).  \\n(𝑎11𝑎12\\n𝑎21𝑎22\\n𝑎31𝑎32)𝑇\\n=(𝑎11𝑎21𝑎31\\n𝑎21𝑎22𝑎32) \\nWeiterführende Rechenregeln für Vektoren  \\nElementweise Multiplikation  \\nBei der elementweisen Multiplikation werden die einzelnen Elemente der Vektoren  mul-\\ntipliziert.  44 \\n (𝑥1\\n𝑥2\\n𝑥3)⨀(𝑦1\\n𝑦2\\n𝑦3)=(𝑥1𝑦1\\n𝑥2𝑦2\\n𝑥3𝑦3) \\nDyadisches Produkt  \\nDas dyadische Produkt wird folgendermaßen definiert (vgl. Weisstein, 2020):  \\n(𝑥1\\n𝑥2\\n𝑥3)⨂(𝑦1\\n𝑦2\\n𝑦3)=(𝑥1𝑦1𝑥1𝑦2𝑥1𝑦3\\n𝑥2𝑦1𝑥2𝑦2𝑥2𝑦3\\n𝑥3𝑦1𝑥3𝑦2𝑥3𝑦3) \\nQuellcode  \\n#!/usr/bin/env  python \\n#!/usr/bin/python  \\n#!python  \\n# -*- coding: utf-8 -*- \\n \\n\"\"\" \\nAutor: Elias Krainer \\nVersion:  Python 3.8.1 \\n \\nDieses Programm  wurde im Rahmen der vorwissensch aftlichen  Arbeit \"Funktionsweise  \\nkünstlicher  neuronaler  Netzwerke:  Grundlagen  und Programmierung  eines \\nBeispiels\"  erstellt.  Es ist um die Klasse \"KNN\" gebaut, diese beschreibt  ein \\nmehrlagiges  künstliches  neuronales  Netz auf Basis der Sigmoidfunktion.  Es lernt \\nüber ein Gradientenabstiegsverfahren  auf Basis von Backpropagation.  \\n \\nInhalt: \\nKlasse ANN: Hauptklasse,  beinhaltet  alle wesentlichen  Funktionen  eines KNN \\nFunktion  format_mnist:  Zum formatieren  von MNIST Dateien im \"Pickle\"  Format \\nFunktion  main: Zum Aufrufen  der zuvorgenannten  Funktionen.  \\n \\nInfo: Die Funktion  \"main\" wird aufgerufen  wenn das Script als Hauptmodul  \\nausgeführt  wird. \\n\"\"\" \\n \\n# Drittanbieter  Module \\nimport numpy as np # für das Erstellen  und Rechnen mit Matrizen  \\n \\nclass ANN(object):  \\n    \"\"\" \\n    Klasse zur Erstellung  eines künstlichen  neuronalen  Netzes. \\n    \"\"\" \\n \\n    def __init__(self,  structure):  \\n        \"\"\" 45 \\n         Konstuktor  der Klasse ANN. Hier werden die Gewichtsmatri zen des Netzes \\n        erstellt.  \\n \\n        Input: structure  [list] \\n        Diese Liste beinhaltet  die Anzahl der Neuronen  pro Schicht inklusive  \\n        der Eingabeschicht.  \\n        Bsp: [4,5,3] Erstellt  ein zweilagiges  Netz mit einer Eingabeschicht  \\n        mit 4 Neuronen,  eine Versteckte  Schicht mit 3 und eine Ausgabeschicht   \\n        mit 3 Neuronen.  \\n \\n        Info: Die Startwerte  für die Gewichtsmatrizen  werden mit \\n        normalverteilten  Zufallszahlen  besetzt, wobei die Standardabweichung  \\n        über folgende  Formel berechnet  wird: 1/squareroot(n)  n ist die Anzahl \\n        der Neuronen  in der vorgeschalteten  Schicht.  \\n        \"\"\" \\n        self.weights  = list() # Liste aller Gewichtsmatrizen  als Attribut  \\n        # target: Anzahl der Neuronen  in aktueller  Schicht \\n        # previous:  Anzahl der Neuronen  in zuvorgeschaltener  Schicht \\n        for previous,  target in zip(structure[ 1:],structure[: -1]): \\n            # eine Matrix mit oben beschriebenem Inhalt  wird erstellt.  Ihre \\n            # Größe setzt sich einereits  aus der Anzahl voriger Neuronen,  sowie \\n            # der Anzahl an Neuronen  in aktueller  Schicht +1 zusammen.  Diese  \\n            # zusätzli che Reihe kommt daher, dass der Bias ein Teil der Matrix \\n            # ist. \\n            weight_matrix  = np.random.normal( 0, 1/np.sqrt(previous),   \\n                                             (target+ 1, previous))   \\n            self.weights.append( weight_matrix)  \\n \\n    def sigmoid(self,  x): \\n        \"\"\" \\n        Sigmoid Funktion;  \\n        Input: x [number oder np.array]  \\n        \"\"\" \\n        return 1 / (1 + np.exp(-x)) \\n \\n    def run(self,  activations):  \\n        \"\"\" \\n        Einfaches  durchlaufen  des Netzes. \\\\n \\n        Input: activations  [np.array]  \\\\n  \\n        Output: Ausgabe [np.array]  \\n        \"\"\" \\n        # activations:  Eingabe der aktuellen  Schicht \\n        # weight_matrix:  aktualle  Gewichtsmatrix  \\n        for weight_matrix  in self.weights:  \\n            # Eine 1 wird an nullter Stelle der Eingabe (activations)  angehängt  \\n            # aufgrund  der Biasspalte  der Matrix \\n            activations  = np.insert(activations, 0,1) \\n            # Die übliche Formel zur Berechnung der Ausgabe einer Schicht 46 \\n             # sigmoid(a*W)  \\n            activations  = self.sigmoid(np.dot(activations,  weight_matrix))  \\n        return activations  \\n \\n    def eval_accuracy(self,  data): \\n        \"\"\" \\n        Testen der Genauigkeit  des Netzes \\n \\n        Input: data [list] Bsp.: [[Eingabe,  Soll-Werte], [...], ...] \\n \\n        Output: p [float];  \\n        p: rellatiever  Anteil der richtig klassifizierter  Daten \\n \\n        Info: Als richtig Klassifiziert  gilt, wenn der Index des größten \\n        Elementes  der Ausgabe gleich dem Index des größten Elementes  der  \\n        Sollwerten  ist. \\n        \"\"\" \\n        correct = 0 # Anzahl richtig klassifizierter  Daten \\n        # activations:  Einage, expectations:  Sollwerte  \\n        for activations,  expectations  in data: \\n            # np.argmax:  Index des größtem Element im Vektor \\n            if np.argmax( self.run(activations))  == np.argmax(expectations):  \\n                correct += 1 \\n        return correct / len(data)  # Berechnung  des relativen  Anteiles  \\n \\n    def backprob(self,  activations,  expectations):  \\n        \"\"\" \\n        Backpropagation  Algorithmus  für ein Trainingsbeispiel.  \\\\n \\n        Input: activations  [np.array];  expectations  [np.array]  -  \\n        Eingabe sowie Sollwerte  des Netzes \\\\n \\n        Output: gradient  [list] - Liste der Gradienten  der Gewichtsmatrizen  \\n        \"\"\" \\n        # Gleich wie in ANN.run nur, dass alle Eingabewerte  in past_activations  \\n        # gespeichert  werden \\n        past_activatios  = [] \\n        for weight_matrix  in self.weights:  \\n            activations  = np.insert(activations, 0,1) \\n            past_activatios.append(activations)  \\n            activations  = self.sigmoid(np.dot(activations,  weight_matrix))  \\n        past_activatios.append(np.insert(activations, 0,1)) \\n \\n        # gradient:  Liste aller Gewichtsmatrizen -Gradienten  \\n        gradient  = list() \\n        # error: Teilfehler;  layer: aktuelle  Schicht,  von hinten nach vorne \\n        # Berechnung  des Teilfehlers  in letzter Schicht:  \\n        error = (activations  - expectations)  * activations  * (1-activations)  \\n        for layer in range(len( self.weights),  0, -1): \\n            # Berechnung  des Gewichtsmatrizen -Gradienten  47 \\n             gradient.append(np.outer(past_activatios[layer -1], error)) \\n            # Berechnung  des Teilfehlers  der zuvorgeschaltenen  Schicht,  wobei \\n            # über das [1:] der Bias weggenommen  wird da dieser hier nicht von \\n            # Bedeutung  ist \\n            error = np.dot(error  , np.transpose( self.weights[layer -1][1:])) \\\\ \\n             * past_activatios[layer -1][1:] * (1-past_activatios[layer -1][1:])  \\n        return gradient[:: -1] # Neusotierung  der Liste (von hinten nach vorne) \\n \\n     \\n    def learn(self, training_data,  test_data,  epochs, learning_rate  ): \\n        \"\"\" \\n        Lernen der Trainingsdaten  \\n         \\n        Input: \\\\n \\n        training_data  [list] - Trainingsdaten  -  \\n        Bsp. [[Eingabe,  Soll-Werte], [...], ...] \\\\n \\n        test_data  [list] - Testdaten  -  \\n        Bsp. [[Eingabe,  Soll-Werte], [...], ...] \\\\n \\n        epochs [integer]  - Anzahl der Epochen \\\\n \\n        learning_rate  [float] - Lernrate   \\n        \"\"\" \\n        for epoch in range(epochs):  # Wiederholt  sich für jede Epoche \\n            # Schreibt  aktuelle  Epoche in den Terminal  \\n            print(\"epoch %i/%i\" % (epoch + 1, epochs))   \\n            np.random.shuffle(training_data)  # Trainingsdaten  werden gemischt  \\n            # activations:  Einage, expectations:  Sollwerte  \\n            for activations,  expectations  in training_data:  \\n                # weight_update  ist der Gradient  \\n                weight_update  = self.backprob(activations,  expectations)  \\n                # Gewichtsmatrizen  werden über folgende  Formel aktuallisiert:  \\n                # W_neu = W_alt - Lernrate  * Gradientenmittelwert  \\n                for n in range(len( self.weights)):  \\n                    self.weights[n]  -= weight_update[n]  * learning_rate  \\n            # Ausgabe der Genauigkeit  \\n            print(\"accuracy:\" , self.eval_accuracy(test_data))  \\n \\n    def feed_backward(self,  output_neuron):  \\n        \"\"\" \\n        Rückwärtsabfrage  des Netzes \\\\n \\n        Input: output_neuron  [integer]  - Index des Ausgabeneurons  welches für \\n        die Rückwärtsabfrage  aktiv ist. \\\\n \\n        Output: Ausgabe [np.array]  - Ausgabe der Rückertsabfrage  in Form \\n        der Eingabe des Netzes. \\n        \"\"\" \\n        inverse_si gmoid = lambda x : -np.log(1/x-1) # inverse Sigmoidfunktion  \\n        activations  = np.full( 10, 0.01) # Alle Ausgabeneuronen  sind inaktiv \\n        activations[output_neuron]  = 0.99 # nur das deklarierte  nicht \\n        # weight_matrix:  aktuelle  Gewichtsmatrix  (von hinten nach vorne) 48 \\n         for weight_matrix  in self.weights[:: -1]: \\n            # Ausgabe wird nach vor geleitet  \\n            activations  = np.dot(inverse_sigmoid(activations),  \\n             np.transpose(weight_m atrix[1:])) \\n            # Ausgabe wird auf 0-1 skaliert  \\n            activations  -= np.min(activations)  \\n            activations  = activations  / np.max(activations)  * 0.98 + 0.01 \\n        return activations  \\n \\n \\n \\ndef format_mnist(file):  \\n    \"\"\" \\n    Importieren  und formatieren  der MNIST Daten. \\\\n \\n    Input: file [string]  - Dateipfad  \\\\n \\n    Achtung:  Die Daten müssen im Pickle Format vorliegen  und alle Bilder als \\n    \"images\"  und alle Beschriftungen  als \"labels\"  in die Datei gespeichert  \\n    worden sein. \\n    \"\"\" \\n    data = np.load(file,  allow_pickle= True) # Importieren  der Datei \\n    labels = data[\"labels\" ] # extrahieren  der Beschriftungen  \\n    images = data[\"images\" ] # extrahieren  der Bilder \\n    # skalieren  der Grauwerte  der Bilder auf 0.01 - 1 \\n    images = images * (0.99 / 255) + 0.01 \\n    formated  = list() # Liste der formatierten  Dateien \\n    # image, label : Aktuelles  Bild und aktuelle  Beschriftung  \\n    for image, label in zip(images,  labels):  \\n        # Beschriftungen  werden zu Soll-Werten \\n        # Bsp. 2 -> (0.01, 0.01, 0.99, 0.01, 0.01, 0.01, ...) \\n        new_label  = np.full( 10, 0.01) \\n        new_label[int( label)] = 0.99 \\n        formated.append((image,  new_label))  \\n    return formated  \\n \\n \\ndef main(): \\n    print(\"import data\") \\n    # Importieren  und Formatieren  der Daten \\n    test_data  = format_mnist( \"mnist_test.npz\" ) \\n    training_data  = format_mnist( \"mnist_train.npz\" ) \\n    print(\"start learning  process\" ) \\n    Net = ANN((784, 50, 50, 10)) # Initialisieren  des Netzes \\n    Net.learn(training_data,  test_data,  5, 0.1) # Lernprozess  \\n    print(\"finished\" ) \\n \\n \\n 49 \\n # Folgende  Bedinnung ist wahr, wenn das Script als Hauptmodul  ausgeführt  wird \\n# also nicht wenn es als Bibliothek  verwendet  wird \\nif __name__  == \"__main__\" : \\n    main() \\n ', 'Künstliche Neuronale Netzwerke und Deep Learning\\nStefan Selle\\nProfessor für Wirtschaftsinformatik\\nFakultät für Wirtschaftswissenschaften\\nHochschule für Technik und Wirtschaft des Saarlandes\\nSaarbrücken, 12.05.2018Kurzfassung\\nDie vorliegende Arbeit mit dem Titel Künstliche Neuronale Netzwerke und Deep Learning\\nbeschäftigt sich mit einem derzeit viel beachteten Teilgebiet der Künstlichen Intelligenz\\n(KI). Künstliche Neuronale Netzwerke ( KNN ) sind von der Natur inspirierte, lernfähige\\nSysteme, die sich auf sehr vielfältige Art und Weise einsetzen lassen, z.B. um Muster in\\nDaten zu ﬁnden ( Data Mining ). Insbesondere in den Bereichen Bild- und Spracherken-\\nnung wurden in den letzten Jahren große Fortschritte durch die Verwendung von Deep\\nLearning ( DL)erzielt. Bei diesen Methoden werden sehr große Datenmengen ( Big Data ) zu-\\nnehmend auf graﬁschen Prozessoren (GPUs) verarbeitet, um spezielle und tiefe KNN zu\\ntrainieren. Typische Anwendungen sind das autonome Fahren im Bereich Bilderkennung\\nund digitale Assistenten wie bspw. Apples Siri, Amazons Alexa oder Googles Assistant\\nim Bereich Spracherkennung.\\nIn dieser Arbeit werden zunächst die Grundlagen zu KNN und DL dargestellt. Kurze\\nErklärungen zu den biologischen Hintergründen liefern zusätzliche Informationen. Drei\\nweit verbreitete Netzwerktypen mit den dahinter liegenden Ideen und Konzepte werden\\nerläutert: Multilayer Perceptron ( MLP ),Convolutional Neural Network ( CNN )und Recurrent\\nNeural Network ( RNN ). Zwei spezielle Netzwerke werden außerdem detailliert vorgestellt:\\nLong Short-Term Memory ( LSTM )und Gated Recurrent Unit ( GRU ). Die Ergebnisse einer\\nintensiven Recherche und aktuellen Marktanalyse führen zur Präsentation von 22 verfüg-\\nbaren Open Source DL-Softwarelösungen. Die Python-Bibliotheken TensorFlow und Ke-\\nras werden schließlich ausgewählt, um exemplarisch drei Anwendungen durchzuführen:\\nzwei Aufgaben stammen aus dem Bereich der Bilderkennung bzw. Objektklassiﬁzierung\\nund eine Aufgabe beschäftigt sich mit dem Thema Text Mining bzw. Sentiment Analysis .\\nMit nur wenigen Zeilen Quelltext lassen sich die oben genannten KNN erfolgreich kon-\\nﬁgurieren, trainieren und evaluieren. Zu den gegebenen Bild- und Textdaten lassen sich\\ndie Modelle auf einer High-End Graﬁkkarte von Nvidia, die CUDA unterstützt, relativ\\nschnell ausführen. Somit ist diese Hardware-Software-Kombination besonders gut geeig-\\nnet, um in der Lehre und der angewandter Forschung an der Hochschule für Technik und\\nWirtschaft des Saarlandes (htw saar) zukünftig eingesetzt zu werden.\\niInhaltsverzeichnis\\nAbbildungsverzeichnis v\\nTabellenverzeichnis vii\\nAbkürzungsverzeichnis ix\\n1 Einleitung 1\\n2 Theoretische Grundlagen 7\\n2.1 Künstliches Neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2.2 Perzeptron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.3 Topologie . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.4 Lernen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.5 Anwendungskategorien . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n2.6 Modelle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n2.7 Optimierungstechniken . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n3 Multilayer Perceptron 25\\n3.1 Aufbau . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n3.2 Backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n3.3 Erweiterungen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n3.4 Aktivierungsfunktionen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n3.5 Kostenfunktionen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n3.6 Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n4 Convolutional Neural Network 33\\n4.1 Visueller Cortex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n4.2 Aufbau und Schichten . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n4.3 LeNet-5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n4.4 AlexNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\\n4.5 GoogleLeNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n4.6 Sonstige . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\n5 Recurrent Neural Network 43\\n5.1 Bausteine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n5.2 Sequenzen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n5.3 Lernverfahren . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n5.4 Long Short-Term Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n5.5 Varianten . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\n5.6 Einsatzgebiete . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n6 Software 53\\n6.1 Caffe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\\n6.2 Caffe2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\\n6.3 Chainer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\niii6.4 CNTK . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\n6.5 Darknet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\n6.6 Deep Water . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\\n6.7 DL4J . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n6.8 Dlib . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\n6.9 DSSTNE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\\n6.10 Gluon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\\n6.11 Keras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\\n6.12 Lasagne . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\\n6.13 MXNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\\n6.14 Neon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\\n6.15 OpenNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\\n6.16 Paddle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\\n6.17 PyTorch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\\n6.18 SINGA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\\n6.19 TensorFlow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\\n6.20 TFLearn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\\n6.21 Theano . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\\n6.22 Torch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\\n7 Anwendungen 79\\n7.1 MNIST . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\\n7.2 CIFAR-10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\\n7.3 IMDb . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\\n8 Zusammenfassung und Ausblick 107\\nQuellenverzeichnis 115\\nivAbbildungsverzeichnis\\n1.1 Darstellung miteinander vernetzter Neuronen . . . . . . . . . . . . . . . . 2\\n2.1 Struktur eines künstlichen Neurons . . . . . . . . . . . . . . . . . . . . . . 7\\n2.2 Struktur eines Perzeptrons . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.3 Darstellung logischer Schaltungen mittels Perzeptronen . . . . . . . . . . . 9\\n2.4 Gradientenabstiegsverfahren im zweidimensionalen Fehlerraum . . . . . 11\\n2.5 Typische Probleme des Gradientenabstiegsverfahrens . . . . . . . . . . . . 12\\n2.6 Black Box Modell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n2.7 Taylorsche Reihe als Funktionennetz . . . . . . . . . . . . . . . . . . . . . . 13\\n2.8 Data Mining Kategorien . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n2.9 Übertrainieren und Trainingsabbruch . . . . . . . . . . . . . . . . . . . . . 21\\n2.10 k-fache Kreuz-Validierung . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n3.1 Mehrschichten-Perzeptron . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n3.2 Logistischen Funktion mit Ableitung . . . . . . . . . . . . . . . . . . . . . . 28\\n3.3 Aktivierungsfunktionen im Vergleich . . . . . . . . . . . . . . . . . . . . . 29\\n3.4 Standardnormalverteilung mit Intervallen . . . . . . . . . . . . . . . . . . . 31\\n4.1 Signalweg von der Netzhaut zur Sehrinde . . . . . . . . . . . . . . . . . . . 34\\n4.2 Funktionale Spezialisierung des visuellen Cortex . . . . . . . . . . . . . . . 35\\n4.3 Feature Learning durch Deep Learning . . . . . . . . . . . . . . . . . . . . 35\\n4.4 Aufbau eines typischen CNN . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n4.5 Max Pooling mit 2x2-Filter und Stride=2 . . . . . . . . . . . . . . . . . . . . 37\\n4.6 Aufbau des CNN LeNet-5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n4.7 Aufbau des AlexNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\\n4.8 Schematischer Aufbau eines Inception Moduls . . . . . . . . . . . . . . . . 40\\n4.9 Graphischer Aufbau des GoogleLeNet . . . . . . . . . . . . . . . . . . . . . 40\\n4.10 Tabellarischer Aufbau des GoogleLeNet . . . . . . . . . . . . . . . . . . . . 41\\n4.11 Funktionsweise einer Skip Connection in einer Residual Unit . . . . . . . 42\\n4.12 Squeeze-and-Excitation-Module ersetzen Inception- und Residuen-Module 42\\n5.1 Beispiel für ein Hidden Markov Model . . . . . . . . . . . . . . . . . . . . . 43\\n5.2 Drei Feedback-Typen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n5.3 Basiseinheit im RNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n5.4 Ausgerollte RNN-Basiseinheit . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n5.5 Verarbeitung von Sequenzen . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n5.6 Aufbau einer LSTM-Einheit . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n5.7 Ausgerollte LSTM-Einheit . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n5.8 Schritt 1: Forget-Gate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\\n5.9 Schritt 2: Input-Gate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\\n5.10 Schritt 3: Zelle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\\n5.11 Schritt 4: Output-Gate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\\n5.12 Peephole-LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\n5.13 Gekoppelte LSTM-Einheit . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\n5.14 Gated Recurrent Unit (GRU) . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\nv5.15 Modell des Spracherkennungsprozesses . . . . . . . . . . . . . . . . . . . . 51\\n6.1 Vergleich Caffe und Caffe2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\\n6.2 Vergleich Deﬁne-and-Run mit Deﬁne-by-Run . . . . . . . . . . . . . . . . . 58\\n6.3 CNTK-Architektur . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\n6.4 Darknet-Nightmare: Der Schrei von Edvard Munch . . . . . . . . . . . . . 60\\n6.5 Deep Water Architektur . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\\n6.6 Komponenten von Dlib . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\n6.7 Architektur mit DSSTNE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\\n6.8 MXNet: Technologie-Stack . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\\n6.9 Neon: Layer Taxonomie . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\\n6.10 OpenNN: Klassendiagramm . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\\n6.11 Vergleich Torch und PyTorch . . . . . . . . . . . . . . . . . . . . . . . . . . 72\\n6.12 Apache SINGA Software Stack . . . . . . . . . . . . . . . . . . . . . . . . . 73\\n6.13 TensorFlow Technologie-Stapel . . . . . . . . . . . . . . . . . . . . . . . . . 74\\n6.14 Theano: Beispiel-Graph mit Apply-Objekt . . . . . . . . . . . . . . . . . . 76\\n6.15 Torch 7 Technologie-Stapel . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\\n7.1 Beispielbilder des MNIST Test-Datensatzes . . . . . . . . . . . . . . . . . . 80\\n7.2 Fehlerraten für verschiedene Klassiﬁzierer zur MNIST Datenbank . . . . . 81\\n7.3 Genauigkeit des MLP für die Klassiﬁkation der MNIST-Datenbank . . . . 84\\n7.4 Fehler bzw. Loss des MLP für die Klassiﬁkation der MNIST-Datenbank . . 85\\n7.5 Genauigkeit des CNN für die Klassiﬁkation der MNIST-Datenbank . . . . 87\\n7.6 Fehler bzw. Loss des CNN für die Klassiﬁkation der MNIST-Datenbank . 87\\n7.7 Beispielbilder der CIFER-10 Datenbank . . . . . . . . . . . . . . . . . . . . 88\\n7.8 CNN verarbeitet Bilder der CIFER-10 Datenbank . . . . . . . . . . . . . . . 89\\n7.9 Genauigkeit des CNN für die Klassiﬁkation der CIFAR-10-Datenbank . . 92\\n7.10 Fehler bzw. Loss des CNN für die Klassiﬁkation der CIFAR-10-Datenbank 93\\n7.11 Genauigkeit des MLP für die Klassiﬁkation der IMDb-Datenbank . . . . . 97\\n7.12 Fehler bzw. Loss des MLP für die Klassiﬁkation der IMDb-Datenbank . . 98\\n7.13 Genauigkeit des CNN für die Klassiﬁkation der IMDb-Datenbank . . . . . 100\\n7.14 Fehler bzw. Loss des CNN für die Klassiﬁkation der IMDb-Datenbank . . 100\\n7.15 Genauigkeit des LSTM für die Klassiﬁkation der IMDb-Datenbank . . . . 102\\n7.16 Fehler bzw. Loss des LSTM für die Klassiﬁkation der IMDb-Datenbank . . 102\\n7.17 Genauigkeit des GRU für die Klassiﬁkation der IMDb-Datenbank . . . . . 104\\n7.18 Fehler bzw. Loss des GRU für die Klassiﬁkation der IMDb-Datenbank . . 104\\n8.1 Gartners magische Quadranten zu Data Science und Maschine Learning . 110\\n8.2 Workﬂow der KNIME Analytics Platform . . . . . . . . . . . . . . . . . . . 111\\nviTabellenverzeichnis\\n4.1 Topologie der LeNet-5-Architektur . . . . . . . . . . . . . . . . . . . . . . . 38\\n4.2 Topologie der AlexNet-Architektur . . . . . . . . . . . . . . . . . . . . . . . 39\\n6.1 Übersicht von DL-Repositories auf der Plattform GitHub . . . . . . . . . . 53\\n6.2 Übersicht von Attributen zu einem Repository auf GitHub . . . . . . . . . 54\\n6.3 Übersicht ausgewählter DL-Softwarelösungen auf der Plattform GitHub . 55\\n6.4 Steckbrief zur Caffe-Bibliothek . . . . . . . . . . . . . . . . . . . . . . . . . 56\\n6.5 Steckbrief zum Caffe2-Framework . . . . . . . . . . . . . . . . . . . . . . . 57\\n6.6 Steckbrief zum Chainer-Framework . . . . . . . . . . . . . . . . . . . . . . 58\\n6.7 Steckbrief zum CNTK-Framework . . . . . . . . . . . . . . . . . . . . . . . 59\\n6.8 Steckbrief zum Darknet-Framework . . . . . . . . . . . . . . . . . . . . . . 60\\n6.9 Steckbrief zur Deepwater-Bibliothek . . . . . . . . . . . . . . . . . . . . . . 61\\n6.10 Steckbrief zur DL4J-Bibliothek . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n6.11 Steckbrief zur Dlib-Bibliothek . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\n6.12 Steckbrief zur DSSTNE-Bibliothek . . . . . . . . . . . . . . . . . . . . . . . 64\\n6.13 Steckbrief zur Gluon-Bibliothek . . . . . . . . . . . . . . . . . . . . . . . . . 65\\n6.14 Übersicht zu Keras-Verwendungsmöglichkeiten . . . . . . . . . . . . . . . 66\\n6.15 Steckbrief zur Keras-Bibliothek . . . . . . . . . . . . . . . . . . . . . . . . . 66\\n6.16 Programmieren mit der Lasagne-Bibliothek . . . . . . . . . . . . . . . . . . 67\\n6.17 Steckbrief zur Lasagne-Bibliothek . . . . . . . . . . . . . . . . . . . . . . . . 67\\n6.18 Programmieren mit dem MXnet-Framework . . . . . . . . . . . . . . . . . 68\\n6.19 Steckbrief zur MXNet-Bibliothek . . . . . . . . . . . . . . . . . . . . . . . . 68\\n6.20 Steckbrief zur Neon-Bibliothek . . . . . . . . . . . . . . . . . . . . . . . . . 69\\n6.21 Steckbrief zur OpenNN-Bibliothek . . . . . . . . . . . . . . . . . . . . . . . 70\\n6.22 Steckbrief zur Paddle-Bibliothek . . . . . . . . . . . . . . . . . . . . . . . . 71\\n6.23 Steckbrief zur PyTorch-Bibliothek . . . . . . . . . . . . . . . . . . . . . . . . 72\\n6.24 Steckbrief zur SINGA-Bibliothek . . . . . . . . . . . . . . . . . . . . . . . . 73\\n6.25 Steckbrief zur TensorFlow-Bibliothek . . . . . . . . . . . . . . . . . . . . . . 74\\n6.26 Steckbrief zur TFLearn-Bibliothek . . . . . . . . . . . . . . . . . . . . . . . 75\\n6.27 Steckbrief zur Theano-Bibliothek . . . . . . . . . . . . . . . . . . . . . . . . 76\\n6.28 Steckbrief zur Torch-Bibliothek . . . . . . . . . . . . . . . . . . . . . . . . . 77\\n7.1 Topologie des CNN für die MNIST-Datenbank . . . . . . . . . . . . . . . . 85\\n7.2 Topologie des CNN für die CIFAR-10-Datenbank . . . . . . . . . . . . . . 89\\n7.3 Vergleich der Ergebnisse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\\nviiAbkürzungsverzeichnis\\nAdaline Adaptive Linear Neuron\\nAdam Adaptive Moment Estimation\\nAI Artiﬁcial Intelligence\\nAPI Application Programming Interface\\nART Adaptive Resonance Theory\\nASF Apache Software Foundation\\nAWS Amazon Web Services\\nBAIR Berkeley Artiﬁcial Intelligence Research\\nBAM Bidirectional Associative Memory\\nBI Business Intelligence\\nBLAS Basic Linear Algebra Subprograms\\nBN Batch Normalization\\nBP Backpropagation\\nBPTT Backpropagation Through Time\\nBSB Brain-State-in-a-Box\\nBSD Berkeley Software Distribution\\nBVLC Berkeley Vision and Learning Center\\nCIFAR Canadian Institute For Advanced Research\\nCGL Corpus Geniculatum Laterale\\nCNN Convolutional Neural Network\\nCNTK Cognitive Toolkit\\nCPU Central Processing Unit\\nDAE Deep Autoencoder\\nDBN Deep Belief Network\\nDL Deep Learning\\nDL4J Deep Learning for Java\\nDSSTNE Deep Scalable Sparse Tensor Network Engine\\nixECS Elastic Container Service\\nEMR Elastic MapReduce\\nFM Frequenzmodulation\\nGAN Generative Adversarial Network\\nGPU Graphics Processing Unit\\nGRU Gated Recurrent Unit\\nHDFS Hadoop Distributed File System\\nHMM Hidden Markov Model\\nHTML Hypertext Markup Language\\nIDS Intrusion Detection System\\nIMDb Internet Movie Database\\nILSVRC ImageNet Large Scale Visual Recognition Challenge\\nIoT Internet of Things\\nIT Information Technology\\nJSON JavaScript Object Notation\\nJVM Java Virtual Machine\\nkHz Kilohertz\\nKDD Knowledge Discovery in Databases\\nKI Künstliche Intelligenz\\nKNN Künstliches Neuronales Netzwerk\\nLasso Least Absolute Shrinkage and Selection Operator\\nLVQ Linear Vector Quantization\\nLRN Local Response Normalisation\\nLSTM Long Short-Term Memory\\nMAE Mean Absolute Error\\nMadaline Multiple Adaline\\nMAPE Mean Absolute Percentage Error\\nMILA Montreal Institute for Learning Algorithms\\nMKL Math Kernel Library\\nMLFF Multilayer Feedforward Network\\nMLP Multilayer Perceptron\\nxMNIST Modiﬁed National Institute of Standards and Technology\\nms Millisekunde\\nMSE Mean Squared Error\\nNER Named Entity Recognition\\nNIST National Institute of Standards and Technology\\nNLP Natural Language Processing\\nNMT Neural Machine Translation\\nNP Nichtdeterministisch polynomiale Komplexität\\nOCR Optical Character Recognition\\nONNX Open Neural Network Exchange\\nOS Operating System\\nPyPI Python Package Index\\nRBF Radial Basis Function\\nRBM Restricted Boltzmann Machine\\nReLU Rectiﬁed Linear Unit\\nRMSprop Root Mean Square Propagation\\nRNN Recurrent Neural Network\\nRprop Resilient Backpropagation\\nS3 Simple Storage Service\\nSA Simulated Annealing\\nSE Squeeze-and-Excitation\\nSDA Stacked Denoising Autoencoder\\nSGD Stochastic Gradient Descent\\nSOM Self-Organizing Map\\nSVM Support Vector Machine\\nTCO Total Cost of Ownership\\nTSP Travelling Salesman Problem\\nUC University of California\\nUKW Ultrakurzwellen\\nXOR Exklusives Oder\\nYOLO You only look once\\nxi1 Einleitung\\nKIbzw. Künstliche Intelligenz (engl. Artiﬁcial Intelligence ( AI)) ist ein Thema, das aktuell\\nfür viel Aufmerksamkeit in den Medien sorgt. Die damit verbundene Unsicherheit be-\\nzüglich der möglichen Verdrängung oder Transformation der bestehenden Arbeitsplätze\\nwird insbesondere in Deutschland umstritten diskutiert. Prominente Wissenschaftler und\\nUnternehmer wie bspw. der erst kürzlich verstorbene britische theoretische Astrophysi-\\nker Stephen Hawking [Dav14] oder die US-amerikanischen High Tech Gründer Elon Musk\\n(PayPal, Tesla, SpaceX) [Mus17] und Sergey Brin (Alphabet bzw. Google) [Bri18] warnen\\nsogar vor den möglichen Risiken der KI für die Menschheit. Wir sind uns zumindest\\ndarüber bewusst, dass die Digitalisierung als globales Phänomen das Arbeits- und Privat-\\nleben in den nächsten Jahren massiv verändern wird. Zukunftsthemen wie das Internet\\nder Dinge (engl. Internet of Things ( IoT)) und Industrie 4.0 sind ohne KI als Motor der\\ndigitalen Transformation kaum vorstellbar.\\nDiese Arbeit beschäftigt sich jedoch nicht mit den gesellschaftlichen, politischen oder\\nökonomischen Fragen zur Künstlichen Intelligenz. Stattdessen werden im Wesentlichen\\ndie Technologien beleuchtet, die z.Zt. einen großen Stellenwert in der KI-Forschung und\\nden KI-Anwendungen einnehmen, nämlich Künstliche Neuronale Netzwerke ( KNN ) und\\nDeep Learning (DL) .\\nBiologische Neuronale Netzwerke Biologische Nervensysteme arbeiten massiv paral-\\nlel, sind weitgehend fehlertolerant, verhalten sich adaptiv und als lernende Systeme, die\\nihre eigenen Parameter bestimmen und anpassen können. Das Wesen der Funktion des\\nNervensystems besteht in der Kontrolle durch Kommunikation. Das menschliche Gehirn\\nbesteht aus etwa 1010bis1012miteinander vernetzten Nervenzellen, den Neuronen. Etwa\\n10% der Neuronen dienen der Eingabe und Ausgabe. Die restlichen 90% sind mit anderen\\nNeuronen verknüpft, die Informationen speichern oder bestimmte Umwandlungen des\\nSignals vornehmen, das sich durch das Netzwerk fortpﬂanzt. Neuronen sind komplexe\\nZellen, die auf elektrochemische Signale reagieren. Sie setzen sich zusammen aus einem\\nZellkern, einem Zellkörper, mehreren Dendriten, die über Synapsen Eingabeverknüpfun-\\ngen zu anderen Neuronen herstellen, sowie einem Axonstrang, der über Endkolben oder\\nSynapsen ein Aktionspotenzial ausgibt (siehe Abb. 1.1). Ein Neuron kann mit hunderten\\nbis tausenden anderen Neuronen verbunden sein. Die Verbindungen erfolgen über zwei\\nallgemeine Synapsentypen: exzitatorische (erregende) und inhibitorische (hemmende).\\nDie neuronale Aktivität wird bestimmt durch die Entstehung eines internen elektrischen\\nPotenzials, dem Membranpotenzial. Dieses Potenzial kann durch die Eingabeaktivitäten\\nseitens anderer Zellen über die Synapsen verstärkt oder abgeschwächt werden. Wenn\\ndie kumulativen Eingaben das Potenzial über einen Schwellenwert heben, sendet das\\nNeuron Impulse aus, indem es eine Folge von Aktionspotenzialen über das Axon aus-\\nschüttet. Diese Impulse bewirken, dass eine chemische Substanz, der Neurotransmitter,\\nan die Synapsen ausgegeben wird, die wiederum andere Neuronen erregen oder hem-\\nmen können. Das Axonsignal ist wegen des Schwellenwerts von Natur aus binär. Die\\nnicht-binären Informationen, die im Nervensystem verarbeitet werden, sind nicht durch\\ndie Größe der Spannungen, sondern durch die zeitlichen Abstände des Aktionspotenzials\\n11 Einleitung\\ncodiert. Das Nervensystem arbeitet demnach mit Frequenzmodulation ( FM), also wie\\nbspw. eine UKW -Radiostation. Die Zeit, die ein Reiz zum Durchqueren einer Synapse\\nbenötigt, beträgt etwa 1 Millisekunde ( ms). Nach dem Feuern entsteht eine unempﬁnd-\\nliche Phase, die etwa 10 msdauert und während derer das Neuron nicht feuern kann.\\nPro Sekunde können fünfzig bis mehrere hundert Ausschüttungen auftreten. Die Taktfre-\\nquenz des biologisch neuronalen Netzwerks liegt damit maximal im unteren kHz -Bereich\\nund ist um mehrere Dimensionen kleiner als die Geschwindigkeit der Prozessoren eines\\nkonventionellen Computersystems, welches auf der Von-Neumann-Architektur basiert.\\nDie Leistungen des menschlichen Gehirns beruhen daher in erster Linie auf der hohen\\nParallelität bei der Informationsverarbeitung. Synapsen können wachsen, verkümmern\\noder ganz verschwinden. Umgekehrt kann ein Axon neue Zweige mit den zugehörigen\\nSynapsen ausbilden und dadurch mit weiteren Nervenzellen in Kontakt treten. Diese\\nWachstumsprozesse sind für Gedächtnis und Lernen verantwortlich.\\nAbbildung 1.1: Darstellung miteinander vernetzter Neuronen [Kru+15]\\nGeschichtlicher Abriss Die Entwicklung der KNN begann in den 1940er Jahren. Der\\nNeurophysiologe Warren S. McCulloch und der Mathematiker Walter Pitts abstrahierten\\nvon den biologischen Vorgängen und schufen 1943 ein Modell des essentiellsten Gehirn-\\nbausteins auf Basis logischen Kalküls, das künstliche Neuron [MP43]. Dieses McCulloch-\\nPitts-Neuron oder Schwellenwert-Neuron genügt fünf Annahmen:\\n- Ein Neuron ist ein binäres Schaltelement, es ist entweder aktiv oder inaktiv.\\n- Jedes Neuron besitzt einen festen Schwellenwert.\\n- Ein Neuron empfängt Eingaben von exzitatorischen Synapsen gleichen Gewichts.\\n-Ein Neuron empfängt Eingaben von inhibitorischen Synapsen, deren Effekt absolut\\nist: eine aktive inhibitorische Synapse verhindert die Aktivierung des Neurons.\\n-Es gibt ein Zeitquantum für die Integration der synaptischen Eingaben. Wenn keine\\ninhibitorische Synapse aktiv ist, werden die exzitatorischen Eingaben addiert und\\ndas Neuron wird aktiv, wenn sein Schwellenwert dadurch überschritten wird.\\n1949 formulierte Donald O. Hebb, ebenfalls Neurophysiologe, ein Modell des mensch-\\nlichen Lernens, das die Lernvorgänge mit der Anpassung von Neuronenverbindungen\\nbei gleichzeitiger Aktivität miteinander verbundener Neuronen begründet [Heb49]. Das\\nModell wurde als Hebbsche Lernregel bekannt.\\nDie Arbeiten in den 1940er Jahren interessierten Forscher verschiedener Gebiete, darunter\\nden Entdecker des modernen Computermodells, John von Neumann, und den Mitbegrün-\\nder der Kybernetik, Norbert Wiener. Aufbauend auf der Hebbschen Lernregel kombinierte\\nder Psychologe Frank Rosenblatt das McCulloch-Pitts-Neuron in einem Netzwerk und\\n2beschrieb 1958 das Perzeptron [Ros58]. Alle grundlegenden Merkmale heutiger KNN\\nsind enthalten: Lernfähigkeit, Selbstorganisation, Generalisierungsfähigkeit und Fehler-\\ntoleranz. Durch die richtige Wahl des Schwellenwerts konnten diese Konstrukte logische\\nOperationen realisieren (vgl. Kap. 2.2).\\nDas erste praktische einsetzbare KNN konstruierten 1960 Bernhard Widrow und Marcian\\nE. Hoff [WH60]. Ihr Adaline (engl. Adaptive Linear Neuron ) konnte einfache Klassiﬁzie-\\nrungsaufgaben ausführen und kam zum Beispiel für die Dämpfung des Echos in Telefon-\\nleitungen zur Anwendung. Wichtigster Schritt für die Entwicklung der KNN war jedoch\\neine neue Lernregel, die gegenüber dem Perzeptron Vorteile aufwies. Adaline benutzt\\nden kleinsten quadratischen Fehler zwischen gewünschtem und erzeugtem Output als\\nFehlermaß. Die Widrow-Hoff-Regel ist auch als Delta-Regel bekannt.\\n1969 erlitt die Erforschung Künstlicher Neuronaler Netzwerke einen Einbruch. Marvin\\nMinsky und Seymour Papert, die zuvor den Begriff Künstliche Intelligenz geprägt ha-\\nben, veröffentlichten eine vernichtende Kritik an den existierenden Neuronen-Modellen\\n[MP69] mit dem Erfolg, dass zwischen 1970 und 1982 nur noch recht wenige Forscher\\nvoneinander getrennt in einzelnen Disziplinen weiterarbeiteten. Dieses Buch war eine\\nelegante mathematische Analyse der Perzeptronen mit ihren Vor- und Nachteilen. Im\\nWesentlichen wurde dabei gezeigt, welche logischen Funktionen einfache Perzeptronen\\nverarbeiten konnten, welche nicht. Ein Perzeptron ist grundsätzlich nämlich nicht in der\\nLage, die zur Klasse der nichtlinearen separabelen Funktionen gehörende exklusive Oder-\\nFunktion (XOR) zu realisieren.\\nDennoch entstanden in den 1970er Jahren richtungsweisende Arbeiten, die aber aufgrund\\nder Kritik Minskys und Paperts zunächst unbeachtet blieben. Der Elektronik-Ingenieur\\nTeuvo Kohonen entwickelte 1972 ein Neuronen-Modell für Steuerungsaufgaben, den\\nKorrelations-Matrix-Speicher [Koh72]. Der Neurophysiologe James A. Anderson entwi-\\nckelte zeitgleich den linearen Assoziator und das Brain-State-in-a-Box Netzwerk [And70;\\nAnd72] in Anlehnung an natürliche biologische und psychologische Anpassungsvorgän-\\nge. Paul J. Werbos legte 1974 in seiner verkannten Dissertation [Wer74] den Grundstein\\nfür den heute bekanntesten Lernalgorithmus Backpropagation . 1976 entwarfen Stephen\\nGrossberg und Gail Carpenter die Adaptive Resonance Theory ( ART )auf der Basis, wie das\\nGehirn Informationen verarbeitet, nämlich als Kombination eines Top-Down - und Bottom-\\nUp-Ansatzes [Gro76]. Eine Vielzahl von ART-Netzwerken sind seitdem entstanden.\\nDie Renaissance der KNN begann zwischen 1979 und 1982. Wissenschaftler verschiedener\\nDisziplinen (Biologen, Neurologen, Physiker, Mathematiker) entdeckten ein gemeinsames\\nInteresse an der Forschung nach neuen Neuronen-Modellen. Allerdings entstand erst\\n1982 ein merklicher Schub für die neuronale Forschung als der renommierte Physiker und\\nNobelpreisträger John J. Hopﬁeld Neuronen-Modelle für die Erklärung physikalischer\\nPhänomene benutzte [Hop82]. Das Hopﬁeld-Modell führte zur Entwicklung weiterer\\nNetzwerkmodelle, die sich an den physikalischen Energiegesetzen orientierten. Ebenfalls\\nim Jahr 1982 stellte Teuvo Kohonen seinen Ansatz über die selbstorganisierende Bildung\\ntopologisch korrekter Merkmalskarten vor [Koh82]. Die Idee Kohonens besteht darin,\\neine Schicht von Neuronen zu bilden, die auf eine Eingabe mit der Aktivierung einer\\nbestimmten Region reagieren. Ähnliche Eingaben sollen benachbarte Regionen erregen.\\nDie 1985 von David H. Ackley, Geoffrey E. Hinton und Terence J. Sejnowski vorgestellte\\nBoltzmann-Maschine verwendet Hopﬁeld-Neuronen und hat ein physikalisches Analo-\\n31 Einleitung\\ngon: das langsame Abkühlen ( Simulated Annealing ( SA)) einer Kristallschmelze [AES85].\\nDie Boltzmann-Maschine war das erste KNN , das in der Lage ist, Neuronen innerer\\nSchichten zu trainieren.\\nDer entscheidende Durchbruch KNN kam 1986 als zeitgleich und unabhängig voneinan-\\nder David E. Rummelhart, Geoffrey E. Hinton und Ronald J. Williams [RHW86a; RHW86b]\\nsowie David B. Parker [Par85] und Yann LeCun [LeC86] eine neue Lernregel, die genera-\\nlisierte Delta-Regel, vorstellten, die die Kritik am Perzeptron aufhob. Diese heute als Back-\\npropagation bekannte Lernregel ist in der Lage, die Verbindungsgewichte zu inneren Ein-\\nheiten in mehrschichtigen KNN zu bestimmen. Die Idee besteht darin, den Fehler, den das\\nNetz bei der Erzeugung einer Ausgabe macht, rückwärts durch das Netzwerk, also von\\nder Ausgabe- zur Eingabeschicht, weiterzureichen (zu propagieren) und zur Gewichtsver-\\nänderung zu verwenden (vgl. Kap. 3.2). Seit Ende der 1980er Jahre werden vollständig-\\nvernetzte Feedforward -Netzwerke, die sogenannten Multilayer Perceptron ( MLP ), sehr er-\\nfolgreich mit dem Backpropagation -Algorithmus trainiert und für zahlreiche Anwendun-\\ngen eingesetzt.\\n1990 hat Jeffrey L. Elman dann eine neue Struktur für Rückkopplungen in einem KNN\\nvorgeschlagen, welches als einfaches Recurrent Neural Network ( RNN )den Namen Elman-\\nNetz bekam [Elm90]. Mit rekurrenten und rekursiven Netzwerken lassen sich besonders\\ngut sequentielle Daten wie Zeitreihen verarbeiten. Sehr ähnliche RNN sind die Jordan-\\nNetze, die nach Michael I. Jordan benannt sind [Jor86; Jor97]. 1997 haben Sepp Hochreiter\\nund Jürgen Schmidhuber den Grundstein für eine bis heute sehr erfolgreich verwendete\\nrekurrente Struktur gelegt und das Long Short-Term Memory ( LSTM )entwickelt [HS97].\\nDiese Einheit besitzt eine innere Struktur und besteht aus einer Zelle und mehreren Ga-\\ntes, die den Informationsﬂuss steuern. Ein Jahr später hat erneut Yann LeCun einen sehr\\nwichtigen Beitrag geleistet und das Convolutional Neural Network ( CNN )für den Anwen-\\ndungsbereich der Bilderkennung konstruiert [LeC+98].\\nSeit den 2000er Jahren werden die Netzwerke immer größer bzw. tiefer, um auch kom-\\nplexe Probleme zu bearbeiten. Beim Training dieser tiefen Netze traten aber unerwartete\\nProbleme auf, bspw. verschwindende Gradienten, durch die das Lernverfahren zum Er-\\nliegen kam. Es wurden daher neue Techniken und Netzwerk-Modelle entwickelt, um\\ndiesen Problemen entgegenzuwirken, die man unter dem Schlagwort Deep Learning zu-\\nsammenfassen kann. Ein neues, erwähnenswertes KNN wurde 2006 von Geoffrey E. Hin-\\nton entwickelt [HOT16]: Deep Belief Network ( DBN ). In diesem tiefen KNN sind nur die\\nSchichten miteinander verbunden, nicht aber die einzelnen Neuronen. Das DBN kann\\nauch als Konstruktion aus einzelnen RMBs betrachtet werden. Eine Restricted Boltzmann\\nMachine (RBM) ist eine Variante der Boltzmann-Maschine (s.o.).\\nDie 2010er Jahre sind geprägt durch Implementierungen von KNN auf graﬁschen Pro-\\nzessoren (GPU). Diese besitzen sehr viele kleine Recheneinheiten, sodass sich das Training\\ngut damit parallelisieren und schnell ausführen lässt. Die bestehenden KNN-Architekturen\\nwerden immer weiter verfeinert. Mit der Gated Recurrent Unit ( GRU )wurde 2014 eine Va-\\nriante von LSTM vorgestellt, die sehr erfolgreich in der automatischen Textübersetzung\\neingesetzt wird [Cho+14]. Besondere Aufmerksamkeit erlangte im März 2016 das System\\nAlphaGo von Google DeepMind, welches mit Deep Learning trainiert wurde. AlphaGo ist\\nes gelungen, den Südkoreanischen Proﬁspieler Le Sedol unter Turnierbedingungen 4:1 im\\nBrettspiel Go zu schlagen [Wik18a].\\n4Einen sehr guten Überblick über die Geschichte Künstlicher Neuronaler Netzwerke und\\nDeep Learning aus verschiedensten Perspektiven bildet die umfassende Arbeit Deep Lear-\\nning in Neural Networks: An Overview von Jürgen Schmidhuber [Sch14].\\nZiel und Vorgehen Diese Arbeit hat das Ziel, typische Anwendungen durchzuführen,\\nbei denen Künstliche Neuronale Netzwerke (KNN) bzw. Techniken des Deep Learning\\nzum Einsatz kommen, die dann als Basis in der Lehre und der angewandten Forschung\\nan der Hochschule für Technik und Wirtschaft des Saarlandes (htw saar) dienen können.\\nUm dieses Ziel zu erreichen, werden zunächst die theoretischen Grundlagen beschrie-\\nben. In Kap. 2 werden die grundlegenden Bausteine des KNN, das künstliche Neuron\\nbzw. das Perzeptron sowie einfache Topologien und Modelle vorgestellt. Insbesonde-\\nre wird dabei auch auf das Lernen und die damit verbundenen Schwierigkeiten sowie\\nerste Lösungsansätze eingegangen. Die unterschiedlichen Kategorien typischer KNN-\\nAnwendungen werden ebenfalls kurz skizziert. Kap. 3 beschäftigt sich dann mit einer\\nspeziellen KNN-Architektur, dem sogenannten Multilayer Perceptron ( MLP ). Dieses Netz-\\nwerk ist sehr populär, u.a. wegen des gut verstandenen und gut funktionierenden Lern-\\nalgorithmus Backpropagation . Wenn allerdings sehr tiefe MLP trainiert werden, so stößt\\nman auf neue Probleme wie bspw. verschwindende Gradienten. Diese Probleme lassen\\nsich mit Hilfe von Deep Learning meistern, wobei auch neue KNN-Architekturen ent-\\nstanden sind, die in den nächsten zwei Kapiteln dargestellt werden. Kap. 4 stellt das\\nConvolutional Neural Network ( CNN )vor und Kap. 5 beschäftigt sich mit dem Recurrent\\nNeural Network ( RNN ). Sehr vereinfacht ausgedrückt, können mit CNN Aufgaben der\\nBilderkennung bearbeitet werden, während RNN vornehmlich in der Spracherkennung\\nverwendet werden. In Kap. 6 werden 22 verschiedene Open Source Softwarelösungen zu\\nDeep Learning vorgestellt, die aktuell am Markt vorhanden sind. Im nächsten Kapitel wer-\\nden dann zwei dieser Softwarelösungen (TensorFlow, Keras) verwendet, um drei typische\\nAufgaben mit Hilfe von KNN zu bearbeiten. Zwei dieser Aufgaben stammen aus dem\\nBereich Bilderkennung, wobei eine Data Mining Klassiﬁkation der auf den Bildern identi-\\nﬁzierten Objekte vorgenommen wird. Die dritte Aufgabe fällt in das Gebiet Text Mining :\\nEine Sentiment Analysis wird benutzt, um die positiven und negativen Stimmungen in\\nFilm-Rezensionen zu erkennen, d.h. letztendlich ist es also ebenfalls eine Klassiﬁkation.\\nDiese Aufgaben lassen sich der Domäne Big Data zuordnen, weil digitale Bilder und un-\\nstrukturierte Textdaten verarbeitet werden müssen. Im letzten Kapitel wird die Arbeit\\ndann zunächst zusammengefasst und anschließend wird ein Ausblick auf die weitere\\nEntwicklung gegeben.\\n52 Theoretische Grundlagen\\nIn diesem Kapitel werden zunächst die grundlegenden Bausteine eines Künstlichen Neu-\\nronalen Netzwerks ( KNN ) vorgestellt, zuerst das künstliche Neuron (vgl. Abschnitt 2.1)\\nund dann das Perzeptron (vgl. Abschnitt 2.2). Größere Netzwerke bestehen aus vielen\\nNeuronen, die in mehreren Schichten angeordnet sind, und je nach Topologie werden\\nverschiedene Arten unterschieden (vgl. Abschnitt 2.3). Die Lernfähigkeit ist die wich-\\ntigste Eigenschaft eines KNN . Anhand der gegebenen Daten wird es trainiert und dabei\\nwerden die Netzwerk-Parameter automatisch angepasst (vgl. Abschnitt 2.4). KNN lassen\\nsich für unterschiedliche Aufgabenstellungen verwenden, die nach Kategorien geordnet\\nim Abschnitt 2.5 kurz vorgestellt werden. In Abschnitt 2.6 wird dann ein Bezug zum\\ngeschichtlichen Abriss hergestellt (vgl. Kap. 1) und es werden verschiedene Modelle\\nbzw. Architekturen von KNN mit den jeweiligen Lernverfahren und typischen Anwen-\\ndungsfällen präsentiert. Ein generelles Problem beim Lernen ist die Überanpassung (engl.\\nOverﬁtting ). Um dieses Problem zu vermindern, lassen sich spezielle Techniken wie Early\\nStopping , Kreuzvalidierung ( Cross Validation ), Ausdünnung ( Pruning ) und Regularisierung\\neinsetzen, die in Abschnitt 2.7 behandelt werden.\\n2.1 Künstliches Neuron\\nDas künstliche Neuron ist die kleinste Struktureinheit eines informationsverarbeitenden\\nElements des KNN . Es besteht aus mehreren gerichteten Eingabeleitungen (Dendriten),\\ndie mit Gewichten (Synapsen) versehen sind, einem Berechnungskörper (Zellkörper) und\\neiner Ausgabeleitung (Axon)1. In Abb. 2.1 ist die Funktionsweise eines künstlichen Neu-\\nrons dargestellt.\\nAbbildung 2.1: Struktur eines künstlichen Neurons\\nDie nEingabesignale xiwerden mit Gewichtsfaktoren wimultipliziert und durch eine\\nPropagierungsfunktion p(x1,\\x01\\x01\\x01,xn)zu einem Gesamt- bzw. Netto-Eingabesignal zu-\\nsammengefasst. Eine Aktivierungs- bzw. Transferfunktion f(p(x1,\\x01\\x01\\x01,xn))sorgt dann\\ndafür, dass unter Berücksichtigung des Netto-Eingabesignals ein Ausgabesignal ygene-\\nriert wird.\\n1In Klammern sind die entsprechenden Elemente des biologischen Neurons angegeben (vgl. Kap. 1).\\n72 Theoretische Grundlagen\\nDas KNN wird dann aus diesen künstlichen Neuronen gebildet, wobei man diese Ele-\\nmente in Schichten (engl. Layers ) zusammenfasst. Es gibt eine Eingabeschicht (engl. Input\\nLayer ) und eine Ausgabeschicht (engl. Output Layer ). Die Neuronen der Eingabeschicht\\nbekommen als Eingabesignale externen Input und deren Ausgabesignale werden an die\\nNeuronen der nächsten Schicht als Eingaben weitergeleitet. Die Neuronen der Ausga-\\nbeschicht bekommen als Eingabesignale die Ausgaben der vorgelagerten Schicht und\\ngenerieren als Ausgabesignal externen Output. Häuﬁg gibt es in KNN diese inneren, ver-\\nborgenen Schichten (engl. Hidden Layers ). Die Neuronen in diesen Schichten haben keine\\ndirekte Verbindung mit der Außenwelt, d.h. sie sind nicht mit dem externen Input oder\\nmit dem externen Output verbunden.\\nAls Propagierungsfunktion wird gewöhnlich für alle Neuronen des KNN dieselbe Funk-\\ntion benutzt, und zwar die gewichtete Summe aller Eingaben\\np(x1,\\x01\\x01\\x01,xn) =n\\nå\\ni=1wixi. (2.1)\\nFür die Neuronen in einer Schicht wird normalerweise dieselbe Aktivierungsfunktion ver-\\nwendet. Die Wahl dieser Aktivierungsfunktion ist allerdings sehr stark modellabhängig\\n(vgl. Kap. 3.4).\\n2.2 Perzeptron\\nDas Perzeptron (engl. Perceptron ) ist ein sehr einfaches KNN , das lediglich aus einer ein-\\nzigen Verarbeitungseinheit besteht. Der Name ist vom englischen Begriff Perception abge-\\nleitet und bedeutet Wahrnehmung. Das Perzeptron besteht aus nEingaben xi, die jeweils\\nmit den Gewichten wibehaftet sind. Das Perzeptron kann nur Schwellenwertentschei-\\ndungen treffen, d.h. die Ausgabe der Einheit ist 1, falls w1x1+w2x2+\\x01\\x01\\x01+wnxn\\x15Qgilt,\\nwobei Qder sogenannte Schwellenwert (engl. Bias) der Einheit ist. Falls w1x1+w2x2+\\n\\x01\\x01\\x01+wnxn<Qgilt, wird null ausgegeben. In Abb. 2.2 ist der Aufbau des Perzeptron\\nschematisch dargestellt.\\nAbbildung 2.2: Struktur eines Perzeptrons\\nMit Perzeptronen können logische Funktionen wie UND (engl. AND ) bzw. ODER (engl.\\nOR) bzw. NICHT (engl. NOT ) einfach realisiert werden (siehe Abb. 2.3).\\n82.3 Topologie\\nAbbildung 2.3: Darstellung logischer Schaltungen mittels Perzeptronen\\nDie AND-Verknüpfung zweier binärer Variablen x1und x2kann durch ein Perzeptron\\nmit zwei Eingabeleitungen implementiert werden, bei der w1und w2beide gleich 1 sind\\nundQ=2gilt. Das Perzeptron wird nur dann eine 1 feuern, wenn x1+x2\\x152erfüllt ist,\\nd.h. bei binären Eingaben genau dann, wenn x1=x2=1gilt. Bei der OR-Verknüpfung\\nwird entsprechend Q=1gesetzt. Es genügt also, wenn x1=1oder x2=1gilt. Zur\\nDarstellung der NOT-Verknüpfung genügt eine Eingabeleitung x1, die mit dem Gewicht\\nw1=\\x001versehen wird. Der Schwellenwert ist hier null. Die logische Funktion XOR , also\\ndas exklusive ODER, lässt sich jedoch nicht mit einem einzelnen Perzeptron realisieren\\n[MP69].\\n2.3 Topologie\\nKünstliche Neuronale Netzwerke lassen sich hinsichtlich ihrer Topologie klassiﬁzieren:\\n-Vorwärtsgekoppelte Netzwerke (engl. Feedforward Networks ) sind Netzwerke, in\\ndenen die Verbindungen nur in eine Richtung gehen, von der Eingabe zur Ausgabe.\\n-Rekurrente Netzwerke (engl. Recurrent Networks ) sind Netzwerke, in denen es auch\\nVerbindungen in Rückwärtsrichtung gibt, sodass Rückkopplungen entstehen.\\n-Zyklische Netzwerke sind Netzwerke, in denen sich einige Neuronen gegenseitig\\nreizen. Auch hier gibt es also Rückkopplungen.\\n-Vollkommen verbundene Netzwerke sind Netzwerke, in denen jedes Neuron mit\\nallen anderen Neuronen verbunden ist (meistens bis auf sich selbst).\\n-Geschichtete Netzwerke sind Netzwerke, in denen die Neuronen in getrennten,\\nnicht verbundenen Mengen angeordnet sind.\\n-Symmetrische Netzwerke sind Netzwerke, in denen die Verbindung zwischen zwei\\nbeliebigen Neuronen in beiden Richtungen gleich ist.\\n-Selbstassoziative Netzwerke sind Netzwerke, in denen Eingabe- und Ausgabe-\\nNeuronen übereinstimmen.\\n-Stochastische Netzwerke sind Netzwerke, in denen eine gewisse Wahrscheinlich-\\nkeit besteht, dass ein Neuron nicht aktiviert wird, obwohl es Reize bekommen hat.\\n-Asynchrone Netzwerke sind Netzwerke, in denen die Neuronen nicht alle auf ein-\\nmal (synchron), sondern zufällig eins nach dem anderen aktiviert werden.\\n2.4 Lernen\\nEin wesentliches Merkmal von Künstlichen Neuronalen Netzwerken ist die allgemeine\\nLernfähigkeit, d.h. KNN lernen aus Erfahrungen, die sie durch präsentierte Trainingsda-\\nten gewinnen. Lernen heißt Selbstanpassung der Parameter, also der Gewichtungsfakto-\\nren zwischen den Neuronen des KNN , sodass das Netzwerk das gewünschte Verhalten\\n92 Theoretische Grundlagen\\nzeigt. Die Schwellenwerte (engl. Bias) der Neuronen sind ebenfalls anpassbare Parameter\\nund können als zusätzliche Gewichtungsfaktoren interpretiert werden. Lernmethoden für\\nKNN können in drei grundsätzliche Kategorien eingeteilt werden: überwacht, bestärkt\\noder nicht-überwacht.\\nBeim überwachten Lernen (engl. Supervised Learning ) ist während des Lernprozesses ein\\nLehrer anwesend, und jedes Beispielmuster für das Training des Netzwerks beinhaltet\\nein Eingabemuster sowie ein Ziel oder ein gewünschtes Ausgabemuster, d.h. die korrekte\\nAntwort. Während des Lernprozesses kann ein Vergleich zwischen der vom Netzwerk\\nberechneten und der korrekten Ausgabe angestellt werden, um den Fehler festzustellen.\\nDer Fehler kann dann verwendet werden, um die Netzparameter (Gewichtungsfaktoren)\\nentsprechend anzupassen, sodass der Netzwerk-Fehler reduziert wird. Nachdem die Ge-\\nwichtungen für alle Trainingsmuster iterativ angepasst wurden, konvergieren die Gewich-\\ntungswerte gegen eine Wertemenge, mit der die erforderlichen Mustervorgaben erzielt\\nwerden können. Das Lernen wurde dann erreicht, wenn die Fehler für alle Trainingsmus-\\nter auf einen akzeptablen Wert für neue Muster, die sich nicht in der Trainingsmenge\\nbeﬁnden, minimiert wurde. Überwachtes Lernen wird auch induktives oder assoziatives\\nLernen genannt.\\nBeim bestärkenden Lernen (engl. Reinforcement Learning ) wird ebenfalls die Anwesenheit\\neines Lehrers vorausgesetzt, aber dem Netzwerk wird die korrekte Antwort nicht präsen-\\ntiert. Stattdessen erhält es nur einen Hinweis darauf, ob die berechnete Antwort richtig\\noder falsch ist. Anhand dieser Information muss das Netzwerk seine Performance verbes-\\nsern. Normalerweise erhält es eine Belohnung, indem die Gewichtungen für Einheiten,\\ndie die richtige Antwort erzeugt haben, erhöht werden, während für die Einheiten mit\\nder falschen Antwort die Gewichtungswerte reduziert werden.\\nBeim nicht-überwachten Lernen (engl. Unsupervised Learning ) bzw. entdeckendem Ler-\\nnen erhält das Netzwerk kein Feedback über die gewünschte oder die korrekte Ausgabe.\\nEs gibt keinen Lehrer, der Zielmuster präsentiert. Deshalb muss das System durch Ent-\\ndecken und die Übernahme strukturierter Eigenschaften der Eingabemuster lernen, d.h.\\ndurch die Anpassung an statistische Gleichmäßigkeiten oder Muster-Cluster aus den\\nEingabebeispielen der Trainingsdaten. Dieses Lernen kann durch die Verstärkung aus-\\ngewählter Gewichtungen bewerkstelligt werden, sodass eine Übereinstimmung mit zen-\\ntralen, prototypischen Trainingsmustern erzielt wird, welche eine repräsentative Gruppe\\nähnlicher Muster oder Cluster darstellen.\\nIm Folgenden werden einige Methoden beschrieben, die beim überwachten und nicht-\\nüberwachten Lernen zum Einsatz kommen. Das Hebbsche Lernen stellt eine Form der\\nkorrelativen Gleichgewichtsanpassung dar. Die dafür grundlegende Theorie wurde von\\nDonald Hebb vorgestellt [Heb49]. Wenn ein Axon der Zelle A nah genug an einer Zelle\\nB liegt, um diese zu erregen, und wiederholt oder andauernd feuert, erfolgt in einer oder\\nbeiden Zellen ein Wachstumsprozess oder eine metabolische Veränderung, so dass sich\\ndie Einﬂussefﬁzienz von A auf B erhöht. Für diese Hebbsche Lernregel wurden zahlrei-\\nche Varianten vorgeschlagen, unter anderem eine Gewichtsanpassung basierend auf der\\nMinimierung einer Energie- oder Entropiefunktion.\\nBeim konkurrierenden Lernen werden die Gewichtungen so angepasst, dass Neuronen,\\ndie am stärksten auf einen Eingabereiz reagieren, bevorzugt werden. Die Gewichtsanpas-\\nsung ist normalerweise eine modiﬁzierte Form der Hebbschen Anpassung. Die Neuro-\\n102.4 Lernen\\nnen in dieser sogenannten Wettbewerbsschicht konkurrieren. Das Neuron, das die größte\\nÜbereinstimmung seiner Gewichtsinformation mit dem Eingabemuster feststellt, gewinnt\\n(Siegerneuron, engl. Winner-Takes-All ), alle anderen verlieren.\\nDas Stochastische Lernen verwendet Wahrscheinlichkeitsfunktionen, um die Anpassung\\nder Gewichtungen vorzunehmen. Das Simulated Annealing ( SA)gehört zu dieser Klasse\\nvon Lernmethoden. Genau genommen ist es kein Lernalgorithmus, sondern ein heuristi-\\nsches Approximationsverfahren. Aufgrund der hohen Komplexität des Optimierungspro-\\nblems können nicht alle Möglichkeiten ausprobiert werden, stattdessen wird eine Nähe-\\nrung benutzt. Die Grundidee stammt vom physikalisch-chemischen Abkühlungsprozess\\nin der Metallurgie. Durch das langsame und kontrollierte Abkühlen werden die Atome\\neinen neuen, geordneten Zustand überführt und bilden stabile Kristalle. Dieser energiear-\\nme Zustand liegt nahe am globalen Minimum.\\nMehrere Lernparadigmen basieren auf der Reduzierung des Netzwerk-Fehlers mit Hilfe\\nderMethoden des steilsten Gradienten . Diese Methoden machen es erforderlich, dass\\ndie Aktivierungsfunktion stetig differenzierbar ist. Ausgehend vom aktuellen Fehler wird\\ndie Richtung ermittelt, in der sich der Fehler am schnellsten verringert. Damit entspricht\\nder Lernalgorithmus dem Suchen des globalen Minimums in einem nichtlinearen Glei-\\nchungssystem. Abb. 2.4 zeigt das Prinzip des steilsten Gradientenabstiegs im zweidimen-\\nsionalen Fehlerraum.\\nAbbildung 2.4: Gradientenabstiegsverfahren im zweidimensionalen Fehlerraum\\nDie Probleme, die bei dieser Lernmethode auftreten können, sind Oszillationen der Feh-\\nlerfunktion, eine hohe Anzahl von Iterationsschritten des Algorithmus aufgrund von\\nPlateaus innerhalb der Fehlerfunktion, Abbruch des Algorithmus in einem lokalen Mini-\\nmum der Fehlerfunktion und das Verlassen des globalen Minimums der Fehlerfunktion\\n(siehe Abb.2.5).\\nIn Kap. 3.3 werden Möglichkeiten beschrieben, diese typischen Probleme des Gradienten-\\nabstiegverfahrens zu verringern bzw. zu vermeiden.\\n112 Theoretische Grundlagen\\nAbbildung 2.5: Typische Probleme des Gradientenabstiegsverfahrens [Fü96]\\n2.5 Anwendungskategorien\\nEin KNN kann als Black Box Modell betrachtet werden (siehe Abb. 2.6). Es bekommt\\nDaten als Eingaben ( Input ) und generiert Ausgaben ( Output ). Dabei bleibt die innere\\nStruktur verborgen und die Parameter des KNN werden so angepasst, dass ein Verhalten\\nzwischen Input und Output gelernt und abgebildet wird, ohne die eigentlichen kausalen\\nZusammenhänge, d.h. Ursache-Wirkungs-Beziehungen, zu kennen. Das KNN ist also ein\\nModell zur Approximation beliebiger, funktionaler Zusammenhänge.\\nAbbildung 2.6: Black Box Modell\\nEin altes Problem der Approximationstheorie besteht darin, eine vorgegebene Funktion\\ndurch die Komposition primitiver Funktionen exakt oder annähernd auszudrücken. Ein\\nklassisches Beispiel ist die Approximation von eindimensionalen Funktionen mittels Poly-\\nnomen. Die Taylorsche Reihe für eine Funktion F(x), die am Punkt x0approximiert wird,\\nhat die Form\\nF(x) =n\\nå\\ni=1wi(x\\x00x0)i, (2.2)\\nwobei die Konstanten wivom Wert der Funktion Fund ihrer Ableitung am Punkt x0\\nabhängen. Für n!¥wird der Funktionswert exakt durch die Taylorsche Reihe ausge-\\ndrückt.\\nAbb. 2.7 zeigt, wie die polynomielle Approximation einer Funktion F(x)als Funktio-\\nnennetz dargestellt werden kann. An den Knoten des Netzes werden die Funktionen\\nz!z0,z!z1,\\x01\\x01\\x01,z!zn, also die Summanden der Taylorschen Reihe, berechnet. Der\\nAusgabeknoten sammelt additiv die ankommenden Informationen und gibt den Wert des\\nausgewerteten Polynoms aus. Die Netzgewichte wikönnen mit Hilfe eines Lernalgorith-\\nmus bestimmt werden.\\n122.5 Anwendungskategorien\\nAbbildung 2.7: Taylorsche Reihe als Funktionennetz\\nNach dem Theorem von Andrej N. Kolmogorov [Kol57; HN87b] ist es möglich, mehrdi-\\nmensionale stetige Funktionen als ein endliches Netz primitiver eindimensionaler Funk-\\ntionen darzustellen. Die Bestimmung der Netzparameter in einem KNN ist normaler-\\nweise ein NP-vollständiges Problem2. Mit Hilfe des Erfüllbarkeitsproblems kann jedoch\\ngezeigt werden, dass sich ein NP-vollständiges Problem in polynomieller Zeit auf ein\\nLernproblem für KNN reduzieren lässt [GJ79].\\nKünstliche Neuronale Netzwerke werden häuﬁg im Bereich Data Mining eingesetzt. Nach\\nWilliam J. Frawley versteht man unter Data Mining [FPS91]:\\n« die Extraktion und Entdeckung von implizitem, bisher nicht bekanntem und poten-\\nziell nützlichem Wissen aus Daten. »\\nIn diesem Zusammenhang wird auch oft der Begriff Wissensentdeckung in Datenbanken\\n(engl. Knowledge Discovery in Databases ( KDD )) benutzt, den Usama M. Fayyad folgender-\\nmaßen deﬁniert [Fay+96]:\\n« KDD ist der nicht-triviale Prozess der Identiﬁzierung valider, neuer, potenziell nütz-\\nlicher und schließlich verständlicher Muster in Daten. »\\nDaraus lässt sich die folgende Aussage ableiten [Fay+96]:\\n« Data Mining ist ein Schritt im KDD-Prozess; es ist die Anwendung speziﬁscher\\nAlgorithmen zur Extraktion von Mustern aus Daten. »\\nAllgemein lassen sich die Aufgaben bzw. Anwendungen im Bereich Data Mining in vier\\nKategorien einteilen (siehe Abb. 2.8). Bei Prognosen besteht die Aufgabe darin, einen\\nAusschnitt aus der Wirklichkeit zu modellieren, um Vorhersagen, d.h. Prognosen über zu-\\nkünftige Entwicklungen in diesem Ausschnitt treffen zu können. Prognoseanwendungen\\nvon KNN basieren typischerweise auf Zeitreihenanalysen, d.h. einer Zahlenfolge liegt\\nein funktionaler Zusammenhang zugrunde, der in einem mathematischen Modell abge-\\nbildet wird. Es müssen dabei keine expliziten Annahmen über die Zusammenhänge in\\nden Daten gemacht werden ( Black Box Modell). Ein KNN zur Vorhersage zukünftiger Ent-\\nwicklungen wird mit Gegenwarts- und Vergangenheitsdaten trainiert (Ex-Post-Analyse).\\n2NPsteht für nichtdeterministisch polynomiale Komplexität. Das bedeutet, dass kein Algorithmus bekannt\\nist, der alle Instanzen des Problems in polynomieller Komplexität lösen kann.\\n132 Theoretische Grundlagen\\nEs lernt also, wie sich Zustände der Vergangenheit entwickelt haben und überträgt die-\\nse Entwicklung auf die Gegenwart oder Zukunft. Prinzipiell sind alle KNN, die mittels\\nAssoziationen lernen, für diese Aufgaben anwendbar.\\nAbbildung 2.8: Data Mining Kategorien\\nPrognose und Klassiﬁkation sind eng miteinander verknüpft. Das Ziel einer Klassiﬁ-\\nkation ist letztendlich ebenfalls eine Prognose. Der wesentliche Unterschied zwischen\\nPrognose- und Klassiﬁkationsanwendungen liegt in der Art der Eingabe- und Ausgabe-\\ndaten. Im Gegensatz zu Prognosen bestehen die Eingaben bei Klassiﬁkationen aus stati-\\nschen, zeitpunktbezogenen Daten (z.B. abgeschlossenen Kreditfällen). Der Prognosewert\\neiner Klassiﬁkation entspricht daher auch nicht einer Zeitreihenfortsetzung. Prognosen\\ndurch Klassiﬁkation beruhen auf der Annahme, dass die vergangenheitsbezogenen Daten\\ntypische Muster enthalten, die erstens eine Klassiﬁzierung mittels einer Trennfunktion\\nerlauben und zweitens, dass gefundene Trennfunktionen auch für künftige, unbekannte\\nMuster gültig sind. Die Ausgabedaten bei der Prognose sind normalerweise kontinuier-\\nliche Werte, die kardinalskaliert sind, während bei der Klassiﬁkation diskrete Werte, die\\nnominalskaliert sind, betrachtet werden. Im Beispiel Kreditvergabe wird das KNN mit\\nden Daten, der bereits abgeschlossenen Kreditfälle, trainiert. Die Klassen könnten also\\nsein: Kredit wurde zurückgezahlt oder nicht, d.h. im ersten Fall gab es keine Ausfälle und\\nim zweiten Fall hat der Kunde die vereinbarten Raten nicht pünktlich oder sogar gar nicht\\nzurückzahlen können. Man spricht hier auch von einer binären Klassiﬁkation, weil es nur\\nzwei unterscheidbare Klassen gibt. Das KNN wird nun mit den verfügbaren Daten der\\nKunden trainiert und hierzu können Merkmale wie bspw. Alter, monatliches Einkommen,\\nGeschlecht, Familienstatus usw. verwendet werden. Möchte nun ein neuer Kunde einen\\nKreditvertrag abschließen, dann ﬂießen seine Daten in das trainierte Modell ein und eine\\nder beiden Klassen wird bestimmt. Im ersten Fall gilt der Kunde als kreditwürdig und\\nbekommt diesen, während im zweiten Fall kein Kredit gewährt wird. Die Anwendung\\ndes Klassiﬁzierers ist also eigentlich eine Prognose in die Zukunft. Die Methoden der\\nPrognose und Klassiﬁkation werden also auch im Bereich Predictive Analytics eingesetzt.\\nDieses einfache Beispiel zeigt auch bereits die ethischen Probleme, die mit dem Einsatz\\ndas maschinellen Lernens verbunden sind. Ggf. werden einzelne Personen benachteiligt,\\nweil sie die falschen Merkmale aufweisen.\\n142.6 Modelle\\nKlassiﬁkation und Segmentierung sind auf dem ersten Blick ähnliche Anwendungen,\\nauf dem zweiten Blick gibt es aber deutliche Unterschiede. Die Klassiﬁkation gehört zum\\nüberwachten Lernen und die Segmentation zum nicht-überwachten Lernen (vgl. Kap.\\n2.4). Während bei der Klassiﬁkation alle Trainingsdaten bereits in Klassen eingeteilt sind,\\nmuss das Segmentierungsverfahren diese Trennung selbst vornehmen. Dabei wird nach\\nÄhnlichkeiten als Muster in den Daten gesucht. Ähnliche Datensätze bzw. Objekte wer-\\nden in möglichst homogene Teilmengen (Segmente, Gruppen, Cluster) einsortiert. Diese\\nTeilmengen sollten aber wiederum sehr verschieden voneinander sein, d.h. zwischen\\nden Teilmengen sollte eine möglichst große Heterogenität vorliegen. Spezielle Typen von\\nKNN, wie bspw. das Kohonen-Netz, sind geeignet, um mit dem nicht-überwachten Ler-\\nnen diese Segmentierung automatisch vorzunehmen.\\nIn die vierte Kategorie fallen Anwendungen, in denen nach Assoziationen zwischen den\\nbetrachteten Objekten gesucht werden. Damit sind Zusammenhänge, meistens (lineare)\\nKorrelationen, zwischen dem gemeinsamen häuﬁgen Auftreten von zwei oder mehreren\\nEreignissen gemeint. Ein Beispiel ist die Warenkorbanalyse, in der untersucht wird, wel-\\nche Produkte häuﬁg zusammen gekauft werden, um daraus Regeln abzuleiten, die dann\\nfür Produktempfehlungen verwendet werden können: z.B. ”Kunden, die dieses Produkt\\ngekauft haben, kauften auch ...” Einige KNN lassen sich als sogenannte Assoziativspei-\\ncher einsetzen. Assoziationen sind gelernte Verbindungen zwischen den Eingaben und\\nAusgaben. Das menschliche Gedächtnis funktioniert sehr ähnlich und arbeitet ebenfalls\\nmit Assoziationen, die als Erinnerungen an bestimmte Erlebnisse gespeichert werden.\\nDurch ähnliche Eingaben werden Assoziationen geweckt und das gespeicherte Muster\\nmit den größten Ähnlichkeiten als Ausgabe präsentiert.\\n2.6 Modelle\\nEin vollständiger Überblick zu allen existierenden Modellen KNN ist angesichts der un-\\nüberschaubaren Anzahl bekannter, modiﬁzierter und vollkommen neuer Typen kaum\\nmehr möglich. Im Folgenden werden daher einige bekannte Modelle kurz vorgestellt.\\nDabei wird auch auf die Lernalgorithmen und Einsatzgebiete eingegangen.\\nPerzeptron Das Perzeptron wurde bereits in Kap. 2.2 behandelt. Die Lernregel eines\\nPerzeptrons lässt sich folgendermaßen beschreiben:\\n- Wenn die Ausgabe eins (aktiv) ist und eins sein soll oder wenn sie null (inaktiv) ist\\nund null sein soll, dann werden die Gewichtungen nicht verändert.\\n-Wenn die Ausgabe null ist, aber eins sein sollte, werden die Gewichtungen für alle\\naktiven Eingabeverknüpfungen erhöht.\\n-Wenn die Ausgabe eins ist, aber null sein sollte, werden die Gewichtungen für alle\\naktiven Eingabeverknüpfungen verringert.\\nMathematisch lässt sich der Perzeptron-Lernalgorithmus als Änderung der Gewichte\\nDwi=h(t\\x00y)xi (2.3)\\nschreiben, wobei xidiei-te Eingabe, ydie tatsächlich realisierte Ausgabe und tdie ge-\\nwünschte bzw. erwartete Ausgabe (engl. Teaching Input ) sind. Der Parameter hist der\\nLerngeschwindigkeitskoefﬁzient bzw. die Lernrate. Wenn hsehr klein ist, erfolgt das\\nLernen zwar sehr langsam, aber stabil.\\n152 Theoretische Grundlagen\\nAdaline Das Adaline (engl. Adaptive Linear Neuron ) ist ein einzelnes Neuron mit Schwel-\\nlenwertlogik und einer bipolaren Ausgabe mit den Werten f+1,\\x001g[WH60]. Eingaben\\nfür diese Einheit erfolgen normalerweise auch bipolar. Die Gewichtungen werden nach\\nder Lernregel von Widrow-Hoff, die auf der Minimierung des Netzwerkfehlers beruht,\\nangepasst. Der Gesamtfehler des Netzwerks ist gerade die Summe der Einzelfehler\\nEtot=P\\nå\\np=1Ep, (2.4)\\nwobei Pdie Anzahl der Trainingsmuster (engl. Pattern ) darstellt. Der Einzelfehler ergibt\\nsich aus dem Fehlerquadrat\\nEp=1\\n2(tp\\x00yp)2. (2.5)\\nDie Ausgabe ist gerade die gewichtete Summe der nEingaben abzüglich des Schwellen-\\nwertes Q, also\\nyp= \\nn\\nå\\ni=1wixp\\ni!\\n\\x00Q. (2.6)\\nDie Gewichtsänderung ist proportional zum Gradienten des Gesamtfehlers, also\\nDwi=\\x00h¶Etot\\n¶wi=hP\\nå\\np=1(tp\\x00yp)xp\\ni. (2.7)\\nDie Proportionalitätskonstante hkann hier ebenfalls als Lerngeschwindigkeitskoefﬁzient\\ninterpretiert werden. Für hwird oft der Wert 1/ngewählt. Der Ausdruck (tp\\x00yp)wird\\nhäuﬁg mit dpbezeichnet und deshalb trägt der Lernalgorithmus auch den Namen Delta-\\nRegel. Die Delta-Regel konvergiert, der Fehler behält allerdings einen Wert grösser null bei.\\nAdaline-Netzwerke werden zur Mustererkennung und als Assoziativspeicher benutzt.\\nEin gegebenes KNN mitnEingabeeinheiten kann unter Verwendung der Delta-Regel\\nmaximal nlinear unabhängige Muster fehlerfrei speichern.\\nMadaline Durch die Kombination mehrerer Adaline in einem Netzwerk erhält man\\neinMadaline -Netzwerk (engl. Multiple Adaline ) [WL90]. Die Lernregel basiert auf dem\\nPrinzip der minimalen Störung. Dabei werden die Eingaben jedes Elements gestört, in-\\ndem die Eingabe um einen kleinen Betrag Dsverändert wird und die Änderung in der\\nquadratischen Fehlersumme der Ausgabe beobachtet wird. Man erhält so\\nDwi=hP\\nå\\np=1(tp\\x00yp)xp\\niDf\\nDsp\\ni, (2.8)\\nwobei Dfdie Änderung der Ausgabe (Aktivierungsfunktion) ist. Madaline-Netzwerke\\nerlauben die Realisierung von Abbildungen mit nichtlinearer Teilbarkeit, wie z.B. die\\nXOR-Funktion.\\nHopﬁeld-Netz Hopﬁeld-Netzwerke [Hop82] sind einschichtige, rekursive Netzwerke\\nmit symmetrischen Gewichtungsmatrizen, d.h. es gilt wij=wjifür alle i,j=1, 2, ..., n.\\nHopﬁeld-Netze speichern eine Anzahl Pvon Prototypenmustern, die sogenannten Fix-\\npunkt-Attraktoren. Die gespeicherten Muster können durch eine direkte Berechnung spe-\\nziﬁziert werden, wie etwa mit der Hebbschen Lernregel\\nDwij=hxiyj, (2.9)\\n162.6 Modelle\\noder sie werden anhand eines Aktualisierungsschemas wie der Delta-Regel erlernt. Nach-\\ndem ein Netzwerk PPrototyp-Muster gelernt hat, können diese zum assoziativen Wieder-\\nﬁnden herangezogen werden. Um ein spezielles Muster zu ﬁnden, arbeitet das Netzwerk\\nrekursiv, indem die Ausgabesignale des Netzwerks wiederholt in die Eingaben einﬂießen,\\nund zwar zu jedem Aktualisierungszeitpunkt t, bis das Netzwerk sich schließlich stabi-\\nlisiert. Die Lösung kann zyklisch sein mit fester Periodendauer T. Damit das Netzwerk\\nals Assoziativspeicher dient, muss die Lösung jedoch in endlicher Zeitdauer gegen einen\\nFixpunkt ~x(t+1) =~x(t)konvergieren. Der Status des Netzwerks lässt sich durch die\\nEnergiefunktion\\nE=\\x001\\n2n\\nå\\ni=1n\\nå\\nj=1wijxjxj (2.10)\\nbeschreiben. Hopﬁeld konnte zeigen, dass die Energie irgendwann einen stabilen Zustand\\nerreichen muss, wenn sich das KNN gemäß seiner Dynamik entwickelt, weil die deﬁnier-\\nte Energiefunktion Enicht nach jeder Aktualisierung anwachsen kann. Sie muss kleiner\\nwerden oder zumindest gleichbleiben. Weil es eine endliche Anzahl Systemzustände gibt,\\nmuss das Netzwerk irgendwann gegen ein lokales Minimum konvergieren. Den Energie-\\nminima entsprechen Fixpunkt-Attraktoren, den gespeicherten Mustern. Der Status, den\\ndas System bei Konvergenz annimmt, bestimmt das Ausgabemuster. Hopﬁeld-Netze wur-\\nden hauptsächlich in Optimierungsanwendungen eingesetzt, wie etwa beim NP-Problem\\ndes Handlungsreisenden (engl. Travelling Salesman Problem ( TSP)), bei der Erstellung von\\nZeitplänen oder bei der Funktionsoptimierung.\\nBidirektionaler assoziativer Speicher Der bidirektionale assoziative Speicher (engl. Bi-\\ndirectional Associative Memory ( BAM )) ist einerseits ein generalisiertes, heteroassoziati-\\nves arbeitendes Hopﬁeld-Netzwerk, das andererseits auch Ähnlichkeiten mit dem ART -\\nModell hat [Kos87]. Es besteht aus zwei Schichten, wobei die Neuronen nur binäre Zu-\\nstände annehmen können. Die beiden Schichten sind in beide Richtungen vollständig\\nmiteinander verbunden, wobei die Gewichte symmetrisch sind. Das Lernziel eines BAM\\nist es, die Eingabevektoren der Dimension nmitm-dimensionalen Ausgabevektoren inner-\\nhalb der Gewichtungsmatrix adäquat abzubilden. Der Lernschritt innerhalb eines BAM -\\nNetzwerkes ähnelt dem des Hopﬁeld-Netzes, d.h. es kann nach der Regel von Hebb oder\\nder Delta-Regel trainiert werden. Das BAM lässt sich gut zur Musterergänzung einsetzen,\\nwobei in jede der beiden Schichten ein Teilmuster eingegeben wird.\\nBrain-State-in-a-Box Das Brain-State-in-a-Box ( BSB)ist ein rekursives, autoassoziatives\\nNetzwerk, wobei der Systemzustand innerhalb des quadratischen, schachtelähnlichen\\nHyper-Bereichs, der von den Kanten [+1,\\x001]begrenzt ist, gefangen ist [And72]. Das\\nNetzwerk besteht aus einer einzelnen Schicht von nEinheiten. Es ist in der Struktur\\nund Arbeitsweise ähnlich den Hopﬁeld-Netzen. Die Einheiten dürfen Eigen-Feedback-\\nVerbindungen wii6=0haben, und einige Gewichtungsverbindungen dürfen weggelassen\\nwerden, d.h. für einige (i,j)darf gelten wij=0. Für die Anpassung der Gewichtungen in\\neinem BSB-Netzwerk kann das Hebbsche oder das Delta-Regel-Lernen angewendet wer-\\nden. Wie auch bei den Hopﬁeld-Netzen werden BSB-Netzwerke zur Mustererkennung\\nund Optimierung eingesetzt.\\nDie Boltzmann-Maschine Die Boltzmann-Maschine [AES85] ist ein rekursives, stochas-\\ntisches Netzwerk. Die Statuszustände, die das Netzwerk annehmen kann, werden durch\\n172 Theoretische Grundlagen\\ndie Boltzmann-Verteilung bestimmt, einer exponentiellen Form der Wahrscheinlichkeits-\\nverteilung, die für die Modellierung der Statuszustände eines physikalischen Systems bei\\nthermischem Gleichgewicht genutzt wird. Wie das Hopﬁeld-Netzwerk hat eine Boltzmann-\\nMaschine eine symmetrische Gewichtungsmatrix, die Konvergenz gegen einen stabilen\\nStatus garantiert. Anders als das Hopﬁeld-Netzwerk kann die Boltzmann-Maschine je-\\ndoch verborgene Einheiten haben. Während des Arbeitens und des Trainings der Boltz-\\nmann-Maschine wird eine Lernregel benutzt, die man als kontrolliertes Abkühlen (engl.\\nSimulated Annealing ) bezeichnet. Diese Methode ermöglicht dem Netzwerk lokale Mini-\\nma zu verlassen und gegen einen globalen Gleichgewichtszustand zu konvergieren. Die\\nBoltzmann-Maschine kann als Assoziativspeicher oder zur Lösung von Optimierungspro-\\nblemen verwendet werden.\\nSelbstorganisierende Karten Selbstorganisierenden (sensorischen) Karten (engl. Self-\\nOrganizing Map ( SOM )) werden auch nach ihrem Entwickler Kohonen-Netze genannt\\n[Koh82]. Es sind einschichtige, rückkopplungsfreie Netzwerke, die i.a. bis zu zwei Dimen-\\nsionen aufweisen, wobei jeder Eingang mit allen Elementen verbunden ist. Sie arbeiten\\nauf der Basis des konkurrierenden, nicht-überwachten Lernens (vgl. Kap. 2.4). Eine to-\\npologische Struktur wird auf der Wettbewerbsschicht (Kohonen-Schicht) deﬁniert, d.h.\\nNeuronen der Wettbewerbsschicht werden in Form eines Hyperquaders angeordnet. Das\\nZiel ist es, selbstorganisierende Karten so zu trainieren, dass benachbarte Cluster durch\\nbenachbarte Neuronen der Wettbewerbsschicht repräsentiert werden. Während des Lern-\\nprozesses wird dem Netzwerk eine Folge von Eingabemustern präsentiert, die in der\\nRegel durch eine Wahrscheinlichkeitsverteilung erzeugt wurden. Für die Änderung der\\nGewichtsfaktoren des aktivierten j-ten Neurons gilt\\nDwij=h(xi\\x00wij) (2.11)\\nunter der Nebenbedingung (Normierung)\\nn\\nå\\ni=1wij=1 . (2.12)\\nSOM-Netzwerke werden zur Datenkomprimierung, kombinatorische Optimierung, Ro-\\nbotorsteuerung sowie Sprach- und Mustererkennung eingesetzt.\\nLineare Vektor Quantisierung Die Vektorquantisierung ist ein Prozess, stetig, reellwer-\\ntige Vektoren ~xaus einer Menge A\\x12I Rnauf den nächsten Referenz-Gewichtungswert\\n~wiaus der Menge B\\x12I Rmabzubilden. Die Eingabevektoren ~xder Dimension nwer-\\nden in einer endlichen Anzahl von Klassen transformiert, wobei jede Klasse durch einen\\nPrototyp-Vektor ~widargestellt ist. Normalerweise versteht man unter nächstliegend den\\neuklidischen Abstand\\nd(~x,~wi) =vuutn\\nå\\nj=1(wij\\x00xj)2. (2.13)\\nVektorquantisierungs-Netzwerke arbeiten als konkurrierende Netzwerke. Wenn das Ler-\\nnen überwacht erfolgt, nennt man sie auch lineare Vektorquantisierung (engl. Linear Vec-\\ntor Quantization ( LVQ )). Wenn die Klasse korrekt erkannt wurde, wird der Gewichtungs-\\nvektor der gewinnenden Einheit in Richtung des Eingabevektors verschoben. Wenn ein\\nfalscher Prototyp ausgewählt wurde, wird der Gewichtungsvektor vom Eingabevektor\\nweg verschoben. Diese Verschiebungen sind abhängig von dem euklidischen Abstand\\n(Gl. 2.13). Die LVQ wird in den Aufgabenbereichen Mustererkennung und Datenkompri-\\nmierung eingesetzt.\\n182.6 Modelle\\nDas Kognitron Das Kognitron-Netzwerk [Fuk80] stellt eine hierarchische Struktur dar,\\ndie aus mehreren kaskadenförmig angeordneten, modularen Einheiten besteht, die Signa-\\nle in Vorwärtsrichtung verarbeiten. Diese Netzwerkarchitektur wurde ursprünglich für\\nAufgaben der Bildverarbeitung entworfen und orientiert sich daher stark an das System\\ndes menschlichen Sehens. Die Eingabeschicht (Sensorschicht) ist ein rechteckiges Feld aus\\nRezeptorneuronen mit lichtsensiblen Zellen und ist für die Eigenschaften auf Zell- oder Pi-\\nxelebene verantwortlich. Zellen in höheren Schichten lernen, Low-Level -Eigenschaften (z.B.\\ndas Dach des Buchstaben A in der Zeichenerkennung) zu integrieren, und sind deshalb\\nfür globalere Eigenschaften zuständig. Die Integration setzt sich bis zur Ausgabeschicht\\nfort, die vollständige Objekte im Bild identiﬁziert, wobei für jedes identiﬁzierte Objekt\\neine Zelle erforderlich ist. Das Lernen wird schichtweise und konkurrierend ausgeführt,\\nwobei die Zelle, die am meisten auf das Trainingsmuster reagiert, repräsentativ für diese\\nZellebene wird und deren Gewichtung so angepasst wird, dass sie noch mehr auf das Mus-\\nter reagiert. Diese Netzwerke werden hauptsächlich in Anwendungen für die invariante\\nZeichen- und Objekterkennung eingesetzt.\\nCounterpropagation Counterpropagation-Netzwerke [HN87a] sind dreischichtige, vor-\\nwärtsgekoppelte Netzwerke, die in nahezu perfekter Kombination zwei Lernstrategien\\nvereinigen und die durch diese Kopplung wesentlich leistungsfähiger geworden sind, als\\nNetzwerke mit den isolierten Einzelstrategien. Die Eingabeschicht dient dem Counter-\\npropagation-Netz lediglich der Verteilung der Eingangsaktivitäten, sie führt keine Berech-\\nnungen aus. Jedes Eingabeneuron ist mit jedem Neuron der Kohonen-Schicht über ein\\nGewicht wijund letztere sind wiederum über ein Gewicht wjkmit den Neuronen der Aus-\\ngabeschicht (Grossberg-Schicht) verbunden. Die Kohonenschicht wird nach dem Ansatz\\ndes konkurrierenden Lernens trainiert ( Winner-Takes-All ). Die Grossberg-Schicht dagegen\\nwird durch überwachtes Lernen trainiert. Counterpropagation-Netzwerke werden im Be-\\nreich der Mustererkennung, Mustervervollständigung und Signalverbesserung genutzt.\\nAdaptive Resonanztheorie DieAdaptive Resonance Theory ( ART )wurde als Erweiterung\\nkonkurrierender-kooperierender Lernsysteme entwickelt [Gro76]. Dabei wurde versucht,\\ndas Stabilitäts-Elastizitäts-Problem sowie andere instabile Lerneigenschaften von kon-\\nkurrierenden Netzwerken zu umgehen. Damit KNN vergleichbar mit den biologischen\\nVorbildern arbeiten, müssen sie in der Lage sein, sinnvolle Informationen im Gedächt-\\nnis zu behalten, während gleichzeitig neue wichtige Informationen erlernt, irrelevante\\nignoriert und veraltete oder unwichtige vergessen werden. Mit andern Worten sollen die-\\nse Netzwerke einen hohen Stabilitätsgrad aufweisen, wenn sie adaptiv neue Kategorien\\noder Konzepte lernen, und gleichzeitig anpassbar sein, um diese neuen Kategorien oder\\nKonzepte zu erkennen und zu erlernen, also auch ein hohes Maß an Elastizität aufweisen.\\nDiese beiden Ziele konkurrieren miteinander. ART -Netzwerke bilden n-dimensionale\\nEingabemuster auf Ausgabekategorien oder Klassen ab, die auf den Eigenschaften des\\nEingabemusters basieren. Ähnliche Eingabemuster (nächster Nachbar) werden in dersel-\\nben Klasse gruppiert, nicht-ähnliche Muster in separaten, verschiedenen Klassen. Das\\nLernen in ART -Netzwerken erfolgt während der normalen Arbeitsweise des Netzwerkes\\nin Echtzeit. Dabei handelt es sich um eine Form stetigen, nicht-überwachten adaptiven\\nLernens, wobei automatisch eine neue Kategorie gebildet wird, wenn dem Netz ein neues\\nEingabemuster präsentiert wird. Es werden solange neue Kategorien für neue Eingabe-\\nmuster erzeugt, bis das Netzwerk seinen Pool noch nicht verwendeter Ausgabekategorie-\\nNeuronen erschöpft hat. Alle weiteren neuen Eingabemuster werden zurückgewiesen.\\nEingabemuster, die ähnlich bereits eingerichteten Kategorien sind, werden sofort erkannt,\\nindem an dem Neuron der selektierten Kategorie eine hohe Ausgabe erzeugt wird. Einga-\\n192 Theoretische Grundlagen\\nben, die mit existierenden Kategorien übereinstimmen, initiieren außerdem ein gewisses\\nLernen für diese Kategorie, ohne dass dabei die Stabilität der erlernten Kategorien erhöht\\nwird. Einige Einschränkungen von ART-Netzwerken betreffen ihre allgemeine Komple-\\nxität, Schwierigkeiten bei der Festlegung der Fehlerkriterium-Parameter für bestimmte\\nAnwendungen sowie die relativ inefﬁziente Verwendung von Ausgabeneuronen (für jede\\nerlernte Kategorie ist ein Neuron erforderlich). ART -Netzwerke werden bspw. in Anwen-\\ndungen zur Sensordatenfusion, Diagnose und Steuerung eingesetzt.\\nSonstige In den Lehrbüchern Computational Intelligence von Rudolf Kruse et al. [Kru+15]\\nund Grundkurs Künstliche Intelligenz von Wolfgang Ertel [Ert16] sind einige dieser Modelle\\nausführlich dargestellt. Weitere KNN, die in dieser Arbeit zwar genannt, aber nicht wei-\\nter beschrieben werden, sind: Deep Belief Network ( DBN )[HOT16], Generative Adversarial\\nNetwork ( GAN )[Goo+14], Deep Autoencoder ( DAE )[CYL+14] und Stacked Denoising Auto-\\nencoder ( SDA )[Vin+08]. Entsprechende Quellen zu diesen Netzwerken sind angegeben,\\nsodass sich interessierte Leser sich über den Aufbau, das Training und die Verwendung\\ninformieren können.\\n2.7 Optimierungstechniken\\nDie Entwicklung eines KNN kann man in folgenden Phasen einteilen [Fü96]:\\n1. Deﬁnition des Problems\\n2. Datenakquisition\\n3. Datenpräsentation\\n4. Wahl des Netzwerkmodells\\n5. Wahl der Netzwerkstruktur\\n6. Wahl der Netzparameter\\n7. Training des KNN\\n8. Testen des trainierten KNN\\n9. Anwendung\\nIn Phase 1 muss zunächst untersucht werden, ob das Problem geeignet ist, um mit einem\\nKNN bearbeitet zu werden. Die Beschaffenheit der Daten (Phase 2) muss quantitativen\\n(ausreichende Anzahl) und qualitativen Anforderungen (weitgehende Fehlerfreiheit) ge-\\nnügen. In Phase 3 ﬁndet eine Codierung (binäre, ganze oder reelle Werte), eine Skalierung\\n(symmetrische Häuﬁgkeitsverteilung um den Mittelwert) und eine Normierung (lineare\\nAbbildung auf das Intervall [0, 1]) der Daten statt [Ste93]. Die nächsten drei Phasen befas-\\nsen sich mit der konkreten Ausarbeitung des KNN . Die Fragen, die man hier beantworten\\nmuss, sind u.a. welches Netzwerk-Modell benutzt werden soll, wie viele Schichten mit\\nwie vielen Neuronen in der jeweiligen Schicht das KNN haben soll, wie die Verbindungen\\nzwischen den Neuronen zu gestalten sind, oder welche Aktivierungsfunktionen verwen-\\ndet werden können. In Phase 7 steht man möglicherweise vor der Auswahl mehrerer\\nalternativer Lernregeln. Außerdem werden hier die Daten mindestens in zwei disjunkte\\nMengen aufgeteilt: Trainings- und Testmenge. Die Reihenfolge der Trainingsdaten, die\\ndas Netz zum Lernen benutzt, erfolgt nach Möglichkeit zufallsgesteuert. Beim Testen des\\nNetzes (Phase 8) wird die Leistung des trainierten Modells anhand der Testmenge gemes-\\nsen. Nach dem Training und Testen gelangt das KNN schließlich zur Anwendung bzw.\\nGeneralisierung, vorausgesetzt die Tests waren erfolgreich.\\n202.7 Optimierungstechniken\\nDer effektive Einsatz von neuronalen Systemen erfordert eine Feinabstimmung von Para-\\nmeterwerten. Hierzu sind jedoch nur wenige heuristische Regeln bekannt. Daher ist man\\nweitgehend auf eine experimentelle Vorgehensweise ( Trial-and-Error -Strategie) angewie-\\nsen. Einige Optimierungstechniken werden im Folgenden kurz vorgestellt.\\nEarly Stopping Präsentiert man dem Netzwerk zu häuﬁg die gleichen Trainingsmus-\\nter, dann kann es passieren, dass das Netzwerk diese Muster auswendig lernt. Muster,\\ndie nicht zu dieser Trainingsmenge gehören, werden dann schlechter verarbeitet. Um\\ndieses Überlernen (engl. Overlearning ) bzw. diese Überanpassung (engl. Overﬁtting ) zu\\nverhindern, wird während des Lernens der Netzfehler auf einer Trainings- und einer Vali-\\ndierungsmenge gemessen. Wenn der Netzfehler der Validierungsmenge grösser wird, ist\\ndas Training abzubrechen ( Early Stopping ).\\nAbbildung 2.9: Übertrainieren und Trainingsabbruch\\nKreuz-Validierung Die Methode Kreuz-Validierung (engl. Cross Validation ) kann eine\\nÜberanpassung des Netzwerks vermindern. Die aus NElementen bestehende Datenmen-\\nge wird dabei in k\\x14Netwa gleich große disjunkte Teilmengen T1bisTkaufgeteilt, d.h.\\npartitioniert. Dann werden kTrainings- bzw. Testläufe durchgeführt. Im ersten Durchlauf\\nwird bspw. mit den Mengen T2bisTktrainiert und T1zum Validieren verwendet. Im\\nzweiten Durchlauf wird dann mit den Mengen T1und T3bisTktrainiert und T2zum\\nValidieren verwendet, usw. Im k-ten und letzten Durchlauf wird mit den Mengen T1bis\\nTk\\x001trainiert und Tkzum Validieren verwendet (siehe Abb. 2.10).\\nAbbildung 2.10: k-fache Kreuz-Validierung für k=10 [Bro17]\\n212 Theoretische Grundlagen\\nDieses Verfahren nennt man auch k-fache Kreuzvalidierung. Der Vorteil dieser Methode\\nist, dass zwar nicht gleichzeitig aber zumindest nacheinander, alle Daten sowohl zum\\nTrainieren als auch zum Validieren verwendet werden. Allerdings ist noch offen, wie\\ndie Elemente bzw. Datensätze auf die kdisjunkten Mengen verteilt werden ( Sampling ).\\nDies könnte bspw. nach dem Zufallsprinzip erfolgen. Eine Weiterentwicklung stellt die\\nk-fache stratiﬁzierte Kreuzvalidierung dar. Bei der Aufteilung der kTeilmengen wird da-\\nfür gesorgt, dass jede dieser Mengen annähernd die gleiche Verteilung bezüglich eines\\nMerkmals besitzt. Ein Spezialfall stellt die Leave-One-Out -Kreuzvalidierung dar. Hier gilt\\nk=N, d.h. die Anzahl der Durchläufe beträgt Nund die jeweilige Test- bzw. Validie-\\nrungsmenge besteht nur aus einem Datensatz. Der Vorteil dieser Methode ist, dass sich\\ndadurch verschiedene Modelle sehr gut miteinander vergleichen lassen, da Zufallseffek-\\nte in der Aufteilung nicht mehr relevant sind. Ein Nachteil ist allerdings, dass bei einer\\ngroßen Datenmenge sehr viele Durchläufe notwendig sind und dementsprechend das\\nTraining sehr viel Zeit in Anspruch nimmt.\\nAusdünnungstechniken Sehr komplexe Modell können Übertrainieren begünstigen. Es\\ngibt verschiedene Möglichkeiten, Komplexität im Netzwerk durch topologieverändernde\\nEingriffe zu reduzieren. Die sogenannten Ausdünnungstechniken (engl. Pruning ) wer-\\nden benutzt, um die Anzahl der Neuronen bzw. Parameter zu reduzieren. Ansatzpunkt\\nist dabei das Gewicht oder die Ausgabe der Neuronen. Aufgrund einer Sensitivitätsana-\\nlyse werden unwichtige Eingabeneuronen, die den Fehler des Netzwerks nur marginal\\nbeeinﬂussen, ausﬁndig gemacht und entfernt ( Input-Pruning ). Unter Weight-Pruning sol-\\nlen hier allgemein Verfahren verstanden werden, die geeignet sind, nicht benötigte Ge-\\nwichte aus einem Netzwerk zu entfernen oder auf null zu setzen. Die Standardmethode\\n(Kleinste Gewichte) benutzt als Testgröße den Betrag des Gewichts. Bei dem Pruning\\nnach statistischer Signiﬁkanz [HFZ92] werden Informationen über die Verteilung (z.B.\\nStandardabweichung) der Gewichtsveränderungen für jede Verbindung im Netzwerk\\nbetrachtet. Das Optimal-Brain-Damage [LDS89] ist eine Methode, die die zweite Ablei-\\ntung der Fehlerfunktion bei der Gewichtsänderung berücksichtigt. Eine andere Variante\\nbesteht darin, paarweise Korrelationsanalysen der Neuronenaktivitäten in verborgenen\\nSchichten durchzuführen [ZHF92]. Zwei Neuronen mit hohen Korrelationskoefﬁzienten\\nkönnen dabei zusammengelegt werden ( Hidden Merging ). Die Betrachtung der Varianz\\nder Neuronenaktivitäten erlaubt ferner, Neuronen zu identiﬁzieren, die unabhängig vom\\nanliegenden Eingabemuster stets dieselbe oder annähernd gleiche Ausgabe produzieren.\\nSie wirken faktisch als Schwellenwert und können komplett entfernt werden.\\nRegularisierung Eine einfache Möglichkeit, die Komplexität des Netzwerks während\\ndes Trainings zu reduzieren, ist die Dropout -Technik. Hierbei werden per Zufall eine vor-\\nher deﬁnierte Menge an Neuronen in einer Schicht deaktiviert, d.h. ausgeschaltet, sodass\\ndiese Einheiten während einer Trainingsepoche ausfallen (engl. drop out ). Im nächsten\\nBerechnungsschritt können dann per Zufall auch andere Einheiten ausgewählt werden,\\ndie dann kurzfristig ausgeschaltet bleiben. Diese Technik kann auch als temporäre Aus-\\ndünnungstechnik interpretiert werden.\\nEine andere Möglichkeit der Regularisierung setzt auf der zu minimierenden Fehlerfunk-\\ntion des Netzwerkes an und wird damit direkter Bestandteil des Lernverfahrens. Hierzu\\n222.7 Optimierungstechniken\\nwird ein Strafterm bzw. Komplexitätsterm eingeführt. Es gilt:\\nEtot=P\\nå\\np=1Ep+lC(w1,\\x01\\x01\\x01,wn), (2.14)\\nwobei Cder noch unspeziﬁzierte, von den Gewichtungen wiabhängige Komplexitätsterm\\nundl>0ein Parameter ist, der den Ausgleich zwischen der Anpassung an die Trainings-\\ndaten und dem dabei beobachteten Zuwachs an Komplexität beschreibt. Die älteste Form\\nder Modellierung, bekannt unter dem Namen Standard-Gewichtszerfall-Verfahren (engl.\\nWeight Decay ), bestraft größere Gewichte. Der Strafterm wächst quadratisch bei einer trai-\\nningsbedingten linearen Gewichtsvergrößerung:\\nC(w1,\\x01\\x01\\x01,wn) =n\\nå\\ni=1w2\\ni. (2.15)\\nDie Verwendung solcher quadratischen Strafterme wird auch als L2-Regularisierung be-\\nzeichnet. In der Regressionsanalyse ist diese Regularisierungstechnik auch als Ridge Re-\\ngression bekannt.\\nDemgegenüber bestraft der Weigend -Strafterm zwar ebenso größere Gewichte, die Bestra-\\nfung wächst jedoch nicht (quadratisch) unendlich bei einer trainingsbedingten linearen\\nGewichtsvergrößerung, sondern besitzt eine natürliche Obergrenze [WRH91]:\\nC(w1,\\x01\\x01\\x01,wn) =n\\nå\\ni=10\\nB@\\x10\\nwi\\nw0\\x112\\n1+\\x10\\nwi\\nw0\\x1121\\nCA . (2.16)\\nIm Fall eines linearen Verhaltens des Strafterms spricht man von L1-Regularisierung. In\\nder Regressionsanalyse wird auch die Regularisierungstechnik Lasso fürLeast Absolute\\nShrinkage and Selection Operator verwendet. Im Unterschied zu Gl. 2.16 wird jedoch der\\nAbsolutbetrag der Gewichte verwendet:\\nC(w1,\\x01\\x01\\x01,wn) =n\\nå\\ni=1jwij. (2.17)\\n233 Multilayer Perceptron\\nDas Mehrschichten-Perzeptron (engl. Multilayer Perceptron ( MLP )) ist eines der bekann-\\ntesten und vielfältig einsetzbaren KNN, insbesondere weil es einen einfachen Aufbau hat\\n(vgl. Abschnitt 3.1) und mit dem Backpropagation -Algorithmus (vgl. Abschnitt 3.2) eine\\nLernregel zur Verfügung gestellt wird, die häuﬁg zu sehr guten Ergebnissen führt. Zu Back-\\npropagation gibt es mittlerweile einige Erweiterungen (vgl. Abschnitt 3.3). Eine besondere\\nRolle beim Training spielt die Aktivierungsfunktion der Neuronen, für die verschiedene\\nAlternativen verwendet werden können (vgl. Abschnitt 3.4). Beim überwachten Lernen\\nmuss außerdem eine Kostenfunktion benutzt werden, um die Fehler zu berechnen und zu\\nbewerten (vgl. Abschnitt 3.5). Wenn ein MLP aus sehr vielen verborgenen Schichten be-\\nsteht, d.h. das Netzwerk sehr tief ist, dann stößt man auf Schwierigkeiten im Training, die\\nmit Hilfe von speziellen Techniken gelöst werden können, die unter dem Begriff Deep Lear-\\nning ( DL)zusammenfasst werden (vgl. Abschnitt 3.6). Aufgrund der Vielseitigkeit und\\nAbbildungsfähigkeit von MLPs werden diese Netzwerke in zahlreichen Anwendungen in\\nden Bereichen Mustererkennung, Funktionenapproximation, Klassiﬁzierung, Prognose,\\nDiagnose, Steuerung und Optimierung eingesetzt.\\n3.1 Aufbau\\nDas mehrschichtige vorwärtsgekoppelte Netzwerk (engl. Multilayer Feedforward Network\\n(MLFF )) mit Backpropagation -Lernen ( BP) wird auch als Mehrschichten-Perzeptron bezeich-\\nnet, weil es Perzeptron-Netzwerken mit mehreren Schichten sehr ähnlich ist. Es handelt\\nsich um ein vollverknüpftes Netzwerk, das aus einer Eingabeschicht, einer oder mehrerer\\ninnerer Schichten und einer Ausgabeschicht besteht (siehe Abb. 3.1).\\nAbbildung 3.1: Schematische Darstellung eines MLP mit einer inneren Schicht. Aus Darstellungs-\\ngründen sind nicht alle Gewichtungen eingezeichnet.\\nDienNeuronen der Eingabeschicht nehmen den Input x1bisxnentgegen. Über die Ge-\\nwichte v11bisvnhwerden die Eingaben an die hNeuronen der verborgenen Schicht (engl.\\nHidden Layer ) weitergeleitet. Diese Neuronen benutzen eine Aktivierungsfunktion fund\\n253 Multilayer Perceptron\\nbesitzen einen veränderbaren Schwellenwert (engl. Bias). Dadurch wird die Aktivierung\\nals Ausgaben y1bisyhbestimmt. Diese Werte werden über die Gewichte w11biswhm\\nan die mNeuronen der Ausgabeschicht weitergeleitet. Auch diese mNeuronen besitzen\\neine Aktivierungsfunktion fund ein Bias. Die Aktivierungsfunktion dieser Ausgabeneu-\\nronen kann sich von denen der verborgenen Neuronen unterscheiden. Die berechneten\\nAktivierungen werden schließlich als Ergebnis bzw. Output z1biszmzurückgeliefert. Die-\\nser Prozess wird auch als Feedforward bezeichnet. Er wiederholt sich für jeden Datensatz.\\nBeim Training bzw. Lernvorgang werden die Gewichte der Verbindungen und die Schwel-\\nlenwerte der Neuronen angepasst. Hierzu werden die Fehler, die das Netzwerk gemacht\\nhat, zurück durch das Netzwerk propagiert, also von der Ausgabeschicht in Richtung\\nEingabeschicht. Deshalb heißt dieser Lernalgorithmus auch Backpropagation .\\n3.2 Backpropagation\\nDie Bestimmung der Parameter (Gewichte, Schwellenwerte) im MLP erfolgt mit der Back-\\npropagation -Lernmethode [RHW86a; RHW86b][Par85][LeC86]. Es handelt sich um eine\\nOptimierungsprozedur zur Minimierung des Netzfehlers, die auf dem steilsten Gradi-\\nentenabstieg basiert (vgl. Kap. 2.4). Die berechneten Fehler werden als Eingaben für\\nFeedback -Verknüpfungen verwendet, welche dann rückwärts Schicht für Schicht, also ite-\\nrativ, die Gewichtungen anpassen. Betrachtet man ein Netz mit nEingabeneuronen xi,h\\nNeuronen der verborgenen Schicht yjund mNeuronen der Ausgabeschicht zk. Die Ge-\\nwichte zwischen Eingabe- und verborgener Schicht sind vij, die zwischen verborgener\\nund Ausgabeschicht wjk. Folglich gilt für die Ausgabesignale\\nzk=f \\nh\\nå\\nj=1wjkyj!\\n=f \\nh\\nå\\nj=1wjkf \\nn\\nå\\ni=1vijxi!!\\n. (3.1)\\nDie zu minimierende Energie- bzw. Kostenfunktion lautet\\nEtot=1\\nPP\\nå\\np=1Epmit Ep=1\\n2m\\nå\\nk=1(tp\\nk\\x00zp\\nk)2. (3.2)\\nDie Gewichtsänderungen sind proportional zum Gradienten der Energiefunktion:\\nDwjk=\\x00h¶Etot\\n¶wjkund Dvij=\\x00h¶Etot\\n¶vij. (3.3)\\nMit den Deﬁnitionen\\nHp\\nj=n\\nå\\ni=1vijxp\\ni,Ip\\nk=h\\nå\\nj=1wjkyp\\njund (3.4)\\ndp\\nk= (tp\\nk\\x00zp\\nk)f0(Ip\\nk),dp\\nj=f0(Hp\\nj)m\\nå\\nk=1dp\\nkwjk (3.5)\\nfolgt aus Gl. 3.3\\nDwjk=h\\nPP\\nå\\np=1m\\nå\\nk=1dp\\nkyp\\njund Dvij=h\\nPP\\nå\\np=1h\\nå\\nj=1dp\\njxp\\nj. (3.6)\\nDer Backpropagation -Lernalgorithmus heißt auch generalisierte Delta-Regel.\\n263.3 Erweiterungen\\n3.3 Erweiterungen\\nZuBackpropagation existieren sehr viele Erweiterungen [Rud16]. Im Kern ist es ein Gra-\\ndientenabstiegsverfahren (engl. Gradient Descent Method ), also ein Optimierungsproblem.\\nJe nachdem, wie viele Daten zum Trainieren benutzt werden, unterscheidet man drei\\nverschiedene Varianten.\\nDie Methode Batch Gradient Descent , auch Vanilla Gradient Descent genannt, berechnet\\ndie Gradienten der Energie- bzw. Kostenfunktion für die gesamten Trainingsdaten und\\ndanach wird dann die Aktualisierung (engl. Update ) der Netzwerk-Parameter vorgenom-\\nmen. Man spricht auch von Ofﬂine Learning . Dieses Verfahren ist aber ungeeignet, wenn\\nsehr große Datenmengen verarbeitet werden müssen. Ggf. passt die Menge an Daten gar\\nnicht in den Speicherbereich. Außerdem kann das Trainings sehr langsam sein.\\nDie Methode Stochastic Gradient Descent (SGD) dagegen aktualisiert die Parameter in\\njedem Trainingsschritt nach der Berechnung der Gradienten der Fehlerfunktion bzw. Loss-\\nFunktion. Als Abgrenzung zur ersten Variante wird dieses Verfahren als Online Learning\\nbezeichnet. Neue Trainingsdaten können relativ leicht hinzugefügt werden. Des Weiteren\\nist das Training meistens viel schneller als bei Batch Gradient Descent . Allerdings kann\\nes aufgrund der vielen Aktualisierungen zu großen Fluktuationen kommen, welche die\\nKonvergenz des Verfahrens erheblich erschweren.\\nEine Lösung hierfür stellt die dritte Alternative dar, die das Beste aus beiden Welten ver-\\nbindet: Die Methode Mini-Batch Gradient Descent teilt die Trainingsdaten in mehrere\\nMengen auf und führt dann für jede dieser Mengen das Batch Gradient Descent durch.\\nDer Nachteil dieser Methode ist, dass ein weiterer Parameter als sogenannter Hyperpara-\\nmeter hinzukommt, nämlich die Größe der Mini-Batch Teilmengen. Je nach Anwendung\\nverwendet man häuﬁg die Größen 32, 64, 128 oder 256, also ein Vielfaches der Zahl 2, in\\nAnlehnung an die Low Level Datenverarbeitung in Prozessoren (CPU bzw. GPU).\\nProbleme mit dem Gradientenabstiegsverfahren können sich u.a. durch ﬂache Plateaus\\nder Fehlerfunktion ergeben (vgl. Kap. 2.4). Mit dem variablen Trägheitsterms (Momentum\\nTerm ) verhindert man, dass das Lernen in einem ﬂachen Bereich des Fehlergebirges nur\\nnoch langsam vorangeht. Man gibt dem Verfahren einen Schwung mit, indem man einen\\nBruchteil der letzten Gewichtsänderung nochmals addiert [RHW86b]:\\nDwp\\nkj=hm\\nå\\nk=1dp\\nkyp\\nj+aDwp\\x001\\nkjund Dvp\\nij=hh\\nå\\nj=1dp\\njxp\\nj+aDvp\\x001\\nij. (3.7)\\n1992 haben Martin Riedmiller und Heinrich Braun das Lernverfahren Resilient Backpro-\\npagation (Rprop) als Erweiterung von Backpropagation vorgestellt [RB92; RB93]. Übersetzt\\nbedeutet der Name des Algorithmus Elastische Fortpﬂanzung und ist vergleichbar mit dem\\nVerhalten einer Springfeder, bei der eine Rückstellkraft wirkt. Im Gegensatz zu Backpro-\\npagation wird für jedes Gewicht eine individuelle Schrittweite bestimmt und die letzte\\nGewichtsänderung wird im nächsten Iterationsschritt miteinbezogen. Des Weiteren wird\\nnur das Vorzeichen des Gradienten verwendet und nicht der Wert des Gradienten selbst.\\nDie Idee hinter dem adaptiven Vorgehen zur Gewichtsanpassung kann man wie folgt\\nbeschreiben: Falls sich das Vorzeichen des Gradienten in einem Gewicht ändert, d.h. man\\nalso ”über das Ziel hinausgeschossen” ist, dann wird die neue Schrittweite reduziert und\\nman springt wieder etwas zurück. Falls dagegen die Vorzeichen beider Gradienten gleich\\nsind, dann erhöht man die Schrittweite, um schneller voranzukommen.\\n273 Multilayer Perceptron\\nIn seiner Vorlesung Neural Networks for Machine Learning an der Universität Toronto hat\\nGeoffrey E. Hinton am 06.02.2014 eine Erweiterung zu Rprop vorgestellt [HSS14]: Root\\nMean Square Propagation (RMSprop) . Rprop funktioniert sehr gut zusammen mit Batch\\nGradient Descent , wenn die Parameteranpassung ofﬂine erfolgt. In Kombination mit Mini-\\nBatch kann es aber Schwierigkeiten geben, wenn nur das Vorzeichen und nicht auch der\\nWert des Gradienten betrachtet wird. Die Parameter (Gewichte) können evtl. viel zu\\ngroße Werte annehmen. Die Idee von RMSprop ist, dass die jeweilige Lernrate noch\\ndurch einen Korrekturterm dividiert wird. Dieser Korrekturterm hängt nun von der Größe\\ndes Gradienten ab. Es wird ein exponentiell geglätteter Gradient verwendet, d.h. die\\nWurzel (engl. Root ) des Mittelwerts (engl. Mean ) der quadrierten Gradienten (engl. Squared\\nGradients ). RMSprop kann als Verallgemeinerung von Rprop betrachtet werden und ist\\nsowohl für Mini-Batch als auch Full-Batch geeignet.\\nDer Optimierer Adam fürAdaptive Moment Estimation ist wiederum eine Erweiterung\\nvon RMSprop [KB14]. Hier werden nicht nur die Gradienten zur Korrektur der Lernrate\\nverwendet, sondern auch die zweiten partiellen Ableitungen, d.h. die Momente. Adam\\nwird inzwischen in vielen Fällen als Standard-Algorithmus für Optimierungsprobleme\\nwie dem Gradientenabstieg eingesetzt [Kar18].\\n3.4 Aktivierungsfunktionen\\nImBackpropagation -Algorithmus werden die ersten Ableitungen der Aktivierungsfunkti-\\non berechnet. Dies setzt voraus, dass diese Funktion stetig differenzierbar ist. Als Aktivie-\\nrungsfunktion wird häuﬁg die sigmoide, S-förmige, monoton steigende und differenzier-\\nbare logistische Funktion\\nf(x) =1\\n1+exp(\\x00b\\x01x)(3.8)\\nbenutzt, wobei beine Konstante ist, die die Steilheit der Kurve bestimmt (siehe Abb. 3.2).\\nDer Wertebereich der Funktion liegt zwischen 0 und 1. Die Ableitung hat die Form\\nf0(x) =b\\x01f(x)(1\\x00f(x)). (3.9)\\nAbbildung 3.2: Logistischen Funktion mit Ableitung für zwei Parameter b\\nEinerseits ist die sigmoide Aktivierungsfunktion also differenzierbar, andererseits lässt\\nsich die erste Ableitung sehr einfach analytisch bestimmen, weil sich diese aus der Original-\\nFunktion berechnen lässt.\\n283.4 Aktivierungsfunktionen\\nEine andere, sehr beliebte Aktivierungsfunktion, ist der Tangens Hyperbolicus , also:\\nf(x) =tanh(x) =2\\n1+exp(\\x002x)\\x001 (3.10)\\nmit der ersten Ableitung\\nf0(x) =1\\x00tanh2(x). (3.11)\\nIm Jahr 2000 haben Richard H. R. Hahnloser und Kollegen eine neuartige Aktivierungs-\\nfunktion eingeführt, welche biologisch inspiriert ist [Hah+00]:\\nf(x) =x+=max(0,x) =8\\n<\\n:0 falls x<0\\nxfalls x\\x150. (3.12)\\nDiese Funktion liefert nur den positiven Teil des Arguments xzurück. In der Elektrotech-\\nnik gibt es Gleichrichter (engl. Rectiﬁer ) zur Umwandlung von Wechsel- in Gleichspan-\\nnung, die eine ganz ähnliche Funktionsweise haben. Deshalb nennt man diese Funktion\\nauch Rectiﬁer . Ein Neuron, welches dieses Aktivierungsfunktion verwendet, wird auch\\nalsRectiﬁed Linear Unit ( ReLU )bezeichnet. Ein Problem ist allerdings, dass diese Funktion\\nfürx=0nicht stetig differenzierbar ist. Aus diesem Grund wird die Funktion meistens\\nangenähert und stattdessen die Gleichung\\nf(x) =log(1+exp(x)) (3.13)\\nbenutzt. Im Bereich Deep Learning werden ReLU -Aktivierungen sehr erfolgreich eingesetzt.\\nAbb. 3.3 zeigt den Verlauf der drei bisher vorgestellten Aktivierungsfunktionen.\\nAbbildung 3.3: Aktivierungsfunktionen im Vergleich [Mou16]\\nEine sehr spezielle Aktivierungsfunktion stellt die Softmax -Funktion dar:\\nf(x)i=exp(xi)\\nåK\\nk=1exp(xk)für i=1, . . . , K. (3.14)\\nDieSoftmax -Funktion kann auch als normierte Exponentialfunktion bezeichnet werden.\\nIn Klassiﬁkationsaufgaben kommt diese Funktion häuﬁg für die Einheiten der Ausgabe-\\nschicht zum Einsatz, weil die Ausgabe dann als die jeweilige Klassenwahrscheinlichkeit\\npiderKKlassen interpretiert werden kann.\\n293 Multilayer Perceptron\\n3.5 Kostenfunktionen\\nImBackpropagation -Algorithmus wird eine Energie- bzw. Kostenfunktion als Zielfunktion\\ndeﬁniert, die es zu minimieren gilt. Hierbei werden zunächst die Differenzen zwischen\\nder Netzausgabe und den tatsächlichen Werten ( Teaching Input ) aller Ausgabeneuronen\\nquadriert und summiert. Die Fehlerfunktion zu einem Trainingsdatensatz nennt man\\nauch Loss-Funktion. Beim Ofﬂine -Lernen werden die einzelnen Fehler zu allen Trainings-\\ndaten dann noch summiert und anschließend normiert, d.h. durch die Anzahl der Trai-\\nningsdatensätze dividiert. Diese Gesamtfehlerfunktion ist sehr ähnlich zum Mean Squared\\nError ( MSE ), allerdings wird beim MSE noch eine Quadratwurzel aus der Summe gezo-\\ngen. In der Regressionsanalyse benutzt man bspw. die Methode der kleinsten Quadrate\\nals Standardverfahren, um die Parameter eines Modells zu schätzen, das den Zusam-\\nmenhang zwischen einer abhängigen Variable zu unabhängigen Variablen möglichst gut\\nfunktional abbildet. Der Vorteil des Quadrierens liegt darin, dass sich positive und ne-\\ngative Abweichungen bzw. Fehler nicht gegenseitig aufheben können. Eine alternative\\nFehlerfunktion ist der Mean Absolute Error ( MAE ), also der mittlere absolute Fehler. Hier\\nwerden die Differenzen nicht quadriert, sondern der Absolutbetrag benutzt. Statt der ab-\\nsoluten Fehler können auch die relativen Fehler betrachtet werden, also die prozentualen\\nFehler. Das entsprechende Fehlermaß heißt Mean Absolute Percentage Error (MAPE) .\\nIn Klassiﬁkationsaufgaben sind die Datensätze der Trainingsmenge bereits festen Klassen\\nzugeteilt. Man spricht auch von nominalskalierten Merkmalen oder von kategorischen\\nVariablen. In diesen Anwendungen kommen noch andere Kosten- bzw. Loss-Funktionen\\nzum Einsatz. Als Beispiele können die Funktionen Square Loss ,Hinge Loss ,Logistic Loss\\nund Cross Entropy Loss genannt werden [Wik18g]. Letztere Funktion ist besonders inter-\\nessant, weil sie stetig differenzierbar ist und somit in Gradientenabstiegsverfahren benutzt\\nwerden kann. Die Cross Entropy Loss Funktion ist die erste Wahl in tiefen neuronalen Netz-\\nwerken, die zur Klassiﬁkation eingesetzt werden.\\n3.6 Deep Learning\\nWenn KNN sehr tief werden, d.h. aus sehr vielen verborgenen Schichten bestehen, dann\\nkommen spezielle Techniken für das Training zum Einsatz, die man unter dem Begriff\\nDeep Learning ( DL)zusammenfassen kann. Zunächst stellt sich aber die Frage: Was ist\\nbesonders am Training tiefer KNN?\\nZur Beantwortung der Frage sehen wir uns ein Beispiel an. Betrachten wir ein MLP mit\\nsehr vielen Schichten und wenden wir den Backpropagation -Algorithmus an, dann werden\\ndie berechneten Fehler Schicht für Schicht von den Ausgabeneuronen über die verborge-\\nnen Neuronen zu den Eingabeneuronen zurückgegeben und dabei die Parameter ange-\\npasst. Normalerweise werden vor dem Training die Parameter des Netzwerks, also die\\nGewichte der Verbindungen und die Schwellenwerte der Neuronen, zufällig initialisiert,\\nwobei hier meistens die Standardnormalverteilung mit Mittelwert 0 und Standardabwei-\\nchung 1 verwendet wird (siehe Abb. 3.4). Im Mittel sind die Werte der Parameter also\\nnull und 68,2 % der Parameterwerte liegen im Intervall [\\x001,+1]. Die Ausgaben der KNN\\nsind aufgrund der verwendeten Aktivierungsfunktionen meistens auf das Intervall [0, 1]\\n(sigmoide Funktion) oder auf das Intervall [\\x001,+1](Tangens Hyperbolicus ) normiert. Die\\nAktualisierung der Parameter ist proportional zum Gradienten der Fehlerfunktion. Die\\nAbleitungen dieser Aktivierungsfunktionen liegen aber ebenfalls im Intervall [0, 1]. Wenn\\nzwei Faktoren, die im Intervall [0, 1]liegen, miteinander multipliziert werden, dann ist\\n303.6 Deep Learning\\ndas Produkt kleiner als der kleinere der beiden Faktoren. Das Produkt nähert sich also\\nder Zahl Null an, je mehr solcher Multiplikationen durchgeführt werden. Genau solche\\nOperationen werden als Matrix-Multiplikationen beim Backpropagation bzw. Gradienten-\\nabstiegsverfahren zur Bestimmung der Parameteränderungen durchgeführt.\\nAbbildung 3.4: Standardnormalverteilung mit Intervallen [Toe07]\\nBesteht das KNN nun aus sehr vielen verborgenen Schichten, dann werden auch viele\\nsolcher Multiplikationen durchgeführt. Die berechneten Gradienten des Gradientenab-\\nstiegsverfahrens werden somit immer kleiner und verschwinden schließlich. Das Training\\ndes Netzwerks kommt damit praktisch zum Stillstand. Dieses Phänomen wurde bereits\\n1991 von Josef Hochreiter im Rahmen seiner Diplomarbeit [Hoc91] zum ersten Mal er-\\nwähnt. 2010 haben Xavier Glorot und Yoshua Bengio es dann genauer untersucht [GB10].\\nUm das Problem der verschwindenden Gradienten zu behandeln, werden verschiedene\\nLösungsmöglichkeiten vorgeschlagen und eingesetzt:\\n1 Initialisierung Xavier Initialization [GB10]\\n2 Aktivierungsfunktion Rectiﬁed Linear Unit (ReLU) Kap. 3.4\\n3 Normierung Batch Normalization (BN) [IS15]\\n4 Training Gradient Clipping [PMB12]\\n5 Wiederverwendung Pre-Trained Layers [Gé17]\\n6 Spezielle Netzwerke Deep Belief Network (DBN) [HOT16]\\nConvolutional Neural Network (CNN) Kap. 4\\nLong Short-Term Memory (LSTM) Kap. 5.4\\nEinige dieser Lösungsvorschläge werden im Rahmen dieser Arbeit behandelt, somit sind\\nVerweise zu den entsprechenden Kapiteln genannt. Zu den anderen Lösungsvorschlägen\\nsind Quellen angegeben, mit deren Hilfe sich die Lösung erschließen lässt.\\n314 Convolutional Neural Network\\nDas Convolutional Neural Network ( CNN )hat seinen Namen aufgrund der mathematischen\\nOperation Faltung (engl. Convolution ) bekommen. Die Faltung oder Konvolution zweier\\nFunktionen fund gwird durch die Gleichung\\n(f\\x03g)(x) =Z\\nRnf(t)g(x\\x00t)dt (4.1)\\nbeschrieben. Die Faltung kann bspw. benutzt werden, um eine gegebene Funktion zu\\nglätten. Hierzu wird diese mit Hilfe eines speziellen Glättungskerns (engl. Kernels ) gefaltet.\\nDer Kernel wirkt dabei auch wie ein Filter. Bezogen auf die Anwendung der digitalen\\nBildverarbeitung kann ein solcher Kernel bzw. ein solches Filter verwendet werden, um\\nbspw. einen Bereich eines Bildes abzutasten, um dadurch Kanten oder Merkmale (engl.\\nFeatures ) auf dem Bildausschnitt zu entdecken. Nimmt man bspw. ein relativ kleines\\nBild mit einer Größe von 100 x 100 Pixeln, so besteht dieses bereits aus 10.000 einzelnen\\nBildpunkten. Jeder Pixel wird dann noch über einen Wert beschrieben, der bspw. die\\nFarbe oder Intensität (Graustufe) angibt. Mit Hilfe des klassischen Multilayer Perceptron\\n(vgl. Kap. 3) ist es schwierig, solche Bilder zu verarbeiten, um bspw. Objekte darauf\\nzu erkennen und zu klassiﬁzieren. Denn ein solches Netzwerk müsste ja bereits über\\n10.000 Eingabe-Neuronen verfügen. Nun kommt die Idee der Faltung ins Spiel. Das CNN\\nfunktioniert im Prinzip wie ein MLP . Allerdings wird jeweils nur ein Bildausschnitt mit\\nHilfe eines speziellen Filters bzw. Kernel abgetastet und das Ergebnis auf einer Feature Map\\ngespeichert. Die Eingabeschicht dieses KNN besteht dann aus einem Stapel von Feature\\nMaps . Deshalb können CNN auch sehr gut im Bereich der Bildverarbeitung, insbesondere\\nin der Objekterkennung und Objektklassiﬁzierung eingesetzt werden. In den folgenden\\nAbschnitten wird diese grundlegende Idee, der Aufbau und die Funktionsweise weiter\\nausgeführt und vertieft, sowie einige prominente CNN als Beispiele vorgestellt.\\n4.1 Visueller Cortex\\n1981 wurde der Nobelpreis für Physiologie und Medizin u.a. dem gebürtigen Kanadier\\nDavid H. Hubel und dem schwedischen Wissenschaftler Torsten N. Wiesel für ihre Ent-\\ndeckungen über die Informationsbearbeitung im Sehwahrnehmungssystem überreicht.\\nAb 1958 beschäftigten sich die beiden Wissenschaftler mit dem Aufbau und der Informa-\\ntionsverarbeitung des visuellen Cortex und führten hierzu Untersuchungen mit Katzen\\nund Affen durch. Dabei bekamen sie eine erste Ahnung davon, wie das Gehirn die sen-\\nsorischen Informationen analysiert. Ausgehend von den Rezeptorenzellen der Netzhaut\\n(Retina) des Auges, den ca. 6 Millionen Zapfen für Farbinformationen und den ca. 120\\nMillionen Stäbchen für Hell-Dunkel und Bewegungen, werden die auftreffenden Photo-\\nnen des Lichts in elektrische Signale umgewandelt und über den Sehnerv und den seit-\\nlichen Kniehöcker des Thalamus ( Corpus Geniculatum Laterale ( CGL )) zur Sehrinde, dem\\nprimären visuellen Cortex (V1), geleitet, wo dann die eigentliche Verarbeitung beginnt\\n(siehe Abb. 4.1).\\n334 Convolutional Neural Network\\nAbbildung 4.1: Signalweg von der Netzhaut zur Sehrinde [Mad16]\\nHubel studierte zunächst Physik und Mathematik an der McGill Universität in Montréal,\\nbevor er sich dann schließlich für ein Medizin-Studium entschied. Seine interdisziplinären\\nKenntnisse nutze Hubel, um eine Mikroelektrode zu entwickeln, mit der die Aktivität ein-\\nzelner Zellen der primären visuellen Cortex von Katzen erforscht werden konnten. 1958\\ntrafen sich dann Hubel und Wiesel im Labor von Stephen W. Kufﬂer an der Johns Hopkins\\nUniversität, bevor sie dann ein Jahr später zur Harvard Medical School wechselten. Mit\\nihren Untersuchungen haben Hubel und Wiesel schließlich eine komplexe, mehrstuﬁge\\nArchitektur des primären visuellen Cortex V1 identiﬁziert [Ley12].\\nDie Nervenzellen im primären visuellen Cortex sind in sechs Schichten zu Hypersäulen\\nangeordnet. Eine Hypersäule repräsentiert einen Ort auf der Netzhaut, also nur einen klei-\\nnen Bereich im Gesichtsfeld, nimmt eine Fläche von 1 mm x 1 mm ein, enthält zwischen\\n50.000 und 100.000 Nervenzellen und stellt das kleinste vollständige Analysemodul der\\nSehrinde dar [Wis18]. Die Zellen in diesen Säulen reagieren nur auf bestimmte Reize:\\n1 Orientierungssäulen Gleiche Reizrichtung\\n2 Positionssäulen Gleicher Ort auf der Netzhaut\\n3 Augendominanzsäulen Rechtes oder linkes Auge\\n4 Merkmalsdetektoren Lichtstreifen einer bestimmten Ausrichtung\\n5 Komplexe Zellen Lichtstreifen, die sich in einer best. Richtung bewegen\\n6 Hyperkomplexe Zellen Streifen bestimmter Länge oder Ecken und Winkel\\nEs gibt also spezialisierte Zellen in V1, die für speziﬁsche Aufgaben des Sehens benötigt\\nwerden. Diese Spezialisierung setzt sich auch in der sekundären und tertiären visuellen\\nRinde (V2 bis V5) fort, wobei hier bspw. die Wahrnehmungsaspekte Farben und Stereose-\\nhen relevant sind (siehe Abb. 4.2).\\n344.2 Aufbau und Schichten\\nAbbildung 4.2: Funktionale Spezialisierung des visuellen Cortex [LH88]\\nDiese grundlegende Erkenntnis von der Funktionsweise des visuellen Cortex wurde als\\nInspiration bei der Entwicklung der Convolutional Neural Network (CNN) verwendet.\\n4.2 Aufbau und Schichten\\nIm letzten Abschnitt wurde festgestellt, dass im visuellen Cortex verschiedene Zellen auf\\nbestimmte Funktionen bei der Signalverarbeitung spezialisiert sind und nacheinander\\nStufen der Verarbeitung durchlaufen werden. Beim Feature Learning durch Deep Learning\\nwerden stufenweise bestimmte Features erlernt (siehe Abb. 4.3).\\nAbbildung 4.3: Feature Learning durch Deep Learning – in Anlehnung an [Lee+11]\\n354 Convolutional Neural Network\\nBetrachtet man bspw. die Gesichtserkennung als Anwendung der digitalen Bildverar-\\nbeitung, dann werden in der ersten Stufe zunächst Kanten als Features erkannt. In der\\nnächsten Stufe können dann schon größere Bereiche als Features erkannt werden: Augen,\\nNase, Mund usw. In der letzten Stufe wird dann das komplette Gesicht als Feature gelernt.\\nWie das Multilayer Perceptron ist auch das Convolutional Neural Network ein mehrschichtiges\\nvorwärtsgekoppeltes Netzwerk. Dabei wiederholt sich die Kombination aus Convolution\\nLayer und Pooling Layer mehrfach. Mit Hilfe dieser Kombination können Features erlernt\\nwerden. Abb. 4.4 zeigt den schematischen Aufbau eines CNN. Mit Subsampling ist das\\nPooling gemeint. Der Convolutional Layer besitzt mehrere Filter-Kernel. Jeder Filter-Kernel\\nenthält die zu trainierenden Gewichte und zu jedem dieser Filter-Kernel wird eine Feature\\nMap generiert. Es entsteht also ein Stapel von Feature Maps , die zwar alle den gleichen In-\\nput bekommen, aber aufgrund der verschiedenen Kernels auch unterschiedliche Features\\nextrahieren.\\nAbbildung 4.4: Aufbau eines typischen CNN [Aph15b]\\nConvolutional Layer Diese Schicht ist der wesentliche Baustein eines CNN. Die Neuro-\\nnen bzw. Einheiten in dieser Schicht sind entsprechend der jeweiligen Anwendung ange-\\nordnet. Im Bereich Bilderkennung werden normalerweise zweidimensionale digitale Bil-\\nder verarbeitet. Somit sind die Einheiten in diesem Anwendungsfall als 2D-Convolutional\\nLayer angeordnet, nur ist dieser wesentlich kleiner als das gesamte Bild. Die Aktivität\\njeder Einheit wird durch eine diskrete Faltung (engl. Convolution ) berechnet. Dabei wird\\ndie Faltungsmatrix, auch Filter-Kernel genannt, schrittweise über das Inputbild bewegt.\\nEin solcher Schritt (engl. Stride ) kann ein oder mehrere Pixel betragen. Das innere Pro-\\ndukt aus der Faltungsmatrix und dem aktuellen Bildausschnitt ergibt dann gerade die\\nInputaktivität. Wenn die Faltungsmatrix über den Bildrand verschoben wird, kommt\\neine spezielle Technik zum Einsatz, das sogenannte Padding . Hierbei muss deﬁniert wer-\\nden, wie mit den eigentlich nicht-existierenden Bildpunkten verfahren werden soll. Beim\\nZero-Padding werden bspw. einfach Nullwerte hinzugefügt. Eine Besonderheit des Con-\\nvolutional Layers ist, dass die Gewichte des Filter-Kernels für alle Neuronen einer Feature\\nMap gleich sind, d.h. diese Parameter werden geteilt, man spricht auch von sogenannten\\nShared Weights . Deshalb ist der Einsatz von Convolutional Layers besonders efﬁzient. Jede\\nEinheit in diesem Convolutional Layer empfängt also nur Input-Signale von einer lokalen\\nBildumgebung, teilt sich aber die Parameter (Gewichte) mit allen anderen Einheiten. Dies\\nführt dazu, dass Kanten bzw. Features in den Bildern besonders gut erkannt werden. Als\\nAktivierungsfunktion wird zu den Einheiten im Convolutional Layer meistens die Rectiﬁed\\nLinear Unit (ReLU) verwendet.\\n364.2 Aufbau und Schichten\\nPooling Layer Nach jedem Convolutional Layer folgt normalerweise ein Pooling Layer .\\nDieser wird benötigt, um überﬂüssige Informationen zu entfernen. Bspw. wird bei der Ob-\\njekterkennung nicht die exakte Position einer Kante bzw. eines Features benötigt, sondern\\neine ungefähre Lokalisierung ist ausreichend. Häuﬁg werden entweder das Max Pooling\\noder das Avg Pooling eingesetzt. Beim Max Pooling wird nur die Aktivität des aktivsten\\nNeurons bezüglich eines Kernels (z.B. der Größe 2 x 2) geﬁltert. Beim Avg Pooling wird\\ndagegen der Mittelwert der Aktivitäten bestimmt und diese mittlere Aktivität weiter ver-\\nwendet. Mit Hilfe des Poolings , auch Down Sampling oder Subsampling genannt, lässt sich\\nalso die Dimension und damit die Komplexität reduzieren. Dadurch wird Überanpassung\\n(engl. Overﬁtting ) vorgebeugt, der Speicherbedarf sinkt und die Berechnungsgeschwindig-\\nkeit steigt. Pooling ist somit die Voraussetzung, wenn sehr tiefe CNN konstruiert werden\\nsollen, um komplexe Probleme zu lösen. Abb. 4.5 zeigt ein Beispiel für das Max Pooling ,\\nbei dem ein Filter der Größe 2 x 2 mittels einer Schrittweite ( Stride ) von 2 auf eine Input-\\nMatrix der Größe 4 x 4 angewendet wird.\\nAbbildung 4.5: Max Pooling mit 2x2-Filter und Stride=2 [Aph15a]\\nFully-Connected Layer Das CNN wird häuﬁg mit einer oder mehrerer vollvernetzter\\nSchichten abgeschlossen, analog zum Aufbau eines Multilayer Perceptron . Für eine typi-\\nsche Klassiﬁkationsaufgabe im Data Mining gibt es in der letzten Schicht genau so viele\\nNeuronen, wie es Klassen gibt. Es wird also die sogenannte One Hot -Codierung bzw. Dum-\\nmy-Variablen verwendet, durch die jede Klasse als eine eigene Variable mit den Werten\\n0 und 1 für false und true abgebildet wird. Dabei geht man von nominalskalierten Merk-\\nmalen aus, d.h. man muss keine impliziten Annahmen über Ähnlichkeiten von Klassen\\nmachen. Als Aktivierungsfunktion wird meistens Softmax verwendet, da die Summe über\\nalle Elemente genau eins ergibt und diese Funktion dann wie eine Normierung wirkt.\\nDropout Layer Neben dem Pooling Layer kann auch ein Dropout Layer verwendet wer-\\nden, um die Komplexität des Netzwerks zu verringern und um das Übertrainieren bzw.\\ndie Überanpassung zu reduzieren. In dieser Schicht werden die Aktivitäten von einigen\\nNeuronen zufällig auf null gesetzt. Diese Einheiten fallen dann in diesem Trainingsschritt\\naus (engl. drop out ). Über eine einstellbare Ausfallrate in Prozent kann dieses Verhalten\\ngesteuert werden.\\nLoss Layer Normalerweise wird der Backpropagation -Algorithmus bzw. ein Gradienten-\\nabstiegsverfahren verwendet, um die Parameter des CNN mittels überwachten Lernens\\nanzupassen. Der Loss Layer deﬁniert dabei, wie das Training mit der Abweichung zwi-\\nschen den vorhergesagten und tatsächlichen Output-Werten umgehen soll. Somit ist der\\n374 Convolutional Neural Network\\nLoss Layer als letzte Schicht des CNN zu implementieren. Je nach Aufgabe kann als Loss-\\nFunktion bspw. eine der drei Funktionen verwendet werden:\\n1Sigmoide Cross-Entropy binäre Klassiﬁkation Werte 0 oder 1\\n2Softmax Klassiﬁkation Werte 2[0, 1]\\n3Euklid Loss Regression Werte 2[\\x00¥,¥]\\n4.3 LeNet-5\\nDie LeNet-5-Architektur ist eine der ersten und bekanntesten Topologien eines CNN,\\nentwickelt von Yann LeCun, um handgeschriebene Ziffern zu erkennen [LeC+98]. Abb.\\n4.6 zeigt den schematischen Aufbau zu diesem CNN.\\nAbbildung 4.6: Aufbau des CNN LeNet-5 [LeC+98]\\nIn Tab. 4.1 sind die verwendeten Einstellungen der LeNet-5-Architektur im Detail ange-\\ngeben. Die ersten 5 Schichten des CNN dienen dem Feature Learning , während die letzten\\nbeiden Schichten zur Klassiﬁkation genutzt werden.\\nSchicht Typ Maps Größe Kernel Stride Aktiv.\\nIn Input 1 32 x 32 - - -\\nC1 Convolution 6 28 x 28 5 x 5 1 tanh\\nS2 Avg Pooling 6 14 x 14 2 x 2 2 tanh\\nC3 Convolution 16 10 x 10 5 x 5 1 tanh\\nS4 Avg Pooling 16 5 x 5 2 x 2 2 tanh\\nC5 Convolution 120 1 x 1 5 x 5 1 tanh\\nF6 Fully Connected - 84 - - tanh\\nOut Fully Connected - 10 - - RBF\\nTabelle 4.1: Topologie der LeNet-5-Architektur\\nEigentlich bestehen die zu verarbeitenden Bildern aus 28 x 28 Pixeln. Diese werden aber\\nzu etwas größeren Bildern (32 x 32 Pixeln) derart vorverarbeitet, dass an den Rändern\\nweiße Bildpunkte ergänzt werden ( Zero Padding ). Außerdem werden die Graustufenwerte\\n(0 - 255) noch normalisiert, d.h. auf das Intervall [0, 1] gebracht, indem die Werte durch\\ndie Zahl 255 dividiert werden. Die Ausgabeschicht besteht aus 10 Neuronen, weil es 10\\nObjektklassen (Ziffern Null bis Neun) gibt, die in den Bildern erkannt werden sollen.\\n384.4 AlexNet\\n4.4 AlexNet\\nDie AlexNet-Architektur ist benannt nach Alex Krizhevsky, der zusammen mit Ilya Suts-\\nkever und Geoffrey E. Hinton von der Universität Toronto dieses tiefe CNN entwickelt\\nhat [KSH12]. Es hat 2012 den Wettbewerb ImageNet Large Scale Visual Recognition Challen-\\nge (ILSVRC )mit einer Fehlerrate von 15,3 % sehr eindrucksvoll gewonnen und zwar mit\\neinem Vorsprung von über 10 Prozentpunkten zum Zweitplatzierten (Fehlerrate 26,2 %)\\n[Rus+15]. Die Aufgabe bestand darin, Objekte auf Bildern zu erkennen, die zu jeweils\\neiner von 1000 verschiedenen Klassen gehören. Als Trainingsdaten standen 1,2 Millionen\\nBilder zur Verfügung, zum Validieren der Modellgüte der Klassiﬁzierer wurden 50.000\\nBilder aus einer Menge von 150.000 zufällig ausgewählt, die von der Plattform Flickr\\nund anderen Webseiten stammen, und nicht bereits in den Trainingsdaten enthalten sind\\n[Ima14]. Das Sieger-Netzwerk besteht aus 5 Convolutional Layers und 3 Fully-connected\\nLayers (siehe Abb. 4.7).\\nAbbildung 4.7: Aufbau des AlexNet [KSH12]\\nIn Tab. 4.2 sind die verwendeten Einstellungen der AlexNet-Architektur detailliert ange-\\ngeben. Valid Padding bedeutet, dass kein Padding angewendet wird. Beim Same-Padding\\nhaben Input und Output Feature Maps die gleiche Dimension.\\nSchicht Typ Maps Größe Kernel Stride Padding Aktiv.\\nIn Input 3 224 x 224 - - - -\\nC1 Convolution 96 55 x 55 11 x 11 4 Same ReLU\\nS2 Max Pooling 96 27 x 27 3 x 3 2 Valid -\\nC3 Convolution 256 27 x 27 5 x 5 1 Same ReLU\\nS4 Max Pooling 256 13 x 13 3 x 3 2 Valid -\\nC5 Convolution 384 13 x 13 3 x 3 1 Same ReLU\\nC6 Convolution 384 13 x 13 3 x 3 1 Same ReLU\\nC7 Convolution 256 13 x 13 3 x 3 1 Same ReLU\\nF8 Fully Connected - 4.096 - - - ReLU\\nF9 Fully Connected - 4.096 - - - ReLU\\nOut Fully Connected - 1.000 - - - Softmax\\nTabelle 4.2: Topologie der AlexNet-Architektur\\n394 Convolutional Neural Network\\nUm das Übertrainieren bzw. die Überanpassung des Netzwerks zu reduzieren, wurden\\nRegularisierungstechniken verwendet. Die Schichten F8 und F9 des Netzwerks wurde\\nmittels Dropout mit einer Rate von 50 % während des Trainings temporär ausgedünnt.\\nAußerdem kam Data Augmentation zum Einsatz, d.h. die Trainingsbilder wurden zufällig\\nPixelweise verschoben, gedreht, gespiegelt usw. Eine Besonderheit des AlexNet ist auch\\ndieLocal Response Normalisation ( LRN ), eine spezielle Art der Normierung, welche auf die\\nReLUs der Schichten C1 und C3 angewendet wird [KSH12].\\n4.5 GoogleLeNet\\nZwei Jahre später nach dem Erfolg des AlexNet beim Wettbewerb ILSVRC (vgl. Kap. 4.4),\\nist es Christian Szegedy und seinen Kollegen von Google Inc. gelungen, die Fehlerrate\\nsogar auf unter 7 % zu reduzieren [Sze+14]. Hierzu haben sie das GoogleLeNet entwickelt,\\nwelches viel tiefer als das AlexNet ist. Mit Hilfe von speziellen Einheiten, den sogenannten\\nInception Modulen, konnten Parameter viel efﬁzienter genutzt werden, als dies in früheren\\nArchitekturen möglich war. Abb. 4.8 zeigt den Aufbau eines solchen Inception Moduls mit\\nEinheiten zur Dimensionsreduktion ( Pooling ).\\nAbbildung 4.8: Schematischer Aufbau eines Inception Moduls [Sze+14]\\nMit dem Inception Modul lassen sich also vielfache Features in nur einer Stufe erlernen,\\nsowohl generelle Features (5 x 5 Kernel) als auch lokale Features (1 x 1 Kernel). Mit einem\\nspeziellen Filter werden die Ergebnisse am Ende wieder zusammengefügt und an die\\nnächste Schicht weitergereicht. Abb. 4.9 zeigt den graﬁschen Aufbau des kompletten Goo-\\ngleLeNet vom Input (links) zum Output (rechts), in dem neun Inception Module enthalten\\nsind. Aus Darstellungsgründen ist das Netzwerk um 90 Grad gedreht.\\nAbbildung 4.9: Graphischer Aufbau des GoogleLeNet [Sze+14]\\n404.6 Sonstige\\nIn Abb. 4.10 ist der Aufbau von GoogleLeNet tabellarisch dargestellt.\\nAbbildung 4.10: Tabellarischer Aufbau des GoogleLeNet [Sze+14]\\nDas GoogleLeNet hatte trotzt seiner enormen Tiefe mit 22 Schichten nur etwa 6 Millio-\\nnen freie Parameter, wohingegen beim AlexNet ca. 60 Millionen freie Parameter trainiert\\nwerden mussten.\\n4.6 Sonstige\\nNeben dem LeNet-5 (vgl. Kap. 4.3), dem AlexNet (vgl. 4.4) und dem GoogleLeNet (vgl.\\n4.5) gibt es noch zahlreiche andere Architekturen von CNN. Es lässt sich bereits feststellen,\\ndass die Netzwerke von Jahr zu Jahr tiefer und komplexer werden. Ein anderer Trend ist,\\ndass ein Ensemble bzw. Komitee aus Netzwerken verwendet wird, um das Gesamtergeb-\\nnis per demokratische Abstimmung aus den einzelnen Resultaten zu bestimmen.\\nBeispielsweise gewann das ResNet ein Jahr nach dem GoogleLeNet den Wettbewerb\\nILSVRC in der Kategorie Objektklassiﬁkation. Kaiming He, Xiangyu Zhang, Shaoqing\\nRen und Jian Sun trainierten 2015 dieses sehr tiefe CNN mit bis zu 152 Schichten und\\nerzielten mit einem Ensemble von diesen Netzen eine Fehlerrate von nur 3,57 % [He+15].\\nResNet ist die Abkürzung von Residual Network . Es trägt diesen Namen, weil von den\\neigentlichen Output-Werten die Input-Werte abgezogen werden, sodass dieses Netzwerk\\neigentlich die Residuen lernt. Umgesetzt wird diese Idee durch sogenannte Skip Connecti-\\nons, die auch als Shortcut Connections bezeichnet werden (siehe Abb. 4.11). Zu Beginn des\\nTrainings sind die Parameter (Gewichte und Bias) meistens in der Nähe von null und der\\nOutput des Netzwerks ist dann ebenfalls nahe null. Verwendet man nun Skip Connections ,\\ndann ist der Output aber ähnlich zum Input, d.h. die Zielfunktion ist näherungsweise die\\nIdentitätsfunktion. Durch diesen Trick läuft das Training sehr viel schneller an, was bei\\nsolchen tiefen Netzwerken sehr wichtig ist.\\n414 Convolutional Neural Network\\nAbbildung 4.11: Funktionsweise einer Skip Connection in einer Residual Unit [He+15]\\nDas chinesische Siegerteam Trimps-Soushen des Wettbewerbs ILSVRC von 2016 in der\\nKategorie Objektklassiﬁkation hat eigentlich keine neuen Netzwerke konstruiert, sondern\\nim Wesentlichen die bestehenden verfeinert und ein Ensemble über das Ergebnis abstim-\\nmen lassen, sodass die Fehlerrate auf 2,99 % reduziert werden konnte [Ima16]. Ein Jahr\\nspäter wurde die Fehlerrate vom WMW-Team auf 2,25 % verbessert [Ima17]. Hierzu wur-\\nden spezielle Bausteine namens Squeeze-and-Excitation ( SE)entwickelt und anstelle der\\nInception - bzw. Residuen-Module in die Netzwerke integriert (siehe Abb. 4.12).\\nAbbildung 4.12: SE-Module ersetzen Inception- und Residuen-Module [HSS17]\\nDer Aufwand war jedoch enorm: Es wurden 64 Nvidia Pascal Titan X Graﬁkprozesso-\\nren für ein paralleles und synchrones 20-stündiges Training verwendet. Ab dem Jahr\\n2018 wird dieser Wettbewerb dann von der Data Science Plattform Kaggle durchgeführt\\n[Kag18d]. Populäre und erfolgreiche CNN sowie andere KNN werden mittlerweile in\\nsogenannten Model Zoos gesammelt. Je nach Softwarelösung (vgl. Kap. 6) gibt es verschie-\\ndene Implementierungen zu diesen Netzwerken.\\n425 Recurrent Neural Network\\nDie Künstlichen Neuronalen Netzwerke, die bisher näher betrachtet wurden, also das\\nMultilayer Perceptron (vgl. Kap. 3) und das Convolutional Neural Network (vgl. Kap. 4), ge-\\nhören zu den vorwärtsgekoppelten Netzwerken (engl. Feedforward Networks ). Die Daten\\nbzw. Signale durchlaufen diese Netzwerke von den Eingaben-Einheiten zu den Ausgaben-\\nEinheiten. CNN wurden beispielsweise in Anlehnung an das visuelle Sehen entwickelt\\nund werden insbes. im Bereich der Bilderkennung eingesetzt. Aber auch in anderen Berei-\\nchen wie z.B. der Sprachverarbeitung (engl. Natural Language Processing ( NLP )) konnten\\nCNN erste Erfolge aufweisen [Col+11]. Dabei sind andere KNN-Typen aufgrund ihrer Ar-\\nchitektur eigentlich besser zur Problemlösung von Aufgaben geeignet, in denen Sequen-\\nzen verarbeitet werden müssen. Es sind die sogenannten rekurrenten bzw. rekursiven\\nneuronalen Netzwerke. Im Gegensatz zu den bisher betrachteten Netzwerken gibt es in\\neinem Recurrent Neural Network ( RNN )auch Verbindungen zwischen den Neuronen der-\\nselben Schicht oder Verbindungen von Neuronen zu Neuronen einer vorangegangenen\\nSchicht. Auch hier stand wieder die Natur Pate: In der Großhirnrinde, dem Neocortex,\\nsind Neuronen auch auf diese Weise verschaltet. Der Neocortex ist für höhere Gehirn-\\nfunktionen wie Motorik und Sprache verantwortlich. Außerdem spielt er eine wichtige\\nRolle für Gedächtnis und Lernprozesse.\\nAm 13.11.2012 erschien das Buch mit dem Titel How to Create a Mind: The Secret of Hu-\\nman Thought Revealed von Ray Kurzweil [Kur12], die deutsche Übersetzung lautet: Das\\nGeheimnis des menschlichen Denkens. Einblicke in das Reverse Engineering des Gehirns.\\nDer hierarchische Aufbau der Welt spiegelt sich nach Meinung Kurzweils im hierarchi-\\nschen Aufbau des Neocortex wider. Dieser Teil der Großhirnrinde besteht aus 300 Mil-\\nlionen Mustererkennern. Je nach Komplexität laufen die Mustererkennungsprozesse auf\\nverschiedenen Ebenen ab. Kurzweil selbst hat u.a. das Hidden Markov Model ( HMM )als\\nMustererkenner im Bereich der Spracherkennung verwendet. Abb. 5.1 zeigt ein Beispiel\\nfür ein solches stochastisches Modell, welches aus Markov-Ketten besteht. Die Basisele-\\nmente sind Zustände x, Beobachtungen y, Übergangswahrscheinlichkeiten aund Emissi-\\nonswahrscheinlichkeiten b.\\nAbbildung 5.1: Beispiel für ein Hidden Markov Model [Raz12]\\n435 Recurrent Neural Network\\nAuch wenn Kurzweil kein Freund von Künstlichen Neuronalen Netzwerken ist, so stellen\\ndieRNN jedoch ebenfalls sehr gute Mustererkenner im Bereich der Sprachanalyse dar.\\nIn den folgenden Abschnitten werden zunächst die einzelnen Bausteine, d.h. das rekur-\\nrente Neuron bzw. die Memory Cell vorgestellt, anschließend Typen von Input-Output-\\nSequenzen differenziert, spezielle Lernverfahren präsentiert und abschließend zwei neue\\nund bedeutende Konzepte erläutert: Long Short-Term Memory ( LSTM )und Gated Recurrent\\nUnit (GRU) .\\n5.1 Bausteine\\nNeben den Feedforward -Verbindungen zwischen den Neuronen bzw. Einheiten der Künst-\\nlichen Neuronalen Netzwerke, sind auch drei Typen von Feedback -Verbindungen möglich\\n(siehe Abb. 5.2):\\n1 Direkte Rückkopplung Direct Feedback w dblau\\n2 Indirekte Rückkopplung Indirect Feedback w igrün\\n3 Seitliche Rückkopplung Lateral Feedback w lrot\\nAbbildung 5.2: Drei Feedback-Typen [Mer16]\\nDie Basiseinheit eines RNN, das rekurrente Neuron (engl. Recurrent Neural Unit ), ist eine\\nEinheit mit einer direkten Rückkopplung, d.h. der eigene Ausgang wird als weiterer\\nEingang verwendet (siehe blaue Verbindung in Abb. 5.2 bzw. die neue Abb. 5.3).\\nAbbildung 5.3: Basiseinheit im RNN [Ola15]\\n445.2 Sequenzen\\nIn jedem Zeitschritt tbzw. Frame bekommt dieses spezielle Neuron Aneben den normalen\\nEingaben ~xtauch die Ausgabe vom vorherigen Zeitschritt ht\\x001. Wenn man nun viele\\nsolcher Zeitschritte bzw. Frames betrachtet, und für jeden Zeitschritt die RNN-Basiseinheit\\nnacheinander darstellt, ergibt sich das folgende Bild (siehe Abb. 5.4).\\nAbbildung 5.4: Ausgerollte RNN-Basiseinheit [Ola15]\\nDiese Darstellungsform nennt man auch ausgerollte RNN-Einheit. Für den Fall t=0\\nwird noch keine Rückkopplung verwendet bzw. diese wird auf null gesetzt. Das Verhal-\\nten dieser RNN-Einheit erinnert an eine Kette. Deshalb sind RNN auch besonders gut\\ngeeignet, um Sequenzen oder Listen von Daten zu verarbeiten. Im Zeitschritt tﬂießen\\nalso nicht nur die Eingaben ~xt, sondern auch alle vorherigen Eingaben ~x0bis~xt\\x001indirekt\\nin Form der Ausgaben h0bisht\\x001in die Berechnung der neuen Ausgabe htmit ein. Diese\\nRNN-Einheit wirkt also wie ein Gedächtnis. Deshalb wird diese Einheit auch als Memory\\nCellbezeichnet.\\n5.2 Sequenzen\\nEine Sequenz ist eine Folge von normalerweise gleichartigen Elementen. Bspw. kann eine\\nSequenz eine Folge von vektoriellen Eingabedaten ~x0bis~xtsein. Betrachten wir nun die\\nVerarbeitung solcher Sequenzen, die aus Vektoren bestehen. In Abb. 5.5 sind fünf Fälle\\nunterschieden.\\nAbbildung 5.5: Verarbeitung von Sequenzen [Kar15]\\nJedes Rechteck stellt einen Vektor dar, wobei den Farben verschiedene Bedeutungen zu-\\nkommen: Rot für die Eingabe ( Input ), Grün für das Verborgene ( Hidden ) und Blau für\\ndie Ausgabe ( Output ) - analog also zum Aufbau und den Schichten von KNN. Die Pfeile\\nrepräsentieren mathematische Operationen, z.B. Matrix-Multiplikationen. Im ersten Fall\\n(One to One ) kommen noch keine Sequenzen vor. Hier wird ein Eingabe-Vektor auf einen\\n455 Recurrent Neural Network\\nAusgabe-Vektor abgebildet. Dies ist bspw. bei der Objektklassiﬁzierung der Bilderken-\\nnung der Fall: Das 2D-Bild als Input kann auch als Vektor dargestellt werden und der\\nOutput-Vektor enthält die unterschiedlichen Objektklassen. Im zweiten Fall ( One to Many )\\nbekommt man als Output eine Sequenz von Vektoren. Eine Beispielanwendung ist das\\nautomatische Generieren von Bildunterschriften ( Image Captioning ): Als Input wird das\\nBild verwendet, als Output kommt eine Folge von Wörtern heraus. Im dritten Fall ( Many\\nto One ) sind Input und Output im Vergleich zum zweiten Fall gerade vertauscht. Die Sen-\\ntiment Analysis ist eine typische Anwendung: Ein Text, also eine Folge von Wörtern, wird\\nhinsichtlich verschiedener Stimmungen (2 Klassen: positiv, negativ) untersucht. Der vier-\\nte Fall ( Many to Many ) mit vielen verborgenen Einheiten beschreibt bspw. das maschinelle\\nÜbersetzen: Ein englischer Satz, bestehend aus nWörtern, wird ins Deutsche übertra-\\ngen ( mWörter). Der letzte Fall sieht auf dem ersten Blick so ähnlich aus wie der vierte.\\nGemeint ist damit aber die synchrone Verarbeitung der Sequenzen. Eine typische Anwen-\\ndung ist die automatische Objektklassiﬁkation in Videos. Im ersten Fall wurden Objekte\\nauf Bildern klassiﬁziert. Ein Video besteht aus einer Folge von Bildern bzw. Frames . Zu\\njedem Bild erfolgt nun eine synchrone Objektklassiﬁzierung.\\nMan kann also feststellen: Die Anwendungsmöglichkeiten der Verarbeitung von Sequen-\\nzen beliebiger Längen ist sehr vielfältig [Kar15]. Mit Hilfe von RNN lassen sich diese und\\nnoch andere Anwendungsfälle durchführen. Deshalb sind RNN sehr mächtige Werkzeu-\\nge im Rahmen des maschinellen Lernens.\\n5.3 Lernverfahren\\nDas Standard-Lernverfahren, um ein Feedforward -Netzwerk zu trainieren, ist der Backpro-\\npagation -Algorithmus (vgl. Kap. 3.2). Beim überwachten Lernen wird dabei versucht, den\\nNetzwerkfehler zu minimieren, wobei eine Kostenfunktion bzw. Loss-Funktion zur Be-\\nrechnung eingesetzt wird, welche die Abweichung zwischen der Netzwerkausgabe und\\ndem tatsächlichen Wert bewertet. Es handelt sich also um ein Optimierungsproblem, das\\nbspw. mit Hilfe des Gradientenabstiegsverfahrens gelöst werden kann.\\nIn RNN, also rekurrenten Netzwerken, kann das Lernverfahren Backpropagation Through\\nTime ( BPTT )zum Trainieren eingesetzt werden [Wer88]. Dabei wird das Netzwerk nach\\nder Zeit ausgerollt. Dies entspricht dem Ausrollen von vielen RNN-Basiseinheiten (vgl.\\nKap. 5.1). Die Inputs und Outputs jeder Kopie des ausgerollten Netzwerks sind zwar\\nunterschiedlich, aber es werden die gleichen Parameter (Gewichte, Bias) verwendet, d.h.\\ndiese werden somit im Zeitverlauf geteilt. Beim Training des Netzwerks werden die\\nParameter dann so eingestellt, dass die Kostenfunktion möglichst minimal wird. Dies\\nkann wieder mit Hilfe des Gradientenabstiegsverfahrens erfolgen, also mit dem bereits\\nbekannten Backpropagation -Algorithmus. Daher hat der Algorithmus für RNN auch den\\nNamen BPTT bekommen.\\nFür lange Sequenzen kann dieses RNN als sehr tiefes KNN verstanden werden. Beim\\nGradientenabstiegsverfahren in tiefen neuronalen Netzen gibt es jedoch das Problem\\nder verschwindenden Gradienten (vgl. Kap. 3.6). Eine Lösung für dieses Problem stellen\\nspezielle Architekturen wie Long Short-Term Memory (vgl. Kap. 5.4) oder Gated Recurrent\\nUnit (vgl. Kap. 5.5) dar.\\n465.4 Long Short-Term Memory\\n5.4 Long Short-Term Memory\\nDas Netzwerk Long Short-Term Memory ( LSTM ), zu Deutsch: langes Kurzzeitgedächtnis,\\nist ein spezielles RNN, das 1997 von Sepp Hochreiter und Jürgen Schmidhuber entwickelt\\nwurde und das Problem verschwindender Gradienten löst [HS97]. Zwei Jahre später wur-\\nde noch ein Baustein hinzugefügt, damit auch Informationen wieder vergessen werden\\nkönnen bzw. eine Art Reset durchgeführt werden kann [GSC99]. Die Besonderheit einer\\nLSTM-Einheit besteht darin, dass sie eine innere Struktur aufweist (vgl. Abb. 5.6).\\nAbbildung 5.6: Aufbau einer LSTM-Einheit [Gre+15]\\nEine Zelle ist der zentrale Bestandteil, um Informationen zu verarbeiten und drei Tore\\nbzw. Schranken (engl. Gates ) steuern und regeln dabei den Informationsﬂuss:\\nInput Gate Informationsﬂuss in die Zelle\\nForget Gate Informationen behalten bzw. vergessen\\nOutput Gate Informationsﬂuss aus der Zelle\\nDie drei Gates benutzen die sigmoide Aktivierungsfunktion, während ansonsten für die\\nanderen Elemente der Tangens Hyperbolicus verwendet wird. Die ausgerollte LSTM-Einheit\\nist in Abb. 5.7 dargestellt.\\nAbbildung 5.7: Ausgerollte LSTM-Einheit [Ola15]\\n475 Recurrent Neural Network\\nDie Funktionsweise der LSTM-Einheit ist in den folgenden Abbildungen skizziert.\\nAbbildung 5.8: Schritt 1: Forget-Gate [Ola15]\\nAbbildung 5.9: Schritt 2: Input-Gate [Ola15]\\nAbbildung 5.10: Schritt 3: Zelle [Ola15]\\nAbbildung 5.11: Schritt 4: Output-Gate [Ola15]\\n485.5 Varianten\\nIm ersten Schritt (siehe Abb. 5.8) entscheidet die LSTM-Einheit, welche Informationen der\\nZelle nicht mehr benötigt werden. Die Eingaben xtund ht\\x001werden dabei verarbeitet,\\nwobei ht\\x001die Ausgabe dieser rekurrenten Zelle im vorherigen Zeitschritt repräsentiert.\\nDie sigmoide Aktivierungsfunktion des Forget Gate liefert kontinuierliche Werte zwischen\\n0 und 1, wobei 0 bedeutet, dass alle Informationen vergessen werden, während 1 bedeutet,\\ndass die Informationen komplett behalten werden. Als Zwischenergebnis wird der Wert\\nftbestimmt.\\nIm nächsten Schritt (siehe Abb. 5.9) entscheidet dann die LSTM-Einheit, welche neuen\\nInformationen in der Zelle gespeichert werden sollen. Auch hier werden wieder die Ein-\\ngaben xtund ht\\x001verarbeitet. Einerseits wird von der sigmoiden Aktivierungsfunktion\\ndesInput Gate das Zwischenergebnis itberechnet, andererseits wird mittels des Tangens\\nHyperbolicus der neue Kandidat für den Zellenwert ˜Ctbestimmt.\\nIn Schritt 3 (siehe Abb. 5.10) wird der Zellenwert Ctnun aktualisiert. Hierzu wird der alte\\nZellenwert Ct\\x001mit der Ausgabe des Forget Gate ftgewichtet. Der neue Kandidat ˜Ctwird\\nmit dem Zwischenergebnis des Input Gate itgewichtet. Dann werden beide Teile addiert\\nund als Ctgespeichert.\\nIm vierten und letzten Schritt (siehe Abb. 5.11) muss die LSTM-Einheit noch entscheiden,\\nwelcher Wert schließlich weitergegeben werden soll. Hierzu werden zunächst die ur-\\nsprünglichen Eingaben xtund ht\\x001durch die sigmoide Aktivierungsfunktion des Output\\nGate geschickt und otberechnet. Anschließend wird der Tangens Hyperbolicus auf den aktu-\\nellen Zellzustand Ctangewendet und die Ausgabe htals Produkt (Matrix-Multiplikation)\\ndieser beiden Teilergebnisse berechnet.\\n5.5 Varianten\\nIm letzten Abschnitt wurde das Netzwerk Long Short-Term Memory vorgestellt. Zu die-\\nsem Netzwerk gibt es einige Varianten, je nachdem wie die einzelnen inneren Elemente\\nmiteinander verschaltet sind.\\nDas Besondere am Gucklock-LSTM (engl. Peephole-LSTM ) ist, dass die Gates den Status der\\nZelle sehen und somit auch Informationen aus der Zelle mitverarbeiten können [GS00].\\nIn Abb 5.12 ist der Aufbau schematisch dargestellt. Der Unterschied zur normalen LSTM-\\nEinheit ist, dass die sigmoiden Aktivierungsfunktionen der drei Gates auch die Zellwerte\\nCt\\x001bzw. Ctals Input verarbeiten.\\nAbbildung 5.12: Peephole-LSTM [Ola15]\\n495 Recurrent Neural Network\\nIn einer anderen Variante sind Forget Gate und Input Gate gekoppelt. Dies vereinfacht den\\nAufbau, da nun nur noch eine sigmoide Aktivierungsfunktion benötigt wird, um den\\nneuen Zellenzustand Ctzu bestimmen. Statt also zwei unabhängige Entscheidungen dar-\\nüber zu treffen, welche Informationen von xtund ht\\x001vergessen bzw. behalten werden\\nsollen, werden diese Entscheidungen in dieser speziellen LSTM-Einheit nun zusammen\\ngetroffen. Es werden nur dann alte Informationen vergessen, wenn diese durch neue\\nInformationen ersetzt werden. Oder anders ausgedrückt: Es werden nur dann neue Infor-\\nmationen verwendet, wenn dafür alte Informationen aussortiert werden. In Abb. 5.13 ist\\nder prinzipielle Aufbau dieser LSTM-Variante dargestellt.\\nAbbildung 5.13: Gekoppelte LSTM-Einheit [Ola15]\\nDie bekannteste LSTM-Variante ist die GRU , die Gated Recurrent Unit [Cho+14]. Auch in\\ndieser Variante wird auf eine sigmoide Aktivierungsfunktion verzichtet, die Änderungen\\nsind aber gravierender. Das Forget Gate und das Input Gate werden zu einem Update Gate\\nkombiniert. Außerdem werden der Zellzustand Ctund der verborgene Zustand htzu-\\nsammengeführt. Abb. 5.14 zeigt den schematischen Aufbau der GRU. Diese Variante ist\\neinfacher als die Original-LSTM-Einheit aber ähnlich leistungsstark.\\nAbbildung 5.14: Gated Recurrent Unit (GRU) [Ola15]\\nXingjian Shi und Kollegen der Universität Hongkong haben 2015 die beiden Architekturen\\nCNN und RNN zu einem Convolutional LSTM verbunden [Shi+15]. Daneben existieren\\nnoch viele andere LSTM-Varianten [Gre+15] [JZS15], die im Rahmen dieser Arbeit aber\\nnicht weiter betrachtet werden .\\n505.6 Einsatzgebiete\\n5.6 Einsatzgebiete\\nSehr stark vereinfacht ausgedrückt, werden CNN im Bereich der Bilderkennung und RNN\\nim Bereich der Spracherkennung einsetzt. Die Anwendungsbreite von RNN im Allgemei-\\nnen bzw. von LSTM oder GRU im Speziellen ist eigentlich aber viel größer. Diese Netz-\\nwerke können in den folgenden Domänen eingesetzt werden: Robotersteuerung, Zeitrei-\\nhenanalyse und Prognose, Spracherkennung, Rhythmus-Lernen, Musik-Komponieren,\\nGrammatik-Lernen, Handschriften-Erkennung, Erkennung menschlicher Aktionen, Ge-\\nbärdensprache-Übersetzung, Homologie-Modellieren von Proteinen, Überwachung von\\nbetrieblichen Geschäftsprozessen, Vorhersagen im medizinisch-klinischen Behandlungs-\\npfad usw. [Wik18j; Wik18f].\\nTrotzdem bleibt die Spracherkennung das primäre Einsatzgebiet. Der Prozess der Spra-\\ncherkennung kann in mehrere Phasen bzw. Schritte unterteilt werden (siehe Abb. 5.15).\\nAbbildung 5.15: Modell des Spracherkennungsprozesses [Mwk04]\\nZunächst werden die akustischen Eingangssignale analysiert, um Phoneme zu ﬁnden.\\nDies sind die kleinsten unterscheidbaren Einheiten der gesprochenen Sprache. In der\\ndeutschen Sprache werden bspw. lange (/a:/) und kurze (/a/) Vokale, nasale (/m/) und\\nstimmlose (/p/) Konsonanten usw. unterschieden. Im nächsten Schritt werden dann\\naus diesen Lauten mit Hilfe eines Wörterbuchs der zugrundeliegenden Sprache einzel-\\nne Wörter gebildet. Das Sprachmodell versucht dann, aus den Wörtern einen Satz zu\\nbilden. Hierbei können bspw. auch Grammatikmodelle oder statistische Analysen ein-\\ngesetzt werden, denn bestimmte Wortkombinationen treten häuﬁger auf als andere. Die\\nSpracherkennung beschäftigt sich aber nicht mehr damit, den Sinn und die Bedeutung\\ndes gebildeten Satzes zu entschlüsseln. Hierzu kommen dann andere Methoden aus dem\\nBereich Natural Language Processing (NLP) zum Einsatz.\\nMit Hilfe des von Ray Kurzweil favorisierten Hidden Markov Model ( HMM )konnten sehr\\nerfolgreich Phoneme erkannt werden. Die gesprochenen Laute werden dabei als versteck-\\nte Zustände aufgefasst, während die tatsächlich hörbaren Töne als Emissionen betrachtet\\nwerden (vgl. Abb. 5.1).\\nAlternativ werden auch KNN in der Spracherkennung verwendet. Diese konnten sich\\naber erst in den letzten Jahren gegenüber dem HMM durchsetzen, weil es durch De-\\nep Learning möglich war, auch große Datenmengen in tiefen KNN in annehmbarer Zeit\\nauf Prozessoren (CPUs und insbes. GPUs) zu verarbeiten. Die präferierten Netzwerk-\\nArchitekturen sind LSTM und seine Varianten. Inzwischen setzen die großen und nam-\\n515 Recurrent Neural Network\\nhaften US-amerikanischen Technologieunternehmen LSTM in ihren digitalen Produkten\\nbzw. intelligenten Assistenten ein [Sch17], also bspw. Apple in Siri [Efr16], Amazon in\\nPolly und in Alexa [Vog16] sowie Google in seine Smartphone-Spracherkennung [Bea15]\\nund in Allo [Kha16].\\n526 Software\\nNachdem nun in den letzten Kapiteln die Grundlagen zum Thema KNN und Deep Lear-\\nning gelegt und drei erfolgreiche Netzwerk-Architekturen vorgestellt wurden, sollen in\\ndiesem Kapitel ausgewählte Softwarelösungen zu diesem Thema präsentiert werden. Als\\nSoftware soll eine Open Source Lösung eingesetzt werden. Darunter versteht man Soft-\\nware, deren Quelltext (engl. Source Code ) öffentlich ist und die von Dritten eingesehen,\\ngenutzt und verändert werden darf. Somit fallen für diese Art von Software keine Lizenz-\\ngebühren für deren Nutzung an. Die Software ist aber trotzdem nicht kostenlos. Nach\\ndem Konzept Total Cost of Ownership ( TCO )entfallen bspw. auch Kosten auf Hardwa-\\nre, Installation, Konﬁguration, Betrieb, Pﬂege und Wartung, also für Sachmittel, Personal\\nund Energie. Auf der englischsprachigen Wikipedia-Webseite https://en.wikipedia.org/\\nwiki/Comparison_of_deep_learning_software [Wik18c] ﬁndet man eine Übersicht von\\nSoftwarelösungen zum Thema Deep Learning . Die meisten dieser Lösungen sind Open\\nSource und auf der Internetplattform GitHub vorhanden, auf denen Quelltexte öffentlich\\nbereit gestellt werden [Wik18e]. Der Name dieser Plattform setzt sich zusammen aus den\\nWorten Gitund Hub.Gitist ein verteiltes Versionsverwaltungssystem [Wik18d]. Ein Hub\\nist der zentrale Knoten in einem Netzwerk. Am 08.03.2018 wurde eine Suche auf GitHub\\nzu dem Begriff Deep Learning durchgeführt [Git18]. 28.104 Repositories wurden gefunden.\\nRepositories sind Projektverzeichnisse, die mit Git verwaltet und auf der Plattform bereit-\\ngestellt werden. Die meisten dieser Repositories können zu einer Programmiersprache\\nzugeordnet werden. Einige Repositories enthalten aber auch gar keine Quelltexte, sondern\\nsind bspw. als Sammlung nützlicher Links angelegt. Tabelle 6.1 zeigt die Suchergebnisse,\\naufgeschlüsselt nach den Top 10 Programmiersprachen.\\nNr Sprache Anzahl Anteil\\n1 Python 8.579 41,3 %\\n2 Jupyter Notebook 7.641 36,8 %\\n3 HTML 2.392 11,5 %\\n4 C++ 499 2,4 %\\n5 Matlab 442 2,1 %\\n6 Java 333 1,6 %\\n7 JavaScript 294 1,4 %\\n8 Lua 220 1,1 %\\n9 TeX 197 0,9 %\\n10 Cuda 193 0,9 %\\nTabelle 6.1: Übersicht von DL-Repositories auf der Plattform GitHub [Git18] (Stand: 08.03.2018)\\nEs fällt auf, dass die meisten Repositories zu Deep Learning in der Programmiersprache\\nPython geschrieben sind (41,4 %). Knapp dahinter folgt Jupyter Notebook mit 36,8 %.\\nJupyter Notebook ist eigentlich keine eigene Programmiersprache, sondern eine Open\\nSource Webapplikation, mit der Dokumente erstellt und ausgetauscht werden können.\\n536 Software\\nSehr häuﬁg wird diese Anwendung benutzt, um Python-Quelltexte zu schreiben und aus-\\nzuführen. Somit wird sicherlich auch ein großer Teil der Jupyter Notebook Repositories\\nPython-Skripte beinhalten. Hypertext Markup Language ( HTML ) ist genaugenommen\\nkeine Programmiersprache sondern eine Auszeichnungssprache. Andere bekannte Pro-\\ngrammiersprachen wie bspw. C++ oder Java spielen also nur eine untergeordnete Rolle.\\nPython ist also die dominante Programmiersprache im Bereich Deep Learning .\\nDie Programmiersprache Python wurde 1991 von Guido van Rossum entwickelt, wird\\naber mittlerweile von der Python Software Foundation weiterentwickelt und veröffent-\\nlicht [Wik18i]. Sie zählt sowohl zu den objektorientierten Programmiersprachen als auch\\nzu den Skriptsprachen. Python zeichnet sich durch seine Einfachheit, Übersichtlichkeit\\nund Erweiterbarkeit aus. Die Sprache kommt mit relativ wenigen Schlüsselwörtern (ver-\\neinfacht: Vokabeln) aus. Eine Besonderheit ist, dass Anweisungsblöcke durch Einrückun-\\ngen strukturiert werden. Mit dem Paketmanager PIP lassen sich sehr leicht Programmbi-\\nbliotheken als zusätzliche Pakete vom zentralen Repository Python Package Index ( PyPI )\\ninstallieren. Im Bereich Data Science gibt es eine große Auswahl an wissenschaftlichen Bi-\\nbliotheken wie z.B. NumPy, Pandas, Matplotlib usw. Die Python-Distribution Anaconda\\nenthält bereits viele dieser Pakete und außerdem das Jupyter Notebook. Python ist für die\\ngängigen Betriebssysteme Linux, MacOSX und Windows frei erhältlich und wird aktuell\\nin zwei Versionslinien entwickelt. Python 2 gibt es seit dem Jahr 2000 und aktuell wird\\ndie Version 2.7.14 ausgeliefert. Python 3 ist seit 2008 verfügbar und 3.6.4 ist die aktuellste\\nVersion dieser Linie. Beide Versionslinien sind jedoch nicht kompatibel zueinander, d.h.\\nman muss sich für eine entscheiden. Falls man die freie Wahl hat, sollte man die neuere,\\nalso Version 3, benutzen.\\nIn den nächsten Kapiteln werden ausgewählte Repositories und die dahinterliegenden\\nProjekte vorgestellt. Aufgrund der Vielzahl der Repositories ist es unmöglich, alle vor-\\nzustellen. Hierzu muss also eine Auswahl getroffen werden. Die Ergebnisse auf GitHub\\nlassen sich nach der besten Übereinstimmung zum Suchbegriff ( Best Match ), den meis-\\nten positiven Bewertungen ( Most Stars ), den meisten privaten Repository-Kopien ( Most\\nForks ) oder der neusten Aktualisierung ( Recently updated ) sortieren. Außerdem kann man\\nstatistische Daten zu jedem Repository abrufen, deren Attribute in Tab. 6.2 kurz erklärt\\nsind.\\nNr Attribut Erklärung\\n1 Watch Projekt beobachten\\n2 Star Positive Bewertung\\n3 Fork Eigene, private Variante des Repositories\\n4 Commits Einreichen von Quellcode (neu, geändert)\\n5 Contributors Am Projekt beteiligte Mitglieder\\n6 Issues Open Frage stellen, Fehler melden\\n7 Issues Closed Issue beantwortet\\n8 Pull Requests Open Anfrage an den Admin zwecks Commit\\n9 Pull Requests Closed Anfrage abgeschlossen\\n10 Release (Date) Nr. der Release-Version und Datum\\n11 Last Commit Zeitstempel der letzten Änderung\\nTabelle 6.2: Übersicht von Attributen zu einem Repository auf GitHub\\n54Tabelle 6.3 zeigt eine Übersicht von ausgewählten Softwarelösungen zum Thema Deep\\nLearning in alphabetischer Reihenfolge. Die Anzahl der Sterne (engl. Stars ) sind positive\\nBewertungen von Mitgliedern der Plattform GitHub für die jeweilige Softwarelösung\\n(Stand: 08.03.2018).\\nNr Name Stars Organisation\\n1 Caffe 23.110 UC Berkeley\\n2 Caffe2 7.499 Facebook\\n3 Chainer 3.561 Preferred Networks\\n4 CNTK 13.979 Microsoft\\n5 Darknet 6.115 (Joseph Redmon)\\n6 Deepwater 252 H2O.ai\\n7 DL4J 8.447 Skymind\\n8 Dlib 4.365 (Davis E. King)\\n9 Gluon 2.094 Apache\\n10 DSSTNE 4.057 Amazon\\n11 Keras 26.583 (François Chollet)\\n12 Lasagne 3.384 Open Community\\n13 MXNet 13.292 Apache\\n14 Neon 3.422 Intel\\n15 OpenNN 442 Artelnics\\n16 Paddle 6.519 Baidu\\n17 PyTorch 12.777 -\\n18 SINGA 1.317 Apache\\n19 TensorFlow 91.783 Google\\n20 TFLearn 7.705 Open Community\\n21 Theano 7.976 Université de Montréal\\n22 Torch 7.728 -\\nTabelle 6.3: Übersicht ausgewählter DL-Softwarelösungen auf der Plattform GitHub [Git18]\\nFeststellen lässt sich bereits, dass die Softwarelösung TensorFlow innerhalb der Plattform\\nGitHub mit deutlichem Abstand am beliebtesten und bekanntesten ist. Die Lösung TF-\\nLearn ist eine Ergänzung zu TensorFlow. Keras ist eine Software, die auf TensorFlow oder\\nTheano basiert. Die Lösungen von großen, namhaften Unternehmen wie Facebook, Mi-\\ncrosoft, Amazon, Intel, Baidu und Google sind vertreten. Aus dem wissenschaftlichen\\nBereich der Universitäten gibt es nur zwei Lösungen: Caffe und Theano. In den nachfol-\\ngenden Abschnitten werden diese 22 ausgewählten Bibliotheken und Frameworks nun\\nnacheinander kurz vorgestellt.\\n556 Software\\n6.1 Caffe\\nCaffe wurde von Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan\\nLong, Ross Girshick, Sergio Guadarrama und Trevor Darrell vom Berkeley Artiﬁcial Intel-\\nligence Research ( BAIR ) und Berkeley Vision and Learning Center ( BVLC ) an der Kalifor-\\nnischen Universität Berkeley entwickelt [Jia+14]. Die Bibliothek verfügt über eine Vielzahl\\nvon Möglichkeiten, Künstliche Neuronale Netzwerke ( KNN ) aus Schichten (engl. Layers )\\nzu erstellen und diese dann mit diversen Algorithmen zu trainieren. Zu den Schichten\\ngehören:\\n1Data Layers\\n2Vision (Convolutional) Layers\\n3Recurrent Layers\\n4Common Layers\\n5Normalization Layers\\n6Activation / Neuron Layers\\n7Utility Layers\\n8Loss Layers\\nDamit lassen sich u.a. bekannte DL-Architekturen wie MLP ,CNN ,RNN und LSTM ver-\\nwenden. Aus Gründen der Performance ist die Software in C++ entwickelt worden, wobei\\nCuDNN von Nvidia unterstützt wird. Programmierschnittstellen werden zu Python und\\nMatlab angeboten [JS18]. Das Unternehmen Yahoo hat Caffe in die Big Data Anwendung\\nApache Spark eingebunden, um Deep Learning als verteilte Softwarelösung zu verwenden.\\nTabelle 6.4 fasst die wichtigsten Merkmale der Caffe-Bibliothek zusammen [She+18].\\nName Caffe\\nOrganisation UC Berkeley\\nWebseite https://github.com/BVLC/caffe\\nLizenz BSD 2-Clause\\nPlattformen Linux, MacOSX, Windows\\nGeschrieben in C/C++\\nAPIs Python, Matlab\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 2.156 | 23.110 | 14.116\\nCommits | Contributors 4.110 | 264\\nIssues: Open | Closed 536 | 3.622\\nRequests: Open | Closed 241 | 1.872\\nRelease (Date) 1.0 (18.04.2017)\\nLast Commit 07.03.2018\\nTabelle 6.4: Steckbrief zur Caffe-Bibliothek [She+18]\\n566.2 Caffe2\\n6.2 Caffe2\\nCaffe2 ist ein leichtgewichtiges, modular aufgebautes und skalierbares Framework für\\nDeep Learning , das von Yangqing Jia, Bram Wasti, Pieter Noordhuis, Luke Yeager, Simon\\nLayton, Dmytro Dzhulgakov und anderen auf Basis von Caffe (vgl. Kap. 6.14) beim US-\\nUnternehmen und sozialen Netzwerk Facebook entwickelt wurde [Sou18]. Das Frame-\\nwork ist auf Performance optimiert, es verwendet die Bibliotheken cuDNN, cu BLAS und\\nNCCL von Nvidia, d.h. die Rechnungen lassen sich auch auf vielen GPU s ausführen.\\nProgrammierschnittstellen werden zu C++ und Python in der Version 2 angeboten, für\\nPython in der Version 3 ist die Unterstützung momentan nur experimentell.\\nDie Modelle von Caffe lassen sich leicht zu Caffe2 konvertieren und damit wiederverwen-\\nden. Hierzu gibt es auch Skripte, die diese Anpassung automatisch ausführen. Statt der\\nLayers in Caffe sind in Caffe2 die Operatoren (engl. Operators ) die zentralen Elemente, mit\\ndenen gearbeitet wird (vgl. Abb. 6.1). Es gibt über 400 verschiedene Operatoren in Caffe2.\\nAbbildung 6.1: Vergleich Caffe und Caffe2 [Tok+15]\\nTabelle 6.5 fasst die wichtigsten Merkmale des Caffe2-Frameworks zusammen [Jia+18].\\nName Caffe2\\nOrganisation Facebook\\nWebseite https://github.com/caffe2/caffe2\\nLizenz Apache 2.0\\nPlattformen Linux, MacOSX, Windows\\nGeschrieben in C++\\nAPIs C++, Python\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 543 | 7.499 | 1.763\\nCommits | Contributors 175 | 3.466\\nIssues: Open | Closed 517 | 626\\nRequests: Open | Closed 73 | 980\\nRelease (Date) 0.8.1 (08.08.2017)\\nLast Commit 08.03.2018\\nTabelle 6.5: Steckbrief zum Caffe2-Framework [Jia+18]\\n576 Software\\n6.3 Chainer\\nChainer ist ein DL-Framework, das von Yuya Unno, Seiya Tokui, Ryosuke Okuta, Ma-\\nsayuki Takagi und Shunta Saito bei Preferred Networks entwickelt wurde [Net18]. Mo-\\nderne Netzwerke wie das Convolutional Neural Network und das Recurrent Neural Network\\nwerden unterstützt. Eine Python-API wird zur Programmierung zur Verfügung gestellt.\\nGPU -Unterstützung durch CuPy, welches auf CUDA und cuDNN basiert, wird ebenfalls\\nangeboten [Tok+15]. Modelle, die mit Caffe (vgl. Kap. 6.14) erstellt wurden, lassen sich\\nimportieren.\\nBei der Entwicklung der Modelle und dem Training der KNN wird eine neuartiger Ansatz\\nverwendet, der sogenannte Deﬁne-by-Run , der sich gegenüber dem klassischen Verfahren\\nDeﬁne-and-Run insofern unterscheidet, dass nun dynamische statt statische Graphen zum\\nEinsatz kommen (vgl. Abb. 6.2).\\nAbbildung 6.2: Vergleich Deﬁne-and-Run (links) mit Deﬁne-by-Run (rechts) [Tok+15]\\nTabelle 6.6 fasst die wichtigsten Merkmale des Chainer-Frameworks zusammen [Unn+18].\\nName Chainer\\nOrganisation Preferred Networks\\nWebseite https://github.com/chainer/chainer\\nLizenz MIT License\\nPlattformen Linux, MacOSX, Windows\\nGeschrieben in Python\\nAPIs Python\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 305 | 3.561 | 937\\nCommits | Contributors 12.233 | 157\\nIssues: Open | Closed 135 | 1.032\\nRequests: Open | Closed 73 | 3.212\\nRelease (Date) 4.0.0b4 (20.02.2018)\\nLast Commit 08.03.2018\\nTabelle 6.6: Steckbrief zum Chainer-Framework [Unn+18]\\n586.4 CNTK\\n6.4 CNTK\\nDas Microsoft Cognitive Toolkit ( CNTK ) ist ein DL-Framework, das von Frank Seide, Wil-\\nli Richert, Mark Hillebrand, Amit Agarwal, Jean Baptiste Faddoul, Zhou Wang, William\\nDarling und anderen bei Microsoft Research entwickelt wurde und seit April 2015 als\\nOpen Source Lösung angeboten wird [Bas+17]. KNN lassen sich als Serie von Rechnungs-\\nschritten in einem gerichteten Graphen modellieren. Es lassen sich MLP ,CNN und RNN\\nbzw. LSTM konstruieren und trainieren (z.B. SGD , Backpropagation), auch parallel auf\\nmehreren GPU s. Die Basis-Architektur zum CNTK-Framework ist in Abb. 6.3 dargestellt.\\nAbbildung 6.3: CNTK-Architektur [Bas+17]\\nDas CNTK ist zwar in C++ programmiert, für Python existiert aber ebenfalls eine API. Au-\\nßerdem lässt es sich als Backend für das DL-Framework Keras (vgl. Kap. 6.11) verwenden.\\nInsbesondere wird das Austauschformat Open Neural Network Exchange ( ONNX )unter-\\nstüzt, mit dem sich bspw. Modelle zwischen den DL-Frameworks CNTK, Caffe2 (vgl. Kap.\\n6.2), MXNet (vgl. Kap. 6.13) und PyTorch (vgl. Kap. 6.17) austauschen lassen.\\nTabelle 6.7 fasst die wichtigsten Merkmale des CNTK-Frameworks zusammen [Sei+18].\\nName CNTK\\nOrganisation Microsoft\\nWebseite https://github.com/Microsoft/CNTK\\nLizenz MIT License\\nPlattformen Linux, Windows\\nGeschrieben in C++\\nAPIs C#, C++, Python\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 1.330 | 13.979 | 3.705\\nCommits | Contributors 15.538 | 173\\nIssues: Open | Closed 406 | 2.217\\nRequests: Open | Closed 47 | 350\\nRelease (Date) 2.4 (01.02.2018)\\nLast Commit 08.03.2018\\nTabelle 6.7: Steckbrief zum CNTK-Framework [Sei+18]\\n596 Software\\n6.5 Darknet\\nMit dem Darknet ist nicht der manuelle Zusammenschluss von Rechnern zu einem so-\\ngenannten Peer-to-Peer-Overlay-Netzwerk gemeint, sondern das quelloffene Framework\\nzum Thema Künstliche Neuronale Netzwerke, welches von Joseph C. Redmon in der\\nProgrammiersprache C und mit Hilfe der Nvidia-Bibliothek CUDA entwickelt wurde\\n[Red18]. Aus dem Bereich Deep Learning werden die gängigen Modelle wie CNN und\\nRNN unterstützt. Die folgenden Beispiele sind vorhanden:\\n1 YOLO Echtzeit-Objekterkennung\\n2 ImageNet Objekt-Klassiﬁzierung\\n3 Nightmare Bildgenerierung (vgl. Abb. 6.4)\\n4 RNN Textgenerierung\\n5 DarkGo Spiel Go\\n6 Tiny Darknet Bild-Klassiﬁzierung\\n7 CIFAR-10 Bild-Klassiﬁzierung\\nAbbildung 6.4: Nightmare: Der Schrei\\n(Edvard Munch) [Red18]\\nTabelle 6.8 fasst die wichtigsten Merkmale des Darknet-Frameworks zusammen [Red+18].\\nName Darknet\\nOrganisation (Joseph Redmon)\\nWebseite https://github.com/pjreddie/darknet\\nLizenz Public Domain\\nPlattformen Linux, MacOSX\\nGeschrieben in C\\nAPIs C\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 512 | 6.115 | 2.976\\nCommits | Contributors 392 | 3\\nIssues: Open | Closed 303 | 90\\nRequests: Open | Closed 87 | 35\\nRelease (Date) -\\nLast Commit 26.11.2017\\nTabelle 6.8: Steckbrief zum Darknet-Framework [Red+18]\\n606.6 Deep Water\\n6.6 Deep Water\\nDie DL-Bibliothek Deep Water wurde von Wen Phan, Magnus Stensmo, Mateusz Dymc-\\nzyk, Arno Candel und Qiang Kou des Unternehmens H2O.ai als Erweiterung der H2O-\\nPlattform entwickelt [Pha+18]. Als Backend lassen sich bspw. Caffe (vgl. Kap. 6.14), MXNet\\n(vgl. Kap. 6.13) oder TensorFlow (vgl. Kap. 6.19) verwenden. Abb. 6.5 zeigt den schemati-\\nschen Aufbau der skalierbaren Client/Server-Architektur von Deep Water.\\nAbbildung 6.5: Deep Water Architektur [Can+18]\\nTabelle 6.9 fasst die wichtigsten Merkmale der Deep Water-Bibliothek zusammen [Can+18].\\nName Deep Water\\nOrganisation H2O.ai\\nWebseite https://github.com/h2oai/deepwater\\nLizenz Apache 2.0\\nPlattformen Linux, MacOSX, Windows\\nGeschrieben in Java\\nAPIs Flow, R, Python, Java, Scala, REST\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 99 | 252 | 83\\nCommits | Contributors 15 | 724\\nIssues: Open | Closed 7 | 32\\nRequests: Open | Closed 0 | 23\\nRelease (Date) 1.0.1 (21.10.2016)\\nLast Commit 20.02.2018\\nTabelle 6.9: Steckbrief zur Deepwater-Bibliothek [Can+18]\\n616 Software\\n6.7 DL4J\\nDeep Learning for Java ( DL4J )ist eine Bibliothek, die von Alex Black, Adam Gibson, Mela-\\nnie Warrick, Max Pumperla, Justin Long, Samuel Audet and Eron Wright von Skymind\\nursprünglich für die Programmiersprache Java entwickelt wurde [Bla+18]. Die mathema-\\ntischen Kernberechnungen sind in C/C++ und CUDA implementiert. Neben Java werden\\nauch APIs für Scala und Clojure angeboten. Python wird indirekt durch die Verwendung\\nvon DL4J als Backend in Keras (vgl. Kap. 6.11) unterstützt. Mit Keras ist es sogar möglich,\\nModelle aus anderen Lösungen wie Caffe (vgl. Kap. 6.14), TensorFlow (vgl. Kap. 6.19),\\nTheano (vgl. Kap. 6.21) oder Torch (vgl. Kap. 6.22) zu importieren.\\nVerschiedene Typen von Künstlichen Neuronalen Netzwerken lassen sich mit DL4J er-\\nstellen und trainieren, die zugehörigen Algorithmen lassen sich auch parallel auf Apache\\nHadoop und Spark ausführen [GNP+18].\\n1 MLP Multilayer Perceptron\\n2 CNN Convolutional Neural Network\\n3 RNN Recurrent Neural Network mit\\nLSTM Long Short-Term Memory\\n4 GAN Generative Adversarial Network\\n5 RBM Restricted Boltzmann Machine\\n6 DBN Deep Belief Network\\n7 DAE Deep Autoencoder\\n8 SDA Stacked Denoising Autoencoder\\nTabelle 6.10 fasst die wichtigsten Merkmale der DL4J-Bibliothek zusammen [Bla+18].\\nName Deep Learning for Java\\nOrganisation Skymind\\nWebseite https://github.com/deeplearning4j/deeplearning4j\\nLizenz Apache 2.0\\nPlattformen Linux, MacOSX, Windows\\nGeschrieben in C/C++, Java\\nAPIs Java, Scala, Clojure, (Python)\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 790 | 8.447 | 4.075\\nCommits | Contributors 9.468 | 140\\nIssues: Open | Closed 619 | 2.033\\nRequests: Open | Closed 7 | 2.115\\nRelease (Date) 0.9.2 (08.12.2017)\\nLast Commit 08.03.2018\\nTabelle 6.10: Steckbrief zur DL4J-Bibliothek [Bla+18]\\n626.8 Dlib\\n6.8 Dlib\\nDlib ist eine Bibliothek und ein Werkzeug für Maschinenlernen, welches von Davis E. King\\nin der Programmiersprache C++ bereits ab 2002 entwickelt wurde [Kin09]. Die Software\\nist Komponentenbasiert aufgebaut (vgl. Abb. 6.6).\\nAbbildung 6.6: Komponenten von Dlib [Kin09]\\nNeben den klassischen Architekturen von Künstlichen Neuronalen Netzwerken wie bspw.\\nSupport Vector Machine ( SVM ), Multilayer Perceptron ( MLP ) oder RBF-Netze, haben\\nTechniken des Deep Learning ebenfalls Einzug in Dlib gefunden [Kin18]. Mittlerweile wird\\nauch eine API für Python bereitgestellt.\\nTabelle 6.11 fasst die wichtigsten Merkmale der Dlib-Bibliothek zusammen [Kin+18].\\nName Dlib\\nOrganisation (Davis E. King)\\nWebseite https://github.com/davisking/dlib\\nLizenz Boost Software License\\nPlattformen Linux, MacOSX, Windows\\nGeschrieben in C++\\nAPIs C++, Python\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 339 | 4.365 | 1.299\\nCommits | Contributors 7.193 | 101\\nIssues: Open | Closed 119 | 819\\nRequests: Open | Closed 6 | 233\\nRelease (Date) 19.9 (23.01.2018)\\nLast Commit 04.03.2018\\nTabelle 6.11: Steckbrief zur Dlib-Bibliothek [Kin+18]\\n636 Software\\n6.9 DSSTNE\\nDeep Scalable Sparse Tensor Network Engine ( DSSTNE ) – gesprochen Destiny – wurde\\nvon Rejith Joseph, Tristan Penman u.a. bei Amazon entwickelt [JP+18]. Im Mittelpunkt\\ndieser Bibliothek steht daher die Big Data Anwendung, personalisierte Produktempfeh-\\nlungen aus großen Datenmengen zu generieren. Hierzu werden DL-Techniken verwendet\\nund tiefe KNN trainiert, wobei die Berechnungen parallel auf GPUs ausgeführt und dabei\\ndie AWS-Dienste EMR und ECS verwendet werden (siehe Abb. 6.7).\\nAbbildung 6.7: Architektur mit DSSTNE [Chu16]\\nStatt einer API werden Skripte bereitgestellt, mit denen sich KNN mittels Neural Network\\nLayer Deﬁnition Language im JSON-Format beschreiben lassen.\\nTabelle 6.12 fasst die wichtigsten Merkmale der DSSTNE-Bibliothek zusammen [JP+18].\\nName DSSTNE\\nOrganisation Amazon\\nWebseite https://github.com/amzn/amazon-dsstne\\nLizenz Apache 2.0\\nPlattformen Linux\\nGeschrieben in C++\\nAPIs -\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 349 | 4.057 | 666\\nCommits | Contributors 30 | 287\\nIssues: Open | Closed 27 | 60\\nRequests: Open | Closed 0 | 73\\nRelease (Date) -\\nLast Commit 01.03.2018\\nTabelle 6.12: Steckbrief zur DSSTNE-Bibliothek [JP+18]\\n646.10 Gluon\\n6.10 Gluon\\nGluon ist ein High-Level Python-API für MXNet (vgl. Kap. 6.13), welches von Mitgliedern\\nder Apache Software Foundation entwickelt und von Steffen Rochel auf GitHub bereitge-\\nstellt wird [Roc+18]. Selbst komplexe Modelle von Künstlichen Neuronalen Netzwerken\\nlassen sich einfach erstellen und trainieren, wobei prinzipiell immer nach dem gleichen\\nSchema vorgegangen wird:\\n1 Netzwerk deﬁnieren\\n2 Parameter initialisieren\\n3 Schleife über die Eingaben\\n4 Propagation, Output berechnen\\n5 Fehler- bzw. Kosten bestimmen\\n6 Backpropagation der Gradienten\\n7 Parameter aktualisieren\\nFür typische Anwendungen gibt es hierzu bereits die passenden Bausteine wie bspw.\\nvordeﬁnierte Layers ,Optimizers ,Initializers usw. Die Dokumentation von Gluon enthält\\nwiederum mehrere APIs: Gluon Neural Network Layers, Gluon Recurrent Neural Net-\\nwork API, Gluon Loss API, Gluon Data API, Gluon Model Zoo und Gluon Contrib API\\n[Apa18b]. Außerdem wurde das Online-Buch mit dem Titel Deep Learning - The Straight\\nDope und vielen Code-Beispielen in Gluon erstellt, welches laufend erweitert und aktuali-\\nsiert wird [Apa18a].\\nTabelle 6.13 fasst die wichtigsten Merkmale der Gluon-Bibliothek zusammen [Roc+18].\\nName Gluon\\nOrganisation Apache Software Foundation\\nWebseite https://github.com/gluon-api/gluon-api\\nLizenz Apache 2.0\\nPlattformen Linux, MacOSX, Windows\\nGeschrieben in Python\\nAPIs Python\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 172 | 2.094 | 211\\nCommits | Contributors 13 | 6\\nIssues: Open | Closed 1 | 10\\nRequests: Open | Closed 1 | 4\\nRelease (Date) -\\nLast Commit 27.02.2018\\nTabelle 6.13: Steckbrief zur Gluon-Bibliothek [Roc+18]\\n656 Software\\n6.11 Keras\\nKeras ist eine High-Level Python-API zum Thema Deep Learning , die vom Google-Ingenieur\\nFrançois Chollet im Rahmen eines Forschungsprojekts entwickelt wurde [Cho+18b]. Als\\nBackend kann dabei bspw. TensorFlow (vgl. Kap. 6.19), CNTK (vgl. Kap. 6.4), MXNet (vgl.\\nKap. 6.13) oder Theano (vgl. Kap. 6.21) eingesetzt werden. Keras bietet Bausteine an, mit\\ndenen sich Modelle einfach erstellen und wiederverwenden lassen:\\nlayers Schichten\\nobjectives Ziel(funktionen)\\nactivation functions Aktivierungsfunktionen\\noptimizers Optimierer\\ntools Werkzeuge\\nEine Besonderheit von Keras ist auch, dass die entwickelten Modelle sich leicht auf einer\\nVielzahl von Plattformen ausführen lassen (vgl. Tab. 6.14).\\nPlattform Voraussetzung / Umsetzung\\niOS Apple CoreML\\nAndroid TensorFlow Android Runtime\\nWebbrowser JavaScript (Keras.js) und WebDNN\\nGoogle Cloud TensorFlow Serving\\nPython WebApp Flask App\\nJVM DL4J (vgl. Kap. 6.7)\\nRaspberry Pi -\\nTabelle 6.14: Übersicht zu Keras-Verwendungsmöglichkeiten\\nTabelle 6.15 fasst die wichtigsten Merkmale der Keras-Bibliothek zusammen [CRZ+18].\\nName Keras\\nOrganisation François Chollet\\nWebseite https://github.com/keras-team/keras\\nLizenz MIT License\\nPlattformen Linux, MacOSX, Windows\\nGeschrieben in Python\\nAPIs Python\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 1.577 | 26.583 | 9.698\\nCommits | Contributors 4.402 | 638\\nIssues: Open | Closed 1.120 | 5.718\\nRequests: Open | Closed 23 | 2.726\\nRelease (Date) 2.1.5 (06.03.2018)\\nLast Commit 08.03.2018\\nTabelle 6.15: Steckbrief zur Keras-Bibliothek [CRZ+18]\\n666.12 Lasagne\\n6.12 Lasagne\\nLasgne ist eine leichtgewichtige DL-Bibliothek bzw. High-Level Python-API, die von Eric\\nBattenberg, Sander Dieleman, Daniel Nouri, Eben Olson, Aäron van den Oord, Colin\\nRaffel, Jan Schlüter und Søren Kaae Sønderby auf Basis des Theano-Frameworks (vgl.\\nKap. 6.21) entwickelt wurde [Bat+18]. Analog zu Keras (vgl. Kap. 6.11) werden dem\\nProgrammierer bereits viele fertige Bausteine zur Modellierung und zum Trainieren der\\nKünstlichen Neuronalen Netzwerke angeboten, wie z.B. Layers ,Regularizers ,Optimizers\\nusw. Die Hauptmerkmale von Lasagne bezüglich der Programmierung sind in Tab. 6.16\\ndargestellt.\\nNr Merkmal Beispiele\\n1 KNN Multi-Input-Multi-Output, MLP\\n2 DL CNN, RNN mit LSTM\\n3 Optimierer Nesterov Momentum, RMSprop, Adam\\n4 Kostenfunktion Automatische symbolische Differentiation\\n5 Prozessor CPU und GPU\\nTabelle 6.16: Programmieren mit der Lasagne-Bibliothek\\nDie Optimierer verwenden häuﬁg Gradientenabstiegsverfahren, um die Fehler- bzw. Kos-\\ntenfunktionen zu minimieren. Dabei müssen die ersten partiellen Ableitungen gebildet\\nwerden. Weil als Backend Theano verwendet wird, können mittels symbolischer Differen-\\ntiation beliebige Kostenfunktionen automatisch partiell abgeleitet werden.\\nTabelle 6.17 fasst die wichtigsten Merkmale der Lasagne-Bibliothek zusammen [Sch+18].\\nName Lasagne\\nOrganisation Open Community\\nWebseite https://github.com/Lasagne/Lasagne\\nLizenz MIT License\\nPlattformen Linux, MacOSX, (Windows)\\nGeschrieben in Python\\nAPIs Python\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 216 | 3.384 | 916\\nCommits | Contributors 1.150 | 64\\nIssues: Open | Closed 113 | 388\\nRequests: Open | Closed 25 | 376\\nRelease (Date) 0.1 (13.08.2015)\\nLast Commit 01.03.2018\\nTabelle 6.17: Steckbrief zur Lasagne-Bibliothek [Sch+18]\\n676 Software\\n6.13 MXNet\\nMXNet ist ein DL-Framework der Apache Software Foundation, das maßgeblich von\\nTianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing\\nXu, Chiyuan Zhang und Zheng Zhang entwickelt wurde [Che+15]. Es bietet sehr viele\\nProgrammierschnittstellen (APIs) an: C++, Python, Julia, Matlab, JavaScript, Go, R, Scala,\\nPerl und Wolfram [Apa18c]. Weitere Merkmale sind in Tab. 6.18 dargestellt, der schich-\\ntenweise, modulare Aufbau ist in Abb. 6.8 skizziert.\\nNr Merkmal Beispiele\\n1 Programmierung Symbolisch & Imperativ\\n2 DL CNN, LSTM\\n3 Prozessor Multi-CPU und Multi-GPU\\n4 Cloud Computing Amazon S3, Apache HDFS, Microsoft Azure\\n5 Portabilität Mobile (Amalgamation), IoT (AWS Greengrass)\\nTabelle 6.18: Programmieren mit dem MXnet-Framework\\nAbbildung 6.8: MXNet: Technologie-Stack [Che+15]\\nTabelle 6.19 fasst die wichtigsten Merkmale der MXNet-Bibliothek zusammen [Xie+18].\\nName MXNet\\nOrganisation Apache Software Foundation\\nWebseite https://github.com/apache/incubator-mxnet\\nLizenz Apache 2.0\\nPlattformen Linux, MacOSX, Windows\\nGeschrieben in C++\\nAPIs C++, Python, Julia, Matlab, JavaScript, Go, R, Scala, Perl\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 1.130 | 13.292 | 4.889\\nCommits | Contributors 6.722 | 490\\nIssues: Open | Closed 762 | 4.755\\nRequests: Open | Closed 72 | 4.412\\nRelease (Date) 1.1.0 (19.02.2018)\\nLast Commit 08.03.2018\\nTabelle 6.19: Steckbrief zur MXNet-Bibliothek [Xie+18]\\n686.14 Neon\\n6.14 Neon\\nDas DL-Framework Neon wurde ursprünglich von Alex Park, Scott Leishman, Anil Tho-\\nmas, Urs Köster und anderen bei Nervana Systems entwickelt [Par+18], 2016 dann aber\\nvom US-amerikanische Halbleiterhersteller Intel übernommen. Die typischen KNN-Archi-\\ntekturen wie MLP ,CNN ,RNN ,LSTM und GRU werden unterstützt, aber auch spezielle\\nFormen wie bspw. Deep Autoencoder ( DAE )können verwendet werden. Ein zentrales Ge-\\nstaltungsmittel sind dabei viele verschiedene Layers -Objekte (vgl. Abb. 6.9), die zu einem\\nKünstlichen Neuronalen Netzwerk angeordnet werden können.\\nAbbildung 6.9: Neon: Layer Taxonomie [Sys18]\\nTabelle 6.20 fasst die wichtigsten Merkmale der Neon-Bibliothek zusammen [Par+18].\\nName Neon\\nOrganisation Intel\\nWebseite https://github.com/NervanaSystems/neon\\nLizenz Apache 2.0\\nPlattformen Linux, MacOSX, Windows\\nGeschrieben in C, CUDA, Python\\nAPIs Python\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 348 | 3.422 | 775\\nCommits | Contributors 1.112 | 78\\nIssues: Open | Closed 64 | 300\\nRequests: Open | Closed 5 | 77\\nRelease (Date) 2.6.0 (05.01.2018)\\nLast Commit 08.02.2018\\nTabelle 6.20: Steckbrief zur Neon-Bibliothek [Par+18]\\n696 Software\\n6.15 OpenNN\\nDie Bibliothek OpenNN für Advanced Analytics wurde von Fernando P . Gomez beim\\nUnternehmen Artelnics in der Programmiersprache C++ entwickelt [Gom+18]. Im Mit-\\ntelpunkt stehen Anwendungen zum Thema Business Intelligence ( BI)wie z.B. Kundenseg-\\nmentierung, Abwanderungsanalyse ( Churn Prevention ) und Vorausschauende Wartung\\n(Predictive Maintenance ) [Art18]. Hierzu werden Künstliche Neuronale Netzwerke einge-\\nsetzt. Es lassen sich zwar MLP konstruieren, aber keine CNN ,RNN oder LSTM . Abb. 6.10\\nzeigt das zentrale Klassendiagramm zu der Software-Bibliothek.\\nAbbildung 6.10: OpenNN: Klassendiagramm [Art18]\\nTabelle 6.21 fasst die wichtigsten Merkmale der OpenNN-Bibliothek zusammen [Gom+18].\\nName OpenNN\\nOrganisation Artelnics\\nWebseite https://github.com/Artelnics/OpenNN\\nLizenz LGPL 3.0\\nPlattformen Linux, MacOSX, Windows\\nGeschrieben in C++\\nAPIs C++\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 96 | 442 | 156\\nCommits | Contributors 70 | 5\\nIssues: Open | Closed 12 | 9\\nRequests: Open | Closed 13 | 7\\nRelease (Date) -\\nLast Commit 20.04.2017\\nTabelle 6.21: Steckbrief zur OpenNN-Bibliothek [Gom+18]\\n706.16 Paddle\\n6.16 Paddle\\nPaddle steht für P Arallel Distributed Deep LEarning – eine DL-Plattform, die von Yu Yang,\\nQiao Longfei, Tao Luo, QI JUN, JiayiFeng und anderen Wissenschaftlern und Ingenieu-\\nren des chinesischen Unternehmens und der gleichnamigen Suchmaschine Baidu in C++\\nentwickelt wurde [Yan+18]. Die Software unterstützt eine Reihe von KNN-Architekturen,\\ndarunter CNN ,RNN ,LSTM und GRU . Außerdem gibt es bereits einige vordeﬁnierte\\nModelle mit zugehörigen Beispielanwendungen, die sich sehr leicht verwenden und an-\\npassen lassen [Bai18]:\\n1Word Embedding Merkmalsextraktion ( Word Vector Learning )\\n2RNN language model Text-Generierung\\n3Click-Through Rate prediction Erfolg von Online-Werbung vorhersagen\\n4Text classiﬁcation Stimmungserkennung ( Sentiment Analysis )\\n5Learning to rank Empfehlungssystem ( Recommender Engine )\\n6Semantic model Suchmaschine\\n7Sequence tagging Eigennamenerkennung ( Named Entity Recognition )\\n8Sequence to sequence learning Übersetzung ( Neural Machine Translation )\\n9Image classiﬁcation Objekterkennung\\nPaddle ist optimiert für mathematische Operationen ( Math Kernel Library ( MKL ),Basic\\nLinear Algebra Subprograms ( BLAS )) und skalierbar (Multi-CPUs/GPUs). Mit der Python-\\nAPI lassen sich DL-Projekte wesentlich einfacher umsetzen.\\nTabelle 6.22 fasst die wichtigsten Merkmale der Paddle-Bibliothek zusammen [Yan+18].\\nName Paddle\\nOrganisation Baidu\\nWebseite https://github.com/PaddlePaddle/Paddle\\nLizenz Apache 2.0\\nPlattformen Linux, MacOSX, Windows\\nGeschrieben in C++, Go\\nAPIs C++, Python\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 551 | 6.519 | 1.721\\nCommits | Contributors 12.380 | 103\\nIssues: Open | Closed 992 | 3.929\\nRequests: Open | Closed 245 | 3.721\\nRelease (Date) 0.11.0 (09.12.2017)\\nLast Commit 08.03.2018\\nTabelle 6.22: Steckbrief zur Paddle-Bibliothek [Yan+18]\\n716 Software\\n6.17 PyTorch\\nPyTorch ist eine High-Level Python-API und DL-Bibliothek auf Basis von Torch (vgl. Kap.\\n6.22), die von Adam Paszke, Soumith Chintala, Gregory Chanan, Sam Gross, Edward\\nZ. Yang, Zachary DeVito, Trevor Killeen und anderen entwickelt wurde [Pas+18]. Diese\\nSoftwarelösung unterstützt Tensor-Rechnungen mit starker GPU-Beschleunigung sowie\\neine spezielle Form des automatischen Differenzierens ( Reverse-Mode Auto-Differentiation ).\\nIm Vergleich zu Torch werden bspw. im CNN die Zwischenzustände nicht in Modulen\\nsondern im Graphen selbst gespeichert (vgl. Abb. 6.11). Dadurch lassen sich Gewichte\\nleichter teilen und Module einfacher wiederverwenden.\\nAbbildung 6.11: Vergleich Torch (links) und PyTorch (rechts) [PyT18]\\nTabelle 6.23 fasst die wichtigsten Merkmale der PyTorch-Bibliothek zusammen [Pas+18].\\nName PyTorch\\nOrganisation Open Community\\nWebseite https://github.com/pytorch/pytorch\\nLizenz BSD License\\nPlattformen Linux, MacOSX, Windows\\nGeschrieben in Python\\nAPIs Python\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 679 | 12.777 | 2.720\\nCommits | Contributors 6.594 | 414\\nIssues: Open | Closed 690 | 2.152\\nRequests: Open | Closed 116 | 2.672\\nRelease (Date) 0.3.1 (13.02.2018)\\nLast Commit 08.03.2018\\nTabelle 6.23: Steckbrief zur PyTorch-Bibliothek [Pas+18]\\n726.18 SINGA\\n6.18 SINGA\\nDie DL-Bibliothek SINGA wurde ursprünglich von der DB System Group an der National\\nUniversity of Singapore in Zusammenarbeit mit der Database Group an der Zhejiang Uni-\\nversity entwickelt [Ooi+15] [Wan+15]. Mittlerweile wird die Software durch die Apache\\nSoftware Foundation ( ASF)und dort maßgeblich durch Wei Wang, Zhongle Xie und Wang\\nSheng weiterentwickelt [WX+18]. Abb. 6.12 zeigt den technologischen Software-Stapel.\\nAbbildung 6.12: Apache SINGA Software Stack [Apa18d]\\nAuf der obersten Ebene stehen dem Programmierer bzw. Anwender die Klassen Layers ,\\nInitializer ,Loss,Metric und Optimizer zur Verfügung.\\nTabelle 6.24 fasst die wichtigsten Merkmale der SINGA-Bibliothek zusammen [WX+18].\\nName SINGA\\nOrganisation Apache Software Foundation (ASF)\\nWebseite https://github.com/apache/incubator-singa\\nLizenz Apache 2.0\\nPlattformen Linux, MacOSX, Windows\\nGeschrieben in C++\\nAPIs C++, Python\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 119 | 1.317 | 309\\nCommits | Contributors 863 | 30\\nIssues: Open | Closed 0 | 0\\nRequests: Open | Closed 18 | 332\\nRelease (Date) 1.1.1 (29.07.2017)\\nLast Commit 22.01.2018\\nTabelle 6.24: Steckbrief zur SINGA-Bibliothek [WX+18]\\n736 Software\\n6.19 TensorFlow\\nDie Software-Bibliothek TensorFlow wurde von Benoit Steiner, Shanqing Cai, Vijay Vasu-\\ndevan, Derek Murray, Gunhan Gulsoy und anderen des Google Brain Teams entwickelt\\nund ist seit dem 09.11.2015 Open Source [Ste+18]. Mathematische Operationen mit Ten-\\nsoren (bzw. Vektoren, Matrizen) werden graphenbasiert formuliert. Jeder Knoten reprä-\\nsentiert dabei eine mathematische Operation und jede Kante ein Tensor. Der Fluss (engl.\\nFlow ) durch den Graphen spiegelt dann die Berechnung wieder [Ten18].\\nDie TensorFlow-Bibliothek ist in mehreren Ebenen aufgebaut (vgl. Abb. 6.13). Mit Hilfe\\nderMid-Level API lassen sich eigene, tiefe Künstliche Neuronale Netzwerke konstruieren\\nund trainieren. Die High-Level API bietet auch bereits vorgefertigte Modelle und typi-\\nsche Anwendungen des Maschinenlernens an. Außerdem ist mit dem TensorBoard ein\\nWerkzeug zum Visualisieren von Daten enthalten.\\nAbbildung 6.13: TensorFlow Technologie-Stapel [Ten18]\\nTabelle 6.25 fasst die wichtigsten Merkmale der TensorFlow-Bibliothek zusammen [Ste+18].\\nName TensorFlow\\nOrganisation Google\\nWebseite https://github.com/tensorﬂow/tensorﬂow\\nLizenz Apache 2.0\\nPlattformen Linux, MacOSX, Windows\\nGeschrieben in C++, CUDA\\nAPIs C++, Python, Java, Go\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 7.555 | 91.783 | 59.230\\nCommits | Contributors 29.717 | 1.357\\nIssues: Open | Closed 1.254 | 9.397\\nRequests: Open | Closed 203 | 6.600\\nRelease (Date) 1.6.0 (02.03.2018)\\nLast Commit 08.03.2018\\nTabelle 6.25: Steckbrief zur TensorFlow-Bibliothek [Ste+18]\\n746.20 TFLearn\\n6.20 TFLearn\\nTFLearn ist eine DL-Bibliothek und High-Level Python-API auf Basis von TensorFlow (vgl.\\nKap. 6.19), die von Aymeric Damien, Will Ballard und anderen Mitstreitern einer Open\\nCommunity entwickelt wurde [DB+18] [Dam+18]. Künstliche Neuronale Netzwerke lassen\\nsich aus einer Vielzahl von Layers konstruieren:\\ncore input_data, fully_connected, dropout, custom_layer, reshape, ﬂatten,\\nactivation, single_unit, highway, one_hot_encoding, time_distributed\\nconv conv_2d, conv_2d_transpose, max_pool_2d, avg_pool_2d, conv_1d,\\nconv_3d, max_pool_3d, highway_conv_1d, global_avg_pool, ...\\nrecurrent simple_rnn, lstm, gru, bidirectionnal_rnn, dynamic_rnn\\nembedding embedding\\nnormalization batch_normalization, local_response_normalization, l2_normalize\\nmerge merge, merge_outputs\\nestimator regression\\nDes Weiteren lassen sich viele Operatoren verwenden:\\nactivations linear, tanh, sigmoid, softmax, softplus, softsign, relu, relu6, prelu, elu\\nobjectives softmax_categorical_crossentropy, mean_square, hinge_loss, ...\\noptimizers SGD, RMSProp, Adam, Momentum, AdaGrad, Ftrl, AdaDelta\\nmetrics Accuracy, Top_k, R2\\ninitializations zeros, uniform, uniform_scaling, normal, truncated_normal, xavier, ...\\nlosses l1, l2\\nTabelle 6.26 fasst die wichtigsten Merkmale der TFLearn-Bibliothek zusammen [DB+18].\\nName TFLearn\\nOrganisation Open Community\\nWebseite https://github.com/tﬂearn/tﬂearn\\nLizenz MIT License\\nPlattformen Linux, MacOSX, Windows\\nGeschrieben in Python\\nAPIs Python\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 440 | 7.705 | 1.818\\nCommits | Contributors 587 | 110\\nIssues: Open | Closed 476 | 325\\nRequests: Open | Closed 16 | 208\\nRelease (Date) 0.3.2 (18.06.2017)\\nLast Commit 10.02.2018\\nTabelle 6.26: Steckbrief zur TFLearn-Bibliothek [DB+18]\\n756 Software\\n6.21 Theano\\nTheano ist eine Python-Bibliothek für numerisches Rechnen, die von Frédéric Bastien, Pas-\\ncal Lamblin, Ian Goodfellow, Aaron Courville, Yoshua Bengio und anderen Wissenschaft-\\nlern am Montreal Institute for Learning Algorithms ( MILA )der Universität von Montréal ent-\\nwickelt wurde [AR+16]. Das Standardwerk zum Thema Deep Learning stammt ebenfalls\\nvon dieser Forschungsgruppe [GBC16]. Theano verwendet Graphen, um mathematische\\nBerechnungen symbolisch darzustellen, wobei der Graph aus Variablen, Operatoren und\\ninternen Apply -Objekten konstruiert wird (vgl. Abb. 6.14).\\nAbbildung 6.14: Theano: Beispiel-Graph mit Apply-Objekt [Ben+17]\\nAm 28.09.2017 hat Pascal Lamblin eine Nachricht vom Leiter der MILA-Forschungsgruppe,\\nYoshua Bengio, verbreitet, dass die Weiterentwicklung von Theano nach der Release-\\nVersion 1.0 aufgrund von starker Konkurrenz seitens der Industrie eingestellt wird [Lam17].\\nDamit wird vermutlich auf die Macht von Google und dem Produkt TensorFlow (vgl. Kap.\\n6.19) angespielt. Die Version 1.0 wurde am 15.11.2017 veröffentlicht. Seitdem wurde nur\\nnoch ein Update 1.0.1 nachgereicht, das Fehlerkorrekturen enthält.\\nTabelle 6.27 fasst die wichtigsten Merkmale der Theano-Bibliothek zusammen [BLG+18].\\nName Theano\\nOrganisation Université de Montréal\\nWebseite https://github.com/Theano/Theano\\nLizenz BSD License\\nPlattformen Linux, MacOSX, Windows\\nGeschrieben in Python\\nAPIs Python\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 571 | 7.976 | 2.418\\nCommits | Contributors 27.942 | 328\\nIssues: Open | Closed 529 | 2.003\\nRequests: Open | Closed 101 | 3.934\\nRelease (Date) 1.0.1 (07.12.2017)\\nLast Commit 07.03.2018\\nTabelle 6.27: Steckbrief zur Theano-Bibliothek [BLG+18]\\n766.22 Torch\\n6.22 Torch\\nTorch ist ein Framework für wissenschaftliches Rechnen und Maschinenlernen, das ur-\\nsprünglich von Ronan Collobert, Koray Kavukcuoglu und Clement Farabet entwickelt\\nwurde und mittlerweile in Version 7 von Soumith Chintala, Keren Zhou, Nicholas Léo-\\nnard und anderen unterstützt wird [Chi+18]. Als API wird Lua verwendet (vgl. Abb. 6.15).\\nDies ist eine Programmiersprache, die häuﬁg in Computerspielen zum Einsatz kommt,\\num Spielcharaktere mittels KI-Algorithmen zu steuern. Mit PyTorch (vgl. Kap. 6.17) gibt\\nes eine Erweiterung, die auch eine Programmierschnittstelle zu Python anbietet.\\nAbbildung 6.15: Torch 7 Technologie-Stapel [Col+18]\\nDas in Torch 7 enthaltene Paket nnkann verwendet werden, um Künstliche Neuronale\\nNetzwerke zu erstellen und zu trainieren. Die Bausteine sind dabei: Module ,Container ,\\nTransfer function ,Simple layer ,Table layer ,Convolution layer und Criterion .\\nTabelle 6.28 fasst die wichtigsten Merkmale der Torch-Bibliothek zusammen [Chi+18].\\nName Torch\\nOrganisation Open Community\\nWebseite https://github.com/torch/torch7\\nLizenz BSD License\\nPlattformen Linux, MacOSX\\nGeschrieben in C, CUDA\\nAPIs Lua\\nStatistik vom 08.03.2018\\nWatch | Star | Fork 677 | 7.728 | 2.250\\nCommits | Contributors 1.335 | 133\\nIssues: Open | Closed 216 | 412\\nRequests: Open | Closed 13 | 494\\nRelease (Date) -\\nLast Commit 26.09.2017\\nTabelle 6.28: Steckbrief zur Torch-Bibliothek [Chi+18]\\n777 Anwendungen\\nIn Kapitel 6 wurden 22 Softwarelösungen zum Thema Deep Learning vorgestellt. Aufgrund\\nder Statistik der Plattform GitHub nimmt die Lösung TensorFlow (vgl. Kap. 6.19) eine\\ndominante Stellung ein. Einige Softwareentwickler haben aber bereits auch Schwächen\\nbzw. Lücken von TensorFlow identiﬁziert und versuchen diese nun zu schließen, indem\\nsie eine spezielle High-Level API auf Basis von TensorFlow anbieten. Zu diesen Lösungen\\ngehören Deep Water (vgl. Kap. 6.6), Keras (vgl. Kap. 6.11) und TFLearn (vgl. Kap. 6.20).\\nVon diesen drei Lösungen ist Keras wiederum die bekannteste bzw. populärste. Eine\\ngewisse Nähe dieser beiden Software-Lösungen ist unverkennbar, denn Keras wurde\\nvom Google-Ingenieur François Chollet entwickelt.\\nUm in diesem Kapitel typische Anwendungen im Bereich Deep Learning durchzuführen,\\nwerden somit die Software-Bibliotheken TensorFlow und Keras verwendet. Das Ziel ist\\nnicht, neue Rekorde hinsichtlich der Genauigkeit und Güte der trainierten Modelle auf\\nden Testdaten zu erzielen. Dies wäre auch gar nicht möglich, weil hierzu ein hoher Auf-\\nwand in der Modellbildung und eine enorme Rechenleistung des Systems nötig wäre.\\nStattdessen wird der Fokus auf die Verwendbarkeit der High-Level Bibliothek Keras ge-\\nrichtet. Die Frage ist also: Ist es möglich, mit nur wenigen Programmzeilen in Python,\\nein Künstliches Neuronales Netzwerk zu konﬁgurieren und zu trainieren, sodass die\\nerzielten Ergebnisse, gemessen auf den Testdaten, akzeptabel sind.\\nHierzu werden drei typische Problemstellungen betrachtet, die mit Hilfe von mindestens\\ndrei unterschiedlichen Typen von KNN mit Hilfe der DL-Bibliothek Keras bearbeitet\\nwerden sollen.\\n1 MNIST Bilderkennung Klassiﬁkation MLP\\n2 CIFAR-10 Bilderkennung Klassiﬁkation CNN\\n3 IMDb Textanalyse Klassiﬁkation LSTM\\nAlle drei Anwendungsbeispiele fallen somit in die Data Mining Kategorie Klassiﬁkation.\\nWährend in den Bildern Objekte erkannt und unterschieden werden sollen, geht es bei der\\nTextanalyse darum, Stimmungen zu erkennen und diese zu klassiﬁzieren. Das Ergebnis\\neines Klassiﬁzierers, also eines trainierten Modells, welches eine Klassiﬁkation vornimmt,\\nkann bspw. bewertet werden, indem die Genauigkeit auf einer Testdatenmenge bestimmt\\nwird. Diese Testdatenmenge ist disjunkt zur Trainingsdatenmenge, d.h. in der Testdaten-\\nmenge sind andere Datensätze enthalten als in der Trainingsdatenmenge. Diese Testdaten\\nsind also dem Modell, bspw. also dem Künstlichen Neuronalen Netzwerk, nicht bekannt.\\nSomit kann dann ein aussagekräftiges Gütekriterium zur Bewertung bzw. Evaluation des\\nModells auf dieser Testdatenmenge bestimmt werden. Im Fall der Klassiﬁkation ist dies\\ndie Genauigkeit (engl. Accuracy ) in Prozent. Sie ist deﬁniert als Quotient aus der Anzahl\\nder korrekten Klassiﬁkationen zu der Anzahl der Datensätze in der Testdatenmenge. Je\\nnäher dieser Wert an 100 % liegt, desto besser ist das Modell. Je nach Anwendungsbereich\\nkann eine Genauigkeit von 80 % bereits eine sehr gute Leistung sein. Die Wahrheitsmatrix\\nbzw. Konfusionsmatrix (engl. Confusion Matrix ) kann zusätzlich benutzt werden, um die\\nKlassiﬁkationsergebnisse detaillierter zu untersuchen.\\n797 Anwendungen\\n7.1 MNIST\\nDie MNIST -Datenbank ( Modiﬁed National Institute of Standards and Technology ) enthält\\n70.000 Datensätze von Bildern zu handgeschriebenen Ziffern Null bis Neun. Die Daten-\\nsätze wurden von der ursprünglichen Datenbank des National Institute of Standards and\\nTechnology ( NIST )neu zusammengestellt, weil die Trainings- und Testdaten aus unter-\\nschiedlichen Quellen stammten: Die Trainingsdaten kamen von Angestellten des Ame-\\nrican Census Bureau , während die Testdaten durch amerikanische Highschool -Studenten\\nerstellt wurden. Außerdem wurden die ursprünglich schwarz-weißen Bilder nun in ein\\neinheitliches Format von 28 x 28 Pixeln in Graustufen mit Kantenglättung (Anti-Aliasing)\\ngebracht [LCB18]. 60.000 Bilder sind zum Trainieren vorgesehen, während 10.000 Bilder\\nzum Testen, d.h. Evaluieren des Modells, benutzt werden können. Abbildung 7.1 zeigt\\neinige Beispielbilder der zehn Ziffern aus der Testmenge der MNIST-Datenbank.\\nAbbildung 7.1: Beispielbilder des MNIST Test-Datensatzes [Ste17]\\nDie MNIST-Datenbank ist sehr populär und wird zum Trainieren von Bilderkennungssys-\\ntemen eingesetzt [Wik18h]. Die Aufgabe fällt in die Data Mining Kategorie Klassiﬁkation.\\nDenn das trainierte Modell soll die Bilder zu einer der zehn Klassen (Ziffern Null bis\\nNeun) möglichst korrekt zuordnen. Somit sind die Genauigkeit bzw. die Fehlerrate die\\nGütekriterien zu den Klassiﬁzierern. Die Anwendung gehört auch zum Gebiet Optical\\nCharacter Recognition ( OCR ). Als Algorithmen können bspw. klassische Methoden wie Nai-\\nve Bayes ,K Nearest Neighbor , Entscheidungsbäume, Support Vector Machine eingesetzt oder\\nmoderne Techniken des Maschinenlernens wie bspw. Künstliche Neuronale Netzwerke\\nangewendet werden.\\n1998 haben Yann LeCun, Léon Bottou, Yoshua Bengio und Patrick Haffner in ihrer wis-\\nsenschftlichen Studie [LeC+98] u.a. die Ergebnisse von 27 trainierten Klassiﬁzierern (vgl.\\nAbb. 7.2) zur MNIST-Datenbank zusammengestellt. Sehr einfache lineare Klassiﬁzierer\\nkommen dabei auf Fehlerraten im Bereich von 10 %, während der beste Klassiﬁzierer\\nbereits eine Fehlerrate von nur 0,7 % erreicht, also die Ziffern der Testmenge mit einer\\nGenauigkeit von 99,3 % korrekt klassiﬁziert. Dieser beste Klassiﬁzierer basiert auf einem\\nEnsemble von drei Netzwerken vom Typ Convolutional Neural Network . Jedes dieser drei\\nCNNs ist folgendermaßen aufgebaut: Die 784 Eingaben (28 x 28 Pixel) werden auf 4 erste\\nFeature Maps und 8 Subsampling Maps abgebildet. Danach folgen 16 Feature Maps und 16\\n807.1 MNIST\\nSubsampling Maps sowie eine vollständig vernetzte Schicht mit 120 Neuronen und schließ-\\nlich die Ausgabeschicht mit 10 Neuronen für die 10 Klassen. Insgesamt besteht ein solches\\nCNN aus ca. 260.000 Verbindungen und es hat etwa 17.000 freie Parameter.\\nAbbildung 7.2: Fehlerraten für verschiedene Klassiﬁzierer zur MNIST Datenbank [LeC+98]\\nIn den folgenden Jahren konnte die Genauigkeit noch weiter verbessert werden. 2003\\nerreichten Patrice Y. Simard, Dave Steinkraus und John C. Platt ebenfalls die Fehlerrate\\nvon 0,7 %, wobei diesmal allerdings ein einfaches Multilayer Perceptron mit nur einer ver-\\nborgenen Schicht von 800 Neuronen, also dem topologischen Aufbau 784-800-10, erzielt\\nwerden konnte [SSP03]. Der Trick bestand u.a. darin, die Anzahl der Trainingsbeispiele\\nzu erhöhen, indem aus den bestehenden Bildern mit Hilfe von elastischen Verzerrungen\\n(engl. Elastic Distortions ) weitere Beispiele generiert wurden. 2010 haben Dan Claudiu\\nCiresan, Ueli Meier, Luca Maria Gambardella und Jürgen Schmidhuber mit einem tiefen\\nKNN der Topologie 784-40-80-500-1000-2000-10 eine Fehlerrate von 0,35 % erzielt [Cir+10].\\nDie gleiche Forschungsgruppe hat 2012 sogar eine Fehlerrate von 0,23 % erreicht [CMS12].\\nDiesmal allerdings mit einem Ensemble bzw. Komitee von 35 CNNs der Topologie 1-20-\\nP-40-P-150-10.\\nDieData Science Plattform Kaggle hat die MNIST-Klassiﬁkationsaufgabe inzwischen als\\nWettbewerb in sein Portfolio aufgenommen und beschreibt sie als ”Hallo Welt”-Aufgabe\\nim Bereich des maschinellen Sehens in Anspielung auf ein ”Hallo Welt”-Computerpro-\\ngramm als Standard-Beispiel zu einer Programmiersprache [Kag18b].\\nMit Hilfe der DL-Bibliothek Keras auf Basis des Backends TensorFlow wurde ein einfaches\\nMLP mit der Topologie 784-256-128-10 trainiert. Der Python-Quelltext hierzu ist in dem\\nfolgenden Listing gegeben und inline kommentiert.\\n817 Anwendungen\\n# Importiere Keras-Bibliotheken und -Funktionen\\ni m p o r t k e r a s\\nf r o m k e r a s . d a t a s e t s i m p o r t m n i s t\\nf r o m k e r a s . m o d e l s i m p o r t S e q u e n t i a l\\nf r o m k e r a s . l a y e r s i m p o r t D e n s e\\nf r o m k e r a s . o p t i m i z e r s i m p o r t R M S p r o p\\nf r o m k e r a s . c a l l b a c k s i m p o r t E a r l y S t o p p i n g\\n# Importiere sonstige Bibliotheken und Funktionen\\ni m p o r t m a t p l o t l i b . p y p l o t a s p l t\\ni m p o r t t i m e\\n# Trainingsparameter\\nb a t c h _ s i z e = 1 2 8\\nn u m _ c l a s s e s = 1 0\\ne p o c h s = 1 0 0\\n# Bild-Dimensionen\\ni m g _ r o w s , i m g _ c o l s = 2 8 , 2 8\\ni n p u t _ s i z e = i m g _ r o w s * i m g _ c o l s\\n# Partitionierung: Training vs Test\\n( x _ t r a i n , y _ t r a i n ) , ( x _ t e s t , y _ t e s t ) = m n i s t . l o a d _ d a t a ( )\\n# Datenkonvertierung 1: X-Werte\\nx _ t r a i n = x _ t r a i n . r e s h a p e ( x _ t r a i n . s h a p e [ 0 ] , i n p u t _ s i z e )\\nx _ t e s t = x _ t e s t . r e s h a p e ( x _ t e s t . s h a p e [ 0 ] , i n p u t _ s i z e )\\nx _ t r a i n = x _ t r a i n . a s t y p e ( \\' f l o a t 3 2 \\' )\\nx _ t e s t = x _ t e s t . a s t y p e ( \\' f l o a t 3 2 \\' )\\nx _ t r a i n / = 2 5 5\\nx _ t e s t / = 2 5 5\\n# Datenkonvertierung 2: Y-Werte (Klassenvektoren => Binäre Klassenmatrizen)\\ny _ t r a i n = k e r a s . u t i l s . t o _ c a t e g o r i c a l ( y _ t r a i n , n u m _ c l a s s e s )\\ny _ t e s t = k e r a s . u t i l s . t o _ c a t e g o r i c a l ( y _ t e s t , n u m _ c l a s s e s )\\n# Modell\\nm o d e l = S e q u e n t i a l ( )\\nm o d e l . a d d ( D e n s e ( 2 5 6 , a c t i v a t i o n = \\' r e l u \\' , i n p u t _ s h a p e = ( i n p u t _ s i z e , ) ) )\\nm o d e l . a d d ( D e n s e ( 1 2 8 , a c t i v a t i o n = \\' r e l u \\' ) )\\nm o d e l . a d d ( D e n s e ( n u m _ c l a s s e s , a c t i v a t i o n = \\' s o f t m a x \\' ) )\\nm o d e l . s u m m a r y ( )\\nm o d e l . c o m p i l e ( l o s s = \\' c a t e g o r i c a l _ c r o s s e n t r o p y \\' , o p t i m i z e r = R M S p r o p ( ) ,\\nm e t r i c s = [ \\' a c c u r a c y \\' ] )\\n# Training\\ns t a r t _ t i m e = t i m e . t i m e ( )\\nh i s t o r y = m o d e l . f i t ( x _ t r a i n , y _ t r a i n , b a t c h _ s i z e = b a t c h _ s i z e , e p o c h s =\\ne p o c h s , v e r b o s e = 2 , v a l i d a t i o n _ d a t a = ( x _ t e s t , y _ t e s t ) , c a l l b a c k s = [\\nE a r l y S t o p p i n g ( m i n _ d e l t a = 0 . 0 0 0 0 1 , p a t i e n c e = 1 0 ) ] )\\ne n d _ t i m e = t i m e . t i m e ( )\\np r i n t ( \" Z e i t d a u e r [ s ] : { } \" . f o r m a t ( ( e n d _ t i m e - s t a r t _ t i m e ) ) )\\n# Test / Validierung\\nl o s s , a c c = m o d e l . e v a l u a t e ( x _ t e s t , y _ t e s t , v e r b o s e = 0 )\\np r i n t ( \" L o s s : { } \" . f o r m a t ( l o s s ) )\\np r i n t ( \" G e n a u i g k e i t [ % ] : { } \" . f o r m a t ( a c c * 1 0 0 ) )\\n# Visualisierung: Genauigkeit\\np l t . p l o t ( h i s t o r y . h i s t o r y [ \\' a c c \\' ] )\\np l t . p l o t ( h i s t o r y . h i s t o r y [ \\' v a l _ a c c \\' ] )\\np l t . t i t l e ( \\' M o d e l l - G e n a u i g k e i t \\' )\\n827.1 MNIST\\np l t . y l a b e l ( \\' G e n a u i g k e i t \\' )\\np l t . x l a b e l ( \\' E p o c h e \\' )\\np l t . l e g e n d ( [ \\' T r a i n i n g \\' , \\' T e s t \\' ] , l o c = \\' l o w e r r i g h t \\' )\\np l t . s h o w ( )\\n# Visualisierung: Fehler (Loss-Funktion)\\np l t . p l o t ( h i s t o r y . h i s t o r y [ \\' l o s s \\' ] )\\np l t . p l o t ( h i s t o r y . h i s t o r y [ \\' v a l _ l o s s \\' ] )\\np l t . t i t l e ( \\' M o d e l l - F e h l e r ( L o s s - F u n k t i o n ) \\' )\\np l t . y l a b e l ( \\' F e h l e r b z w . L o s s \\' )\\np l t . x l a b e l ( \\' E p o c h e \\' )\\np l t . l e g e n d ( [ \\' T r a i n i n g \\' , \\' T e s t \\' ] , l o c = \\' u p p e r r i g h t \\' )\\np l t . s h o w ( )\\x05\\nListing 7.1: MNIST MLP\\nDer Quelltext in ist mehrere Abschnitte gegliedert und durch Überschriften kommen-\\ntiert. Im Modell wird als Aktivierungsfunktion der Neuronen der Eingabeschicht und\\nder verborgenen Schicht Rectiﬁed Linear Unit ( ReLU )eingesetzt. In der Ausgabeschicht\\nwird dagegen die Softmax -Funktion verwendet. Als Gradientenabstiegsverfahren wird\\nder Algorithmus RMSprop benutzt und als Fehlerfunktion bzw. Loss-Funktion Categorical\\nCrossentropy . Die Anzahl der freien Parameter dieses vollvernetzten Netzwerks setzt sich\\naus den Gewichten der Verbindungen und den Bias-Werten der Neuronen zusammen\\nund beträgt:\\n(784+1)\\x01512+ (512+1)\\x01128+ (128+1)\\x0110=235.146\\nDas Training wird durch Early Stopping abgebrochen, wenn sich in 10 Epochen keine\\nVerbesserung gemäß des berechneten Fehlers ( Loss) auf der Testmenge einstellt. Die Aus-\\ngaben auf der Konsole bei Ausführung des obigen Python-Programms sehen wie folgt\\naus.\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\ndense_1 (Dense) (None, 256) 200960\\n_________________________________________________________________\\ndense_2 (Dense) (None, 128) 32896\\n_________________________________________________________________\\ndense_3 (Dense) (None, 10) 1290\\n=================================================================\\nTotal params: 235,146\\nTrainable params: 235,146\\nNon-trainable params: 0\\n_________________________________________________________________\\nTrain on 60000 samples, validate on 10000 samples\\nEpoch 1/100\\n- 1s - loss: 0.2633 - acc: 0.9211 - val_loss: 0.1137 - val_acc: 0.9642\\nEpoch 2/100\\n- 1s - loss: 0.1025 - acc: 0.9686 - val_loss: 0.0785 - val_acc: 0.9749\\nEpoch 3/100\\n- 1s - loss: 0.0680 - acc: 0.9792 - val_loss: 0.0858 - val_acc: 0.9745\\nEpoch 4/100\\n- 1s - loss: 0.0496 - acc: 0.9843 - val_loss: 0.0724 - val_acc: 0.9781\\nEpoch 5/100\\n- 1s - loss: 0.0370 - acc: 0.9887 - val_loss: 0.0896 - val_acc: 0.9739\\nEpoch 6/100\\n- 1s - loss: 0.0287 - acc: 0.9906 - val_loss: 0.0684 - val_acc: 0.9806\\n837 Anwendungen\\nEpoch 7/100\\n- 1s - loss: 0.0219 - acc: 0.9931 - val_loss: 0.0870 - val_acc: 0.9766\\nEpoch 8/100\\n- 1s - loss: 0.0177 - acc: 0.9943 - val_loss: 0.0785 - val_acc: 0.9805\\nEpoch 9/100\\n- 1s - loss: 0.0142 - acc: 0.9957 - val_loss: 0.0856 - val_acc: 0.9806\\nEpoch 10/100\\n- 1s - loss: 0.0117 - acc: 0.9962 - val_loss: 0.0850 - val_acc: 0.9816\\nEpoch 11/100\\n- 1s - loss: 0.0098 - acc: 0.9970 - val_loss: 0.0885 - val_acc: 0.9801\\nEpoch 12/100\\n- 1s - loss: 0.0083 - acc: 0.9976 - val_loss: 0.0920 - val_acc: 0.9816\\nEpoch 13/100\\n- 1s - loss: 0.0061 - acc: 0.9980 - val_loss: 0.0995 - val_acc: 0.9811\\nEpoch 14/100\\n- 1s - loss: 0.0069 - acc: 0.9979 - val_loss: 0.1021 - val_acc: 0.9803\\nEpoch 15/100\\n- 1s - loss: 0.0057 - acc: 0.9983 - val_loss: 0.1068 - val_acc: 0.9807\\nEpoch 16/100\\n- 1s - loss: 0.0046 - acc: 0.9985 - val_loss: 0.1177 - val_acc: 0.9803\\nZeitdauer[s]: 11.67345929145813\\nLoss: 0.11772279915162692\\nGenauigkeit[%]: 98.03\\nFür das Training der 16 Epochen dieses Netzwerks werden nur knapp 12 Sekunden auf\\nder Graﬁkkarte Nvidia GTX 1080 Ti benötigt. Die Genauigkeit, gemessen auf den Testda-\\nten, beträgt dabei bereits 98 %, d.h. die Fehlerrate ist somit 2 %. Abb. 7.3 zeigt die zeitlichen\\nVerläufe der Genauigkeiten des Klassiﬁzierers auf den Trainings- und Testdaten.\\nAbbildung 7.3: Genauigkeit des MLP für die Klassiﬁkation der MNIST-Datenbank\\nIn Abb. 7.4 sind die zeitlichen Verläufe der Netzwerkfehler, d.h. der berechnete Loss, auf\\nden Trainings- und Testdaten dargestellt.\\n847.1 MNIST\\nAbbildung 7.4: Fehler bzw. Loss des MLP für die Klassiﬁkation der MNIST-Datenbank\\nEine längere Trainingsdauer, gemessen an der Anzahl der Epochen, führt mit dieser Netz-\\ntopologie nicht unbedingt zu einer Verbesserung der Genauigkeit der Klassiﬁkation auf\\nden Testdaten. Trotzdem ist dies ein gutes Ergebnis, wenn man bedenkt, dass es sich bei\\ndem Künstlichen Neuronalen Netzwerk um ein kleines MLP handelt.\\nMit der Verwendung eines Convolutional Neural Network kann dieses Ergebnis noch ver-\\nbessert werden. Der Aufbau des hierzu verwendeten CNN ist in Tab. 7.1 dargestellt.\\nSchicht Typ Maps Größe Kernel Stride Aktiv.\\nIn Input 1 28 x 28 - - -\\nC1 Convolution 24 28 x 28 5 x 5 1 ReLU\\nS2 Max Pooling 24 14 x 14 2 x 2 2 -\\nC3 Convolution 48 14 x 14 3 x 3 1 ReLU\\nS4 Max Pooling 48 7 x 7 2 x 2 2 -\\nF5 Fully Connected - 2.352 - - ReLU\\nF6 Fully Connected - 256 - - ReLU\\nOut Fully Connected - 10 - - Softmax\\nTabelle 7.1: Topologie des CNN für die MNIST-Datenbank\\nDas folgende Listing zeigt nur die Quelltextblöcke, die in dem ursprünglichen Listing\\ngeändert werden müssen.\\n# Importiere Keras-Bibliotheken und -Funktionen\\nf r o m k e r a s . l a y e r s i m p o r t D e n s e , D r o p o u t , F l a t t e n , C o n v 2 D , M a x P o o l i n g 2 D\\n# Bild-Dimensionen\\ni m g _ r o w s , i m g _ c o l s = 2 8 , 2 8\\ni n p u t _ s h a p e = ( i m g _ r o w s , i m g _ c o l s , 1 )\\n857 Anwendungen\\n# Datenkonvertierung 1: X-Werte\\nx _ t r a i n = x _ t r a i n . r e s h a p e ( x _ t r a i n . s h a p e [ 0 ] , i m g _ r o w s , i m g _ c o l s , 1 )\\nx _ t e s t = x _ t e s t . r e s h a p e ( x _ t e s t . s h a p e [ 0 ] , i m g _ r o w s , i m g _ c o l s , 1 )\\n# Modell\\nm o d e l = S e q u e n t i a l ( )\\nm o d e l . a d d ( C o n v 2 D ( 2 4 , k e r n e l _ s i z e = ( 5 , 5 ) , s t r i d e s = ( 1 , 1 ) , p a d d i n g = \\' s a m e \\' ,\\na c t i v a t i o n = \\' r e l u \\' , i n p u t _ s h a p e = i n p u t _ s h a p e ) )\\nm o d e l . a d d ( M a x P o o l i n g 2 D ( p o o l _ s i z e = ( 2 , 2 ) , s t r i d e s = ( 2 , 2 ) ) )\\nm o d e l . a d d ( C o n v 2 D ( 4 8 , k e r n e l _ s i z e = ( 3 , 3 ) , s t r i d e s = ( 1 , 1 ) , p a d d i n g = \\' s a m e \\' ,\\na c t i v a t i o n = \\' r e l u \\' ) )\\nm o d e l . a d d ( M a x P o o l i n g 2 D ( p o o l _ s i z e = ( 2 , 2 ) , s t r i d e s = ( 2 , 2 ) ) )\\nm o d e l . a d d ( F l a t t e n ( ) )\\nm o d e l . a d d ( D e n s e ( 2 5 6 , a c t i v a t i o n = \\' r e l u \\' ) )\\nm o d e l . a d d ( D r o p o u t ( 0 . 5 ) )\\nm o d e l . a d d ( D e n s e ( n u m _ c l a s s e s , a c t i v a t i o n = \\' s o f t m a x \\' ) )\\nm o d e l . s u m m a r y ( )\\nm o d e l . c o m p i l e ( l o s s = \\' c a t e g o r i c a l _ c r o s s e n t r o p y \\' , o p t i m i z e r = R M S p r o p ( ) ,\\nm e t r i c s = [ \\' a c c u r a c y \\' ] )\\x05\\nListing 7.2: MNIST CNN\\nDie Konsolen-Ausgaben sehen dann wie folgt aus:\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\nconv2d_1 (Conv2D) (None, 28, 28, 24) 624\\n_________________________________________________________________\\nmax_pooling2d_1 (MaxPooling2 (None, 14, 14, 24) 0\\n_________________________________________________________________\\nconv2d_2 (Conv2D) (None, 14, 14, 48) 10416\\n_________________________________________________________________\\nmax_pooling2d_2 (MaxPooling2 (None, 7, 7, 48) 0\\n_________________________________________________________________\\nflatten_1 (Flatten) (None, 2352) 0\\n_________________________________________________________________\\ndense_1 (Dense) (None, 256) 602368\\n_________________________________________________________________\\ndropout_1 (Dropout) (None, 256) 0\\n_________________________________________________________________\\ndense_2 (Dense) (None, 10) 2570\\n=================================================================\\nTotal params: 615,978\\nTrainable params: 615,978\\nNon-trainable params: 0\\n_________________________________________________________________\\nTrain on 60000 samples, validate on 10000 samples\\nEpoch 1/100\\n- 3s - loss: 0.2092 - acc: 0.9356 - val_loss: 0.0500 - val_acc: 0.9855\\nEpoch 2/100\\n- 2s - loss: 0.0652 - acc: 0.9803 - val_loss: 0.0280 - val_acc: 0.9905\\n...\\nEpoch 19/100\\n- 2s - loss: 0.0168 - acc: 0.9952 - val_loss: 0.0291 - val_acc: 0.9928\\nEpoch 20/100\\n- 2s - loss: 0.0176 - acc: 0.9949 - val_loss: 0.0269 - val_acc: 0.9935\\nZeitdauer[s]: 40.922513246536255\\nLoss: 0.02688839722765547\\nGenauigkeit[%]: 99.35000000000001\\n867.1 MNIST\\nDiesmal müssen also schon 615.978 Parameter trainiert werden. Für das Training von\\n20 Epochen des CNN werden nun knapp 41 Sekunden auf der Graﬁkkarte Nvidia GTX\\n1080 Ti benötigt. Die Genauigkeit, gemessen auf den Testdaten, beträgt dabei nun 99,35 %,\\nd.h. die Fehlerrate liegt somit nur bei 0.65 %. Das ist bereits ein hervorragendes Ergebnis,\\ngemessen an dem Programmier- und Trainingsaufwand. Abb. 7.5 zeigt die zeitlichen\\nVerläufe der Genauigkeiten des Klassiﬁzierers auf den Trainings- und Testdaten. Die\\nzeitlichen Verläufe der Netzwerkfehler ( Loss) auf den Trainings- und Testdaten sind in\\nAbb. 7.6 dargestellt.\\nAbbildung 7.5: Genauigkeit des CNN für die Klassiﬁkation der MNIST-Datenbank\\nAbbildung 7.6: Fehler bzw. Loss des CNN für die Klassiﬁkation der MNIST-Datenbank\\n877 Anwendungen\\n7.2 CIFAR-10\\nDie CIFAR-10-Datenbank des Canadian Institute For Advanced Research ist eine Samm-\\nlung von 60.000 Farb-Bildern im Format 32 x 32 Pixeln, die von Alex Krizhevsky, Vi-\\nnod Nair und Geoffrey E. Hinton zusammengestellt wurde [Kri09]. Die Bilder zeigen 10\\nunterschiedliche Klassen von Objekten: Flugzeuge, Autos, Vögel, Katzen, Rehe, Hunde,\\nFrösche, Pferde, Schiffe und LKWs . Die Datensätze zu den Klassen sind gleichverteilt,\\nd.h. zu jeder Klasse gibt es 6.000 Bilder. Außerdem ist in jedem Bild nur ein Objekt einer\\ndieser Klassen zu erkennen. Abb. 7.7 zeigt zu diesen 10 Klassen einige Beispielbilder.\\nAbbildung 7.7: Beispielbilder der CIFER-10 Datenbank [Kri18]\\nÄhnlich wie die MNIST-Datenbank (vgl. Kap. 7.1) kann die CIFAR-10-Datenbank im\\nBereich maschinelles Sehen, insbesondere der Objekterkennung, eingesetzt werden, um\\nneue Modelle und Algorithmen zu entwickeln, zu testen und miteinander zu vergleichen\\n[Wik18b]. Auch die Data Science Plattform Kaggle bietet zu dieser Datenbank ebenfalls\\neinen Wettbewerb an [Kag18a]. Die Aufgabe ist aber schon wesentlich schwieriger im\\nVergleich zum MNIST-Datensatz, in dem die zehn Ziffern als Klassen erkannt werden\\nmussten. Mit einem einfachen Multilayer Perceptron lässt sich diese Aufgabe nicht adäquat\\nlösen. Hierzu muss schon ein Convolutional Neural Network eingesetzt werden. 2010 betrug\\ndie Fehlerrate noch 21,1 %, was einer Genauigkeit von 78,9 % entspricht, wobei bereits\\neine spezielle Art eines CNN , dem Deep Belief Network ( DBN ), eingesetzt wurde [Kri10].\\n2018 konnte die Fehlerrate sogar auf 2,13 % reduziert werden [Rea+18]. Dabei mussten\\nallerdings auch 34,9 Millionen freie Parameter des sogenannten AmoebaNet-B (6, 128)\\ntrainiert werden.\\n887.2 CIFAR-10\\nAbb. 7.8 zeigt schematisch ein typisches Convolutional Neural Network mit seinen verschie-\\ndenen Arten von Schichten zur Verarbeitung der Bilddaten der CIFAR-10-Datenbank und\\nzur anschließenden Klassiﬁkation der 10 Objekte.\\nAbbildung 7.8: CNN verarbeitet Bilder der CIFER-10 Datenbank [Mat18]\\nTab. 7.2 zeigt die Parameter des Aufbaus des verwendeten Convolutional Neural Network .\\nSchicht Typ Maps Größe Kernel Stride Padding Aktiv.\\nIn Input 1 32 x 32 - - - -\\nC1 Convolution 32 30 x 30 3 x 3 1 valid ReLU\\nC2 Convolution 32 30 x 30 3 x 3 1 same ReLU\\nP3 Max Pooling 32 15 x 15 2 x 2 2 - -\\nC4 Convolution 64 13 x 13 3 x 3 1 valid ReLU\\nC5 Convolution 64 13 x 13 3 x 3 1 same ReLU\\nP6 Max Pooling 64 6 x 6 2 x 2 2 - -\\nC7 Convolution 128 4 x 4 3 x 3 1 valid ReLU\\nC8 Convolution 128 4 x 4 3 x 3 1 same ReLU\\nP9 Max Pooling 126 2 x 2 2 x 2 2 - -\\nF10 Fully Connected - 512 - - - ReLU\\nF11 Fully Connected - 1.024 - - - ReLU\\nOut Fully Connected - 10 - - - Softmax\\nTabelle 7.2: Topologie des CNN für die CIFAR-10-Datenbank\\nDas folgende Python-Programm verwendet wieder die Kombination aus Keras und Ten-\\nsorFlow, um dieses CNN auf die CIFAR-10-Datenbank anzusetzen.\\n# Importiere Keras-Bibliotheken und -Funktionen\\ni m p o r t k e r a s\\nf r o m k e r a s . d a t a s e t s i m p o r t c i f a r 1 0\\nf r o m k e r a s . m o d e l s i m p o r t S e q u e n t i a l\\nf r o m k e r a s . l a y e r s i m p o r t D e n s e , D r o p o u t , F l a t t e n , C o n v 2 D , M a x P o o l i n g 2 D\\nf r o m k e r a s . o p t i m i z e r s i m p o r t A d a m\\nf r o m k e r a s . c a l l b a c k s i m p o r t E a r l y S t o p p i n g\\n897 Anwendungen\\n# Importiere sonstige Bibliotheken und Funktionen\\ni m p o r t m a t p l o t l i b . p y p l o t a s p l t\\ni m p o r t t i m e\\n# Trainingsparameter\\nb a t c h _ s i z e = 3 2\\nn u m _ c l a s s e s = 1 0\\ne p o c h s = 1 0 0\\n# Partitionierung: Training vs Test\\n( x _ t r a i n , y _ t r a i n ) , ( x _ t e s t , y _ t e s t ) = c i f a r 1 0 . l o a d _ d a t a ( )\\n# Datenkonvertierung 1: X-Werte\\nx _ t r a i n = x _ t r a i n . a s t y p e ( \\' f l o a t 3 2 \\' )\\nx _ t e s t = x _ t e s t . a s t y p e ( \\' f l o a t 3 2 \\' )\\nx _ t r a i n / = 2 5 5\\nx _ t e s t / = 2 5 5\\n# Datenkonvertierung 2: Y-Werte (Klassenvektoren => Binäre Klassenmatrizen)\\ny _ t r a i n = k e r a s . u t i l s . t o _ c a t e g o r i c a l ( y _ t r a i n , n u m _ c l a s s e s )\\ny _ t e s t = k e r a s . u t i l s . t o _ c a t e g o r i c a l ( y _ t e s t , n u m _ c l a s s e s )\\n# Modell\\nm o d e l = S e q u e n t i a l ( )\\n# In, C1, C2, P3\\nm o d e l . a d d ( C o n v 2 D ( 3 2 , k e r n e l _ s i z e = ( 3 , 3 ) , s t r i d e s = ( 1 , 1 ) , p a d d i n g = \\' v a l i d \\' ,\\na c t i v a t i o n = \\' r e l u \\' , i n p u t _ s h a p e = x _ t r a i n . s h a p e [ 1 : ] ) )\\nm o d e l . a d d ( C o n v 2 D ( 3 2 , k e r n e l _ s i z e = ( 3 , 3 ) , s t r i d e s = ( 1 , 1 ) , p a d d i n g = \\' s a m e \\' ,\\na c t i v a t i o n = \\' r e l u \\' ) )\\nm o d e l . a d d ( M a x P o o l i n g 2 D ( p o o l _ s i z e = ( 2 , 2 ) , s t r i d e s = ( 2 , 2 ) ) )\\nm o d e l . a d d ( D r o p o u t ( 0 . 2 ) )\\n# C4, C5, P6\\nm o d e l . a d d ( C o n v 2 D ( 6 4 , k e r n e l _ s i z e = ( 3 , 3 ) , s t r i d e s = ( 1 , 1 ) , p a d d i n g = \\' v a l i d \\' ,\\na c t i v a t i o n = \\' r e l u \\' ) )\\nm o d e l . a d d ( C o n v 2 D ( 6 4 , k e r n e l _ s i z e = ( 3 , 3 ) , s t r i d e s = ( 1 , 1 ) , p a d d i n g = \\' s a m e \\' ,\\na c t i v a t i o n = \\' r e l u \\' ) )\\nm o d e l . a d d ( M a x P o o l i n g 2 D ( p o o l _ s i z e = ( 2 , 2 ) , s t r i d e s = ( 2 , 2 ) ) )\\nm o d e l . a d d ( D r o p o u t ( 0 . 2 ) )\\n# C7, C8, P9\\nm o d e l . a d d ( C o n v 2 D ( 1 2 8 , k e r n e l _ s i z e = ( 3 , 3 ) , s t r i d e s = ( 1 , 1 ) , p a d d i n g = \\' v a l i d \\'\\n, a c t i v a t i o n = \\' r e l u \\' ) )\\nm o d e l . a d d ( C o n v 2 D ( 1 2 8 , k e r n e l _ s i z e = ( 3 , 3 ) , s t r i d e s = ( 1 , 1 ) , p a d d i n g = \\' s a m e \\' ,\\na c t i v a t i o n = \\' r e l u \\' ) )\\nm o d e l . a d d ( M a x P o o l i n g 2 D ( p o o l _ s i z e = ( 2 , 2 ) , s t r i d e s = ( 2 , 2 ) ) )\\nm o d e l . a d d ( D r o p o u t ( 0 . 2 ) )\\n# F10, F11, Out\\nm o d e l . a d d ( F l a t t e n ( ) )\\nm o d e l . a d d ( D e n s e ( 1 0 2 4 , a c t i v a t i o n = \\' r e l u \\' ) )\\nm o d e l . a d d ( D r o p o u t ( 0 . 5 ) )\\nm o d e l . a d d ( D e n s e ( n u m _ c l a s s e s , a c t i v a t i o n = \\' s o f t m a x \\' ) )\\n#\\nm o d e l . s u m m a r y ( )\\nm o d e l . c o m p i l e ( l o s s = \\' c a t e g o r i c a l _ c r o s s e n t r o p y \\' , o p t i m i z e r = A d a m ( l r = 0 . 0 0 0 1 ,\\nd e c a y = 1 e - 6 ) , m e t r i c s = [ \\' a c c u r a c y \\' ] )\\n# Training\\ns t a r t _ t i m e = t i m e . t i m e ( )\\nh i s t o r y = m o d e l . f i t ( x _ t r a i n , y _ t r a i n , b a t c h _ s i z e = b a t c h _ s i z e , e p o c h s =\\ne p o c h s , v e r b o s e = 2 , v a l i d a t i o n _ d a t a = ( x _ t e s t , y _ t e s t ) , s h u f f l e = T r u e ,\\nc a l l b a c k s = [ E a r l y S t o p p i n g ( m i n _ d e l t a = 0 . 0 0 0 0 1 , p a t i e n c e = 1 0 ) ] )\\ne n d _ t i m e = t i m e . t i m e ( )\\np r i n t ( \" Z e i t d a u e r [ s ] : { } \" . f o r m a t ( ( e n d _ t i m e - s t a r t _ t i m e ) ) )\\n907.2 CIFAR-10\\n# Test / Validierung\\nl o s s , a c c = m o d e l . e v a l u a t e ( x _ t e s t , y _ t e s t , v e r b o s e = 0 )\\np r i n t ( \" L o s s : { } \" . f o r m a t ( l o s s ) )\\np r i n t ( \" G e n a u i g k e i t [ % ] : { } \" . f o r m a t ( a c c * 1 0 0 ) )\\n# Visualisierung: Genauigkeit\\np l t . p l o t ( h i s t o r y . h i s t o r y [ \\' a c c \\' ] )\\np l t . p l o t ( h i s t o r y . h i s t o r y [ \\' v a l _ a c c \\' ] )\\np l t . t i t l e ( \\' M o d e l l - G e n a u i g k e i t \\' )\\np l t . y l a b e l ( \\' G e n a u i g k e i t \\' )\\np l t . x l a b e l ( \\' E p o c h e \\' )\\np l t . l e g e n d ( [ \\' T r a i n i n g \\' , \\' T e s t \\' ] , l o c = \\' l o w e r r i g h t \\' )\\np l t . s h o w ( )\\n# Visualisierung: Fehler (Loss-Funktion)\\np l t . p l o t ( h i s t o r y . h i s t o r y [ \\' l o s s \\' ] )\\np l t . p l o t ( h i s t o r y . h i s t o r y [ \\' v a l _ l o s s \\' ] )\\np l t . t i t l e ( \\' M o d e l l - F e h l e r ( L o s s - F u n k t i o n ) \\' )\\np l t . y l a b e l ( \\' F e h l e r b z w . L o s s \\' )\\np l t . x l a b e l ( \\' E p o c h e \\' )\\np l t . l e g e n d ( [ \\' T r a i n i n g \\' , \\' T e s t \\' ] , l o c = \\' u p p e r r i g h t \\' )\\np l t . s h o w ( )\\x05\\nListing 7.3: CIFAR-10 CNN\\nDie Konsolenausgabe sieht dann wie folgt aus:\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\nconv2d_13 (Conv2D) (None, 30, 30, 32) 896\\n_________________________________________________________________\\nconv2d_14 (Conv2D) (None, 30, 30, 32) 9248\\n_________________________________________________________________\\nmax_pooling2d_7 (MaxPooling2 (None, 15, 15, 32) 0\\n_________________________________________________________________\\ndropout_5 (Dropout) (None, 15, 15, 32) 0\\n_________________________________________________________________\\nconv2d_15 (Conv2D) (None, 13, 13, 64) 18496\\n_________________________________________________________________\\nconv2d_16 (Conv2D) (None, 13, 13, 64) 36928\\n_________________________________________________________________\\nmax_pooling2d_8 (MaxPooling2 (None, 6, 6, 64) 0\\n_________________________________________________________________\\ndropout_6 (Dropout) (None, 6, 6, 64) 0\\n_________________________________________________________________\\nconv2d_17 (Conv2D) (None, 4, 4, 128) 73856\\n_________________________________________________________________\\nconv2d_18 (Conv2D) (None, 4, 4, 128) 147584\\n_________________________________________________________________\\nmax_pooling2d_9 (MaxPooling2 (None, 2, 2, 128) 0\\n_________________________________________________________________\\ndropout_7 (Dropout) (None, 2, 2, 128) 0\\n_________________________________________________________________\\nflatten_1 (Flatten) (None, 512) 0\\n_________________________________________________________________\\ndense_1 (Dense) (None, 1024) 525312\\n_________________________________________________________________\\ndropout_8 (Dropout) (None, 1024) 0\\n_________________________________________________________________\\ndense_2 (Dense) (None, 10) 10250\\n=================================================================\\n917 Anwendungen\\nTotal params: 822,570\\nTrainable params: 822,570\\nNon-trainable params: 0\\n_________________________________________________________________\\nTrain on 50000 samples, validate on 10000 samples\\nEpoch 1/100\\n- 8s - loss: 1.8083 - acc: 0.3184 - val_loss: 1.4979 - val_acc: 0.4422\\nEpoch 2/100\\n- 8s - loss: 1.4655 - acc: 0.4573 - val_loss: 1.3259 - val_acc: 0.5153\\nEpoch 3/100\\n- 8s - loss: 1.3139 - acc: 0.5226 - val_loss: 1.1934 - val_acc: 0.5682\\n...\\nEpoch 45/100\\n- 8s - loss: 0.2950 - acc: 0.8936 - val_loss: 0.6073 - val_acc: 0.8077\\nEpoch 46/100\\n- 8s - loss: 0.2865 - acc: 0.8978 - val_loss: 0.6424 - val_acc: 0.8129\\nEpoch 47/100\\n- 8s - loss: 0.2803 - acc: 0.8997 - val_loss: 0.6085 - val_acc: 0.8178\\nZeitdauer[s]: 373.5088930130005\\nLoss: 0.6085280065774917\\nGenauigkeit[%]: 81.78\\nFür das Training der 47 Epochen hat dieses Netzwerks mit seinen 822.570 freien Para-\\nmetern etwas mehr als 6 Minuten auf der Graﬁkkarte Nvidia GTX 1080 Ti benötigt. Die\\nGenauigkeit, gemessen auf den Testdaten, beträgt 81,78 %, d.h. die Fehlerrate ist somit\\n18,22 % und damit sogar etwas besser als das Ergebnis von Alex Krizhevsky aus dem Jahr\\n2010. Damals hat das Training allerdings noch 3 Tage und 9 Stunden auf der Graﬁkkarte\\nNvidia GTX 280 gedauert [Kri10].\\nAbb. 7.9 zeigt die zeitlichen Verläufe der Genauigkeiten des Klassiﬁzierers auf den Trainings-\\nund Testdaten.\\nAbbildung 7.9: Genauigkeit des CNN für die Klassiﬁkation der CIFAR-10-Datenbank\\nDie zeitlichen Verläufe der Netzwerkfehler ( Loss) auf den Trainings- und Testdaten sind\\nin Abb. 7.10 dargestellt.\\n927.3 IMDb\\nAbbildung 7.10: Fehler bzw. Loss des CNN für die Klassiﬁkation der CIFAR-10-Datenbank\\n7.3 IMDb\\nDie Internet Movie Database ( IMDb )ist die größte Plattform rund um das Thema Film\\n[IMD18a]. Die technische Basis ist eine Datenbank, in der Informationen zu Kino-Filmen,\\nTV-Filmen und -Serien, Videoproduktionen, Computerspielen und Internet-Streams ge-\\nsammelt werden. Nicht nur Produkte, sondern auch Personen wie Schauspieler, Regis-\\nseure, Produzenten usw. werden präsentiert. Die IMDb gehört zum Web 2.0, weil die\\nNutzer maßgeblich Inhalte erstellen ( User Generated Content ), und zwar in Form von\\nOnline-Rezensionen mit Bewertungen. Aktuell (Stand 01.05.2018) gibt es über 91 Mil-\\nlionen registrierte Nutzer und Einträge zu mehr als 4,7 Millionen Titeln und 8,7 Millionen\\nPersonen [IMD18b]. Besitzer und Betreiber der Plattform ist der Online-Händler Amazon.\\nDie verwendeten IMDb -Daten enthalten 50.000 Nutzer-Kritiken (engl. Reviews ), die je-\\nweils sehr eindeutig eine positive oder negative Meinung vertreten [Cho+18a]. Mit Hilfe\\ndieser Daten lässt sich eine Stimmungserkennung (engl. Sentiment Analysis ) durchführen\\n[Kag18c]. Diese Analyseart gehört zum Bereich Text Mining , ein Teilgebiet des Data Mining .\\nZunächst müssen dabei nämlich unstrukturierte Daten, die Rezensionstexte, verarbeitet\\nwerden. Anschließend werden diese dann klassiﬁziert, wobei es nur zwei Klassen gibt: po-\\nsitive oder negative Rezension. Man spricht deshalb auch von binärer Klassiﬁkation. Des\\nWeiteren kann man diesem Anwendungsbeispiel auch das Etikett Big Data geben. Wenn\\nman nämlich alle Rezensionen analysieren würde, wären dies enorme Datenmengen, die\\nverarbeitet werden müssten. Stattdessen wird hier aber nur eine Stichprobe benutzt.\\nDie 50.000 Daten sind in zwei gleiche Hälften aufgeteilt: Trainings- und Testdaten. Insge-\\nsamt werden in diesen Rezensionstexten 85.585 verschiedene Wörter benutzt. Im Mittel\\nbesteht ein Rezensionstext aus 234,76 Wörtern mit einer Standardabweichung von 172,91\\nWörter [Bro16a]. Die Klassen sind entweder mit dem Etikett (engl. Label ) 0 für eine ne-\\ngative Rezension oder 1 für eine positive Rezension versehen. Es gibt zu beiden Klassen\\ngleich viele Datensätze. Im Folgenden sind zwei Beispiele dargestellt.\\n937 Anwendungen\\nLabel 0\\nMy dear Lord,what a movie! Let’s talk about the special effects ﬁrst. Don’t get me wrong here, I\\nam not one of those effect fanatics but I was truly thinking that superimposition was a practice\\nof the long gone past, mainly the 60’s. So for some time I thought they might have recorded this\\nmovie a long time ago and it took them forever to cut and release it. But as far as I know they\\ndid not have cell phones in the 60’s...<br /><br />What I am looking for in movies is mainly\\na good story with a really good message. Acting is secondary, effects are secondary, I do not\\neven mind a few little inconsistencies. However, in a movies like this bad acting, incredibility,\\netc. add up to make a bad movie even worse - that’s what happened for me with the Celestine\\nProphecy.<br /><br />My wife said the book was actually really good and even though I am\\nnot into all that spiritual stuff I can somehow see that it can be brought across in a believable\\nway - the movie failed to do so.<br /><br />There could be one single reason to watch this one\\nthough. If you really love cheesy movies it’ll be the right one for you. If the IMDb stars were\\nfor cheesiness instead of quality I MUST have rated this movie ten stars.<br /><br />By the\\nway, three stars are for the fact that there are worse movies out there, like \"Critical Mass\"(look\\nup the comments on that one - hilarious). The Celestine Prophecy is at least entertaining to a\\ncertain degree.\\nLabel 1\\nI loved this movie. It is a deﬁnite inspirational movie. It ﬁlls you with pride. This movie is\\nworth the rental or worth buying. It should be in everyones home. Best movie I have seen in a\\nlong time. It will make you mad because everyone is so mean to Carl Brashear, but in the end it\\ngets better. It is a story of romance, drama, action, and plenty of funny lines to keep you tuned\\nin. I love a lot of the quotes. I use them all the time. They help keep me on task of what I want\\nto do. It shows that anyone can achieve their dreams, all they have to do is work for it. It is a\\nlong movie, but every time I watch it, I never notice that it is as long as it is. I get so engrossed\\nin it, that it goes so quick. I love this movie. I watch it whenever I can.\\nWenn man sich die Beispiele genauer betrachtet und die Texte liest, stellt man fest, dass\\ndie zweite Rezension sehr eindeutig und leicht als positive Kritik erkannt werden kann.\\nDirekt in den ersten Sätzen werden Schlüsselwörter wie loved ,inspirational ,pride ,worth ,\\nbestbenutzt. Das erste Beispiel dagegen ist sehr viel schwieriger zu interpretieren, denn es\\nenthält sowohl Schlüsselwörter wie badund worse aber genauso good und love. Außerdem\\nsind im ersten Beispiel noch sogenannte Tags wie bspw. <br/> enthalten. Dies sind Textfor-\\nmatierungen in Hypertext Markup Language ( HTML ), die also keinerlei Bedeutung haben.\\nIn den Modellen, die in dieser Arbeit für die Sentiment Analysis zum Einsatz kommen,\\nwerden aber keinerlei Hinweise gegeben, welche (Schlüssel-)Wörter welche Bedeutung\\nhaben, auch nicht, ob diese Wörter etwas positiv oder negativ beschreiben.\\nIn Keras sind diese 50.000 IMDb-Datensätze bereits vorverarbeitet. Jeder Rezensionstext\\nist dabei als Sequenz von Wörtern dargestellt. Jedes Wort wird als ganze Zahl ( Integer ) aus-\\ngedrückt, abhängig davon, wie häuﬁg es insgesamt vorkommt. D.h. das häuﬁgste Wort\\nbekommt die Nummer 1, das zweithäuﬁgste die Nummer 2 usw. Die Zahl 0 steht nicht\\nfür ein spezielles Wort, sondern für irgendein unbekanntes Wort, das nicht im zugrunde-\\nliegenden Wörterbuch enthalten ist. Die Länge dieser Sequenzen ist für jeden Datensatz\\nverschieden, da die Rezensionstexte unterschiedlich lang sind.\\nEine der ersten Aufgaben ist es, diese Sequenzen auf die gleiche Länge zu bringen, damit\\nsie weiterverarbeitet werden können. Nach einer festgelegten Länge von Wörtern wird\\ndie jeweilige Sequenz einfach abgeschnitten. Falls der Rezensionstext weniger Wörter ent-\\nhält, dann wird mit Nullwerten aufgefüllt ( Padding ). Ein weiterer, wichtiger Schritt ist die\\nTechnik Word Embedding . Dabei werden Sequenzen von Wörtern, die als diskrete, ganze\\n947.3 IMDb\\nZahlen repräsentiert sind, auf einen Vektor abgebildet, der aus Elementen von kontinuier-\\nlichen Gleitkommazahlen besteht. Dies ist notwendig, weil KNN diese kontinuierlichen\\nWerte besser verarbeiten können.\\nIn den Modellen dieser Arbeit werden nur die 1.000 häuﬁgsten Wörter betrachtet und die\\nRezensionen werden auf die ersten 100 Wörter gekürzt. Beim Word Embedding werden\\ndann die Sequenzen der 100 Wörter auf Vektoren der Dimension 16 abgebildet. Da es sehr\\nviele Trainingsdaten gibt, wird als batch_size hier kein Wert zwischen 32 und 256, sondern\\nsogar mit den Längen von 1.024 bzw. 4.096 gearbeitet. Man könnte daher auch von einer\\nMaxi-Batch statt von Mini-Batch sprechen (vgl. Kap. 3.3). Diese Einstellgröße hat Auswir-\\nkungen auf das Lernen und somit muss man mit den Einstellungen des Lernalgorithmus,\\ninsbes. der Lernrate, experimentieren. Als Optimierer des Gradientenabstiegs wird Adam\\nverwendet. Vier unterschiedliche Netzwerkmodelle (MLP , CNN, LSTM und GRU) wer-\\nden ausprobiert und die jeweiligen Quellcodes, Ausgaben und graﬁschen Ergebnisse im\\nFolgenden dargestellt. Das Training wird abgebrochen, wenn in den letzten 20 Epochen\\nkeine signiﬁkante Verbesserung erzielt werden konnte.\\nMLP Das MLP ist eines der einfachsten, aber auch populärsten KNN (vgl. Kap. 3). In\\ndieser Stimmungsanalyse bzw. Klassiﬁkationsaufgabe wird die Topologie 256-16-1 ver-\\nwendet. Die Output-Einheit benutzt die sigmoide Aktivierungsfunktion. Dadurch wird\\ndas Ergebnis im Intervall [0, 1]abgebildet. Ein Wert nahe null entspricht also einer ne-\\ngativen Rezension, während ein Wert nahe eins für eine positive Kritik steht. Um das\\nÜbertrainieren zu vermindern, kommt die Dropout -Technik zum Einsatz, in der beim Trai-\\nning zufällig eine vorgegebene Zahl von Parametern (Gewichte, Schwellenwerte) auf null\\ngesetzt wird (vgl. Kap. 2.7).\\n# Importiere Keras-Bibliotheken und -Funktionen\\nf r o m k e r a s . p r e p r o c e s s i n g i m p o r t s e q u e n c e\\nf r o m k e r a s . m o d e l s i m p o r t S e q u e n t i a l\\nf r o m k e r a s . l a y e r s i m p o r t D e n s e , D r o p o u t , E m b e d d i n g , F l a t t e n\\nf r o m k e r a s . o p t i m i z e r s i m p o r t A d a m\\nf r o m k e r a s . c a l l b a c k s i m p o r t E a r l y S t o p p i n g\\nf r o m k e r a s . d a t a s e t s i m p o r t i m d b\\n# Importiere sonstige Bibliotheken und Funktionen\\ni m p o r t m a t p l o t l i b . p y p l o t a s p l t\\ni m p o r t t i m e\\n# Trainingsparameter\\nd i c t i o n a r y _ l e n g t h = 1 0 0 0 # Wörterbuch: Die 1.000 häufigsten Wörter in den\\nRezensionen\\nm a x _ r e v i e w _ l e n g t h = 2 0 0 # Betrachte die ersten 200 Wörter je Rezension\\ne m b e d d i n g _ d i m = 1 6 # Jede Rezension wird auf einen Vektor (Dimension 16) abgebildet\\nb a t c h _ s i z e = 1 0 2 4 # Maxi-Batch\\ne p o c h s = 1 0 0 0\\n# Partitionierung: Training vs Test\\n( x _ t r a i n , y _ t r a i n ) , ( x _ t e s t , y _ t e s t ) = i m d b . l o a d _ d a t a ( n u m _ w o r d s =\\nd i c t i o n a r y _ l e n g t h )\\n# Datenkonvertierung: X-Werte => Feste Dimension max_review_length\\nx _ t r a i n = s e q u e n c e . p a d _ s e q u e n c e s ( x _ t r a i n , m a x l e n = m a x _ r e v i e w _ l e n g t h )\\nx _ t e s t = s e q u e n c e . p a d _ s e q u e n c e s ( x _ t e s t , m a x l e n = m a x _ r e v i e w _ l e n g t h )\\n# Modell\\nm o d e l = S e q u e n t i a l ( )\\nm o d e l . a d d ( E m b e d d i n g ( d i c t i o n a r y _ l e n g t h , e m b e d d i n g _ d i m , i n p u t _ l e n g t h =\\nm a x _ r e v i e w _ l e n g t h ) )\\n957 Anwendungen\\nm o d e l . a d d ( D r o p o u t ( 0 . 5 ) )\\nm o d e l . a d d ( F l a t t e n ( ) )\\nm o d e l . a d d ( D r o p o u t ( 0 . 5 ) )\\nm o d e l . a d d ( D e n s e ( 2 5 6 , a c t i v a t i o n = \\' r e l u \\' ) )\\nm o d e l . a d d ( D r o p o u t ( 0 . 5 ) )\\nm o d e l . a d d ( D e n s e ( 1 6 , a c t i v a t i o n = \\' r e l u \\' ) )\\nm o d e l . a d d ( D r o p o u t ( 0 . 5 ) )\\nm o d e l . a d d ( D e n s e ( 1 , a c t i v a t i o n = \\' s i g m o i d \\' ) )\\nm o d e l . s u m m a r y ( )\\nm o d e l . c o m p i l e ( l o s s = \\' b i n a r y _ c r o s s e n t r o p y \\' , o p t i m i z e r = A d a m ( l r = 0 . 0 0 1 , d e c a y\\n= 1 e - 5 ) , m e t r i c s = [ \\' a c c u r a c y \\' ] )\\n# Training\\ns t a r t _ t i m e = t i m e . t i m e ( )\\nh i s t o r y = m o d e l . f i t ( x _ t r a i n , y _ t r a i n , b a t c h _ s i z e = b a t c h _ s i z e , e p o c h s =\\ne p o c h s , v e r b o s e = 2 , v a l i d a t i o n _ d a t a = ( x _ t e s t , y _ t e s t ) , c a l l b a c k s = [\\nE a r l y S t o p p i n g ( m i n _ d e l t a = 1 e - 6 , p a t i e n c e = 2 0 ) ] )\\ne n d _ t i m e = t i m e . t i m e ( )\\np r i n t ( \" Z e i t d a u e r [ s ] : { } \" . f o r m a t ( ( e n d _ t i m e - s t a r t _ t i m e ) ) )\\n# Test / Validierung\\nl o s s , a c c = m o d e l . e v a l u a t e ( x _ t e s t , y _ t e s t , v e r b o s e = 0 )\\np r i n t ( \" L o s s : { } \" . f o r m a t ( l o s s ) )\\np r i n t ( \" G e n a u i g k e i t [ % ] : { } \" . f o r m a t ( a c c * 1 0 0 ) )\\n# Visualisierung: Genauigkeit\\np l t . p l o t ( h i s t o r y . h i s t o r y [ \\' a c c \\' ] )\\np l t . p l o t ( h i s t o r y . h i s t o r y [ \\' v a l _ a c c \\' ] )\\np l t . t i t l e ( \\' M o d e l l - G e n a u i g k e i t \\' )\\np l t . y l a b e l ( \\' G e n a u i g k e i t \\' )\\np l t . x l a b e l ( \\' E p o c h e \\' )\\np l t . l e g e n d ( [ \\' T r a i n i n g \\' , \\' T e s t \\' ] , l o c = \\' l o w e r r i g h t \\' )\\np l t . s h o w ( )\\n# Visualisierung: Fehler (Loss-Funktion)\\np l t . p l o t ( h i s t o r y . h i s t o r y [ \\' l o s s \\' ] )\\np l t . p l o t ( h i s t o r y . h i s t o r y [ \\' v a l _ l o s s \\' ] )\\np l t . t i t l e ( \\' M o d e l l - F e h l e r ( L o s s - F u n k t i o n ) \\' )\\np l t . y l a b e l ( \\' F e h l e r b z w . L o s s \\' )\\np l t . x l a b e l ( \\' E p o c h e \\' )\\np l t . l e g e n d ( [ \\' T r a i n i n g \\' , \\' T e s t \\' ] , l o c = \\' u p p e r r i g h t \\' )\\np l t . s h o w ( )\\x05\\nListing 7.4: IMDB MLP\\nErgebnis:\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\nembedding_38 (Embedding) (None, 200, 16) 16000\\n_________________________________________________________________\\ndropout_120 (Dropout) (None, 200, 16) 0\\n_________________________________________________________________\\nflatten_38 (Flatten) (None, 3200) 0\\n_________________________________________________________________\\ndropout_121 (Dropout) (None, 3200) 0\\n_________________________________________________________________\\ndense_109 (Dense) (None, 256) 819456\\n_________________________________________________________________\\ndropout_122 (Dropout) (None, 256) 0\\n_________________________________________________________________\\n967.3 IMDb\\ndense_110 (Dense) (None, 16) 4112\\n_________________________________________________________________\\ndropout_123 (Dropout) (None, 16) 0\\n_________________________________________________________________\\ndense_111 (Dense) (None, 1) 17\\n=================================================================\\nTotal params: 839,585\\nTrainable params: 839,585\\nNon-trainable params: 0\\n_________________________________________________________________\\nTrain on 25000 samples, validate on 25000 samples\\nEpoch 1/1000\\n- 1s - loss: 0.6940 - acc: 0.5091 - val_loss: 0.6921 - val_acc: 0.5242\\nEpoch 2/1000\\n- 0s - loss: 0.6910 - acc: 0.5235 - val_loss: 0.6905 - val_acc: 0.5502\\nEpoch 3/1000\\n- 0s - loss: 0.6853 - acc: 0.5526 - val_loss: 0.6759 - val_acc: 0.6090\\n...\\nEpoch 36/1000\\n- 0s - loss: 0.3136 - acc: 0.8684 - val_loss: 0.3288 - val_acc: 0.8596\\nEpoch 37/1000\\n- 0s - loss: 0.3123 - acc: 0.8710 - val_loss: 0.3272 - val_acc: 0.8596\\nEpoch 38/1000\\n- 0s - loss: 0.3103 - acc: 0.8681 - val_loss: 0.3280 - val_acc: 0.8602\\nZeitdauer[s]: 9.14800477027893\\nLoss: 0.3280345226287842\\nGenauigkeit[%]: 86.016\\nFür das Training der 38 Epochen des MLP werden knapp 10 Sekunden auf der Graﬁkkarte\\nNvidia GTX 1080 Ti benötigt. Die Genauigkeit, gemessen auf den Testdaten, beträgt 86 %,\\nd.h. die Fehlerrate ist somit 14 %. Beim zufälligen Raten und der Gleichverteilung beider\\nKlassen würde man eine Genauigkeit von 50 % erzielen. Da die IMDb Sentiment Analysis\\naber keine Standard-Aufgabe oder Bestandteil eines Wettbewerbs ist, wie dies bspw. bei\\nden MNIST-Daten (vgl. Kap. 7.1) oder den CIFAR-10-Daten (vgl. 7.2) der Fall ist, fehlen\\naussagekräftige Referenzwerte zum Vergleichen der Ergebnisse. Abb. 7.11 zeigt die zeitli-\\nchen Verläufe der Genauigkeiten des Klassiﬁzierers auf den Trainings- und Testdaten.\\nAbbildung 7.11: Genauigkeit des MLP für die Klassiﬁkation der IMDb-Datenbank\\n977 Anwendungen\\nIn Abb. 7.12 sind die zeitlichen Verläufe der Netzwerkfehler, d.h. der berechnete Loss, auf\\nden Trainings- und Testdaten dargestellt.\\nAbbildung 7.12: Fehler bzw. Loss des MLP für die Klassiﬁkation der IMDb-Datenbank\\nCNN EinConvolutional Neural Network wird meistens im Bereich Bilderkennung ein-\\ngesetzt (vgl. Kap. 4). Trotzdem kann man es auch verwenden, um aus den Rohdaten\\nsogenannte Features zu generieren und auf Basis dieser Features eine Klassiﬁzierung vor-\\nzunehmen. In dieser Sentiment Analysis können diese Features als Kombinationen von\\nWörtern interpretiert werden, die häuﬁg zusammen verwendet werden, um entweder\\neine positive oder eine negative Stimmung auszudrücken. Hierzu wird ein eindimensio-\\nnaler Convolutional Layer mit 128 Feature Maps und einem Kernel der Dimension 3 ver-\\nwendet. Auf dieser Schicht folgt dann ein dazu passender Pooling Layer . Zur Aktivierung\\nwerden ReLU s verwendet. Im folgenden Quelltext sind nur die Änderungen gegenüber\\ndem ersten Modell dargestellt.\\n# Importiere Keras-Bibliotheken und -Funktionen\\nf r o m k e r a s . l a y e r s i m p o r t D e n s e , D r o p o u t , E m b e d d i n g , F l a t t e n , C o n v 1 D ,\\nG l o b a l M a x P o o l i n g 1 D\\n# Modell\\nm o d e l = S e q u e n t i a l ( )\\nm o d e l . a d d ( E m b e d d i n g ( d i c t i o n a r y _ l e n g t h , e m b e d d i n g _ d i m , i n p u t _ l e n g t h =\\nm a x _ r e v i e w _ l e n g t h ) )\\nm o d e l . a d d ( D r o p o u t ( 0 . 3 ) )\\nm o d e l . a d d ( C o n v 1 D ( 1 2 8 , 3 , p a d d i n g = \\' v a l i d \\' , a c t i v a t i o n = \\' r e l u \\' , s t r i d e s = 1 ) )\\nm o d e l . a d d ( G l o b a l M a x P o o l i n g 1 D ( ) )\\nm o d e l . a d d ( D e n s e ( 1 6 , a c t i v a t i o n = \\' r e l u \\' ) )\\nm o d e l . a d d ( D r o p o u t ( 0 . 2 ) )\\nm o d e l . a d d ( D e n s e ( 1 , a c t i v a t i o n = \\' s i g m o i d \\' ) )\\nm o d e l . s u m m a r y ( )\\nm o d e l . c o m p i l e ( l o s s = \\' b i n a r y _ c r o s s e n t r o p y \\' , o p t i m i z e r = A d a m ( l r = 0 . 0 0 1 , d e c a y\\n= 1 e - 5 ) , m e t r i c s = [ \\' a c c u r a c y \\' ] )\\x05\\nListing 7.5: IMDB CNN\\n987.3 IMDb\\nErgebnis:\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\nembedding_29 (Embedding) (None, 200, 16) 16000\\n_________________________________________________________________\\ndropout_57 (Dropout) (None, 200, 16) 0\\n_________________________________________________________________\\nconv1d_29 (Conv1D) (None, 198, 128) 6272\\n_________________________________________________________________\\nglobal_max_pooling1d_28 (Glo (None, 128) 0\\n_________________________________________________________________\\ndense_57 (Dense) (None, 16) 2064\\n_________________________________________________________________\\ndropout_58 (Dropout) (None, 16) 0\\n_________________________________________________________________\\ndense_58 (Dense) (None, 1) 17\\n=================================================================\\nTotal params: 24,353\\nTrainable params: 24,353\\nNon-trainable params: 0\\n_________________________________________________________________\\nTrain on 25000 samples, validate on 25000 samples\\nEpoch 1/1000\\n- 1s - loss: 0.6922 - acc: 0.5311 - val_loss: 0.6892 - val_acc: 0.7021\\nEpoch 2/1000\\n- 0s - loss: 0.6824 - acc: 0.6437 - val_loss: 0.6640 - val_acc: 0.7370\\nEpoch 3/1000\\n- 0s - loss: 0.6234 - acc: 0.7354 - val_loss: 0.5544 - val_acc: 0.7790\\n...\\nEpoch 54/1000\\n- 0s - loss: 0.2447 - acc: 0.9023 - val_loss: 0.3099 - val_acc: 0.8704\\nEpoch 55/1000\\n- 0s - loss: 0.2379 - acc: 0.9067 - val_loss: 0.3186 - val_acc: 0.8693\\nEpoch 56/1000\\n- 1s - loss: 0.2388 - acc: 0.9072 - val_loss: 0.3115 - val_acc: 0.8709\\nZeitdauer[s]: 27.976366996765137\\nLoss: 0.3114780087661743\\nGenauigkeit[%]: 87.092\\nFür das Training der 56 Epochen des CNN wird knapp eine halbe Minute auf der Graﬁk-\\nkarte Nvidia GTX 1080 Ti benötigt. Die Genauigkeit, gemessen auf den Testdaten, beträgt\\n87 %, d.h. die Fehlerrate ist somit 13 %. D.h. das CNN schlägt das MLP also um einen\\nProzentpunkt.\\nAbb. 7.13 zeigt die zeitlichen Verläufe der Genauigkeiten des Klassiﬁzierers auf den\\nTrainings- und Testdaten. In Abb. 7.14 sind die zeitlichen Verläufe der Netzwerkfehler,\\nd.h. der berechnete Loss, auf den Trainings- und Testdaten dargestellt. Nach ca. 12 Epo-\\nchen kreuzen sich die Kurven, d.h. das Training führt ab dann zu immer besseren Werten,\\nwährend die Messung auf der Testmenge keine nennenswerten Verbesserungen mehr\\nbringt. Dies ist ein Zeichen für Übertrainieren. Mit Hilfe von anderen Strategien, z.B. Re-\\ngularisierung, kann die Generalisierungsleistung dieses Modells also ggf. noch gesteigert\\nwerden.\\n997 Anwendungen\\nAbbildung 7.13: Genauigkeit des CNN für die Klassiﬁkation der IMDb-Datenbank\\nAbbildung 7.14: Fehler bzw. Loss des CNN für die Klassiﬁkation der IMDb-Datenbank\\nLSTM Da in dieser Anwendung Sequenzen verwendet werden und diese Sequenzen\\naus Wörter bestehen, sind RNN eigentlich prädestiniert, um solche Probleme anzuge-\\nhen. Die bekanntesten Vertreter dieser rekurrenten Netzwerke sind LSTM (vgl. Kap. 5.4)\\nund seine Varianten wie bspw. GRU (vgl. Kap 5.5). Betrachten wir zunächst die Long\\nShort-Term Memory Einheiten. Generell sind rekurrente Netzwerke sehr schwierig zu trai-\\nnieren. Die batch_size wird auf 4.096 und die Lernrate auf 0,02 erhöht, um ein möglichst\\nschnelles Training zu erreichen. Dafür ist das Training dann ggf. nicht so robust und grö-\\nßere Ausschläge, d.h. Varianzen, werden in Kauf genommen. Es wird eine Schicht mit 16\\nLSTM-Einheiten trainiert. Im Folgenden sind wieder nur die Änderungen im Quelltext\\nim Vergleich zum ersten Modell dargestellt.\\n1007.3 IMDb\\n# Importiere Keras-Bibliotheken und -Funktionen\\nf r o m k e r a s . l a y e r s i m p o r t D e n s e , D r o p o u t , E m b e d d i n g , L S T M\\n# Trainingsparameter\\nb a t c h _ s i z e = 4 0 9 6 # Maxi-Batch\\n# Modell\\nm o d e l = S e q u e n t i a l ( )\\nm o d e l . a d d ( E m b e d d i n g ( d i c t i o n a r y _ l e n g t h , e m b e d d i n g _ d i m , i n p u t _ l e n g t h =\\nm a x _ r e v i e w _ l e n g t h ) )\\nm o d e l . a d d ( D r o p o u t ( 0 . 2 ) )\\nm o d e l . a d d ( L S T M ( 1 6 , d r o p o u t = 0 . 2 , r e c u r r e n t _ d r o p o u t = 0 . 2 ) )\\nm o d e l . a d d ( D e n s e ( 4 , a c t i v a t i o n = \\' r e l u \\' ) )\\nm o d e l . a d d ( D r o p o u t ( 0 . 2 ) )\\nm o d e l . a d d ( D e n s e ( 1 , a c t i v a t i o n = \\' s i g m o i d \\' ) )\\nm o d e l . s u m m a r y ( )\\nm o d e l . c o m p i l e ( l o s s = \\' b i n a r y _ c r o s s e n t r o p y \\' , o p t i m i z e r = A d a m ( l r = 0 . 0 2 , d e c a y = 1\\ne - 5 ) , m e t r i c s = [ \\' a c c u r a c y \\' ] )\\x05\\nListing 7.6: IMDB LSTM\\nErgebnis:\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\nembedding_1 (Embedding) (None, 200, 16) 16000\\n_________________________________________________________________\\ndropout_1 (Dropout) (None, 200, 16) 0\\n_________________________________________________________________\\nlstm_1 (LSTM) (None, 16) 2112\\n_________________________________________________________________\\ndense_1 (Dense) (None, 4) 68\\n_________________________________________________________________\\ndropout_2 (Dropout) (None, 4) 0\\n_________________________________________________________________\\ndense_2 (Dense) (None, 1) 5\\n=================================================================\\nTotal params: 18,185\\nTrainable params: 18,185\\nNon-trainable params: 0\\n_________________________________________________________________\\nTrain on 25000 samples, validate on 25000 samples\\nEpoch 1/1000\\n- 2s - loss: 0.6871 - acc: 0.5389 - val_loss: 0.6639 - val_acc: 0.6695\\nEpoch 2/1000\\n- 1s - loss: 0.6588 - acc: 0.6185 - val_loss: 0.6156 - val_acc: 0.6651\\nEpoch 3/1000\\n- 1s - loss: 0.5979 - acc: 0.6576 - val_loss: 0.5346 - val_acc: 0.7394\\n...\\nEpoch 124/1000\\n- 1s - loss: 0.2681 - acc: 0.8943 - val_loss: 0.3187 - val_acc: 0.8709\\nEpoch 125/1000\\n- 1s - loss: 0.2635 - acc: 0.8948 - val_loss: 0.3230 - val_acc: 0.8711\\nEpoch 126/1000\\n- 1s - loss: 0.2642 - acc: 0.8952 - val_loss: 0.3213 - val_acc: 0.8694\\nZeitdauer[s]: 190.6857328414917\\nLoss: 0.3212622569513321\\nGenauigkeit[%]: 86.936\\n1017 Anwendungen\\nFür das Training der 126 Epochen der LSTM-Einheiten werden bereits ca. 3 Minuten\\nauf der Graﬁkkarte Nvidia GTX 1080 Ti benötigt. Die Genauigkeit, gemessen auf den\\nTestdaten, beträgt 87 %, d.h. die Fehlerrate ist somit 13 %. Dieses Ergebnis ist also mit\\ndem des CNN vergleichbar, wenn auch das Training hierfür ca. 6 Mal so lange dauert. Abb.\\n7.15 zeigt die zeitlichen Verläufe der Genauigkeiten des Klassiﬁzierers auf den Trainings-\\nund Testdaten. In Abb. 7.16 sind die zeitlichen Verläufe der Netzwerkfehler, d.h. der\\nberechnete Loss, auf den Trainings- und Testdaten dargestellt. Man erkennt in beiden\\nAbbildungen sehr deutlich die Ausschläge aufgrund der hohen Werte für die batch_size\\nund Lernrate.\\nAbbildung 7.15: Genauigkeit des LSTM für die Klassiﬁkation der IMDb-Datenbank\\nAbbildung 7.16: Fehler bzw. Loss des LSTM für die Klassiﬁkation der IMDb-Datenbank\\n1027.3 IMDb\\nGRU Das letzte Modell verwendet die Variante Gated Recurrent Unit statt der LSTM-\\nEinheit. Das ist auch schon die einzige Änderung im Quelltext.\\n# Importiere Keras-Bibliotheken und -Funktionen\\nf r o m k e r a s . l a y e r s i m p o r t D e n s e , D r o p o u t , E m b e d d i n g , G R U\\n# Trainingsparameter\\nb a t c h _ s i z e = 4 0 9 6 # Maxi-Batch\\n# Modell\\nm o d e l = S e q u e n t i a l ( )\\nm o d e l . a d d ( E m b e d d i n g ( d i c t i o n a r y _ l e n g t h , e m b e d d i n g _ d i m , i n p u t _ l e n g t h =\\nm a x _ r e v i e w _ l e n g t h ) )\\nm o d e l . a d d ( D r o p o u t ( 0 . 2 ) )\\nm o d e l . a d d ( G R U ( 1 6 , d r o p o u t = 0 . 2 , r e c u r r e n t _ d r o p o u t = 0 . 2 ) )\\nm o d e l . a d d ( D e n s e ( 4 , a c t i v a t i o n = \\' r e l u \\' ) )\\nm o d e l . a d d ( D r o p o u t ( 0 . 2 ) )\\nm o d e l . a d d ( D e n s e ( 1 , a c t i v a t i o n = \\' s i g m o i d \\' ) )\\nm o d e l . s u m m a r y ( )\\nm o d e l . c o m p i l e ( l o s s = \\' b i n a r y _ c r o s s e n t r o p y \\' , o p t i m i z e r = A d a m ( l r = 0 . 0 2 , d e c a y = 1\\ne - 5 ) , m e t r i c s = [ \\' a c c u r a c y \\' ] )\\x05\\nListing 7.7: IMDB GRU\\nErgebnis:\\n_________________________________________________________________\\nLayer (type) Output Shape Param #\\n=================================================================\\nembedding_19 (Embedding) (None, 200, 16) 16000\\n_________________________________________________________________\\ndropout_34 (Dropout) (None, 200, 16) 0\\n_________________________________________________________________\\ngru_19 (GRU) (None, 16) 1584\\n_________________________________________________________________\\ndense_33 (Dense) (None, 4) 68\\n_________________________________________________________________\\ndropout_35 (Dropout) (None, 4) 0\\n_________________________________________________________________\\ndense_34 (Dense) (None, 1) 5\\n=================================================================\\nTotal params: 17,657\\nTrainable params: 17,657\\nNon-trainable params: 0\\n_________________________________________________________________\\nTrain on 25000 samples, validate on 25000 samples\\nEpoch 1/1000\\n- 3s - loss: 0.6799 - acc: 0.5566 - val_loss: 0.6020 - val_acc: 0.6840\\nEpoch 2/1000\\n- 1s - loss: 0.6167 - acc: 0.6534 - val_loss: 0.5922 - val_acc: 0.6800\\nEpoch 3/1000\\n- 1s - loss: 0.5927 - acc: 0.6916 - val_loss: 0.5799 - val_acc: 0.6916\\n...\\nEpoch 70/1000\\n- 1s - loss: 0.2713 - acc: 0.8931 - val_loss: 0.3253 - val_acc: 0.8568\\nEpoch 71/1000\\n- 1s - loss: 0.2795 - acc: 0.8881 - val_loss: 0.3020 - val_acc: 0.8728\\nEpoch 72/1000\\n- 1s - loss: 0.2752 - acc: 0.8898 - val_loss: 0.3010 - val_acc: 0.8743\\nZeitdauer[s]: 88.92190265655518\\nLoss: 0.30100429022312164\\nGenauigkeit[%]: 87.432\\n1037 Anwendungen\\nFür das Training der 72 Epochen des GRU-Netzwerks werden knapp 1,5 Minuten auf der\\nGraﬁkkarte Nvidia GTX 1080 Ti benötigt. Die Genauigkeit, gemessen auf den Testdaten,\\nbeträgt 87,4 %, d.h. die Fehlerrate ist somit 12,6 %. D.h. einerseits konnte die Trainingsdau-\\ner gegenüber dem LSTM-Netzwerk um die Hälfte reduziert werden, andererseits erzielte\\ndieses Netzwerk das beste Ergebnis. Abb. 7.17 zeigt die zeitlichen Verläufe der Genauig-\\nkeiten des Klassiﬁzierers auf den Trainings- und Testdaten. In Abb. 7.18 sind die zeitlichen\\nVerläufe der Netzwerkfehler, d.h. der berechnete Loss, auf den Trainings- und Testdaten\\ndargestellt.\\nAbbildung 7.17: Genauigkeit des GRU für die Klassiﬁkation der IMDb-Datenbank\\nAbbildung 7.18: Fehler bzw. Loss des GRU für die Klassiﬁkation der IMDb-Datenbank\\n1047.3 IMDb\\nInsgesamt lässt sich feststellen, dass mit allen vier Modellen sehr ähnliche Ergebnisse\\n(Genauigkeiten von 86 bis 87,4 %) erzielt werden konnten. In den Tutorials von Jason\\nBrownlee wird ein anderer Ansatz verfolgt [Bro16a; Bro16a]: Hier werden 5.000 statt nur\\n1.000 der häuﬁgsten Wörter betrachtet und die Rezensionen werden auf 500 Wörter statt\\nauf 100 gekürzt. Auch die Dimension des Vektors beim Word Embedding ist mit 32 größer\\nals in dieser Arbeit, nämlich genau doppelt so groß. Dafür werden für die batch_size Werte\\nvon 64 oder 128 verwendet und das Training bereits nach 2 oder 3 Epochen abgebro-\\nchen. Der wesentliche Nachteil dieses Ansatzes ist allerdings, dass eine graﬁsche Analyse\\ndes Trainingsfortschritts bei so wenigen Epochen nicht möglich ist. In Tab. 7.3 sind die\\nErgebnisse dieser Arbeit und die Ergebnisse aus den Tutorials gegenübergestellt.\\nModell Diese Arbeit Brownlee Quelle\\nMultilayer Perceptron 86,02 % 86,94 % [Bro16a]\\nConvolutional Neural Network 87,09 % 87,79 % [Bro16a]\\nLong Short-Term Memory 86,94 % 86,79 % [Bro16b]\\nGated Recurrent Unit 87,43 % - -\\nTabelle 7.3: Vergleich der Ergebnisse\\nDie Unterschiede zwischen den Ergebnissen sind nicht sehr groß. Die Modelle dieser\\nArbeit sind sehr viel einfacher hinsichtlich der genannten Parameter (Länge des Wör-\\nterbuchs, Länge der Rezensionen, Dimension des Vektors zum Word Embedding ). Der\\nEinﬂuss dieser Parameter auf das Ergebnis könnte in weiteren Studien näher untersucht\\nwerden. Hierzu eignen sich insbes. sogenannte Hyperparameter-Optimierungen. Dabei\\nkommen bspw. die Gittersuche (engl. Grid Search ) oder Evolutionäre Algorithmen zum\\nEinsatz, um die optimale Parameter-Kombination zu ﬁnden. Nachgelagert können dann\\nnoch an den eigentlichen Netzparametern (Anzahl der Schichten und Einheiten) und Trai-\\nningsparametern (Aufteilung der Teilmengen, Lernrate) weitere Studien zur Optimierung\\ndurchgeführt werden.\\nDas Anwendungsbeispiel IMDb Sentiment Analysis zeigt die Möglichkeiten des Maschi-\\nnenlernens im Bereich Big Data auf. Aus unstrukturierte Daten wird neues Wissen ge-\\nneriert, nämlich beliebige Rezensionen automatisch möglichst korrekt klassiﬁzieren zu\\nkönnen. Einerseits konnten hierzu vier unterschiedliche Architekturen Künstlicher Neu-\\nronaler Netzwerke auf dieses Problem angesetzt werden, d.h. aus didaktischen Gründen\\nist dieses Anwendungsbeispiel bereits sehr interessant, andererseits sind die mit diesen\\nModellen erzielten Ergebnissen bereits recht gut, die Fehlerquote liegt unter 15 Prozent,\\nsodass man die trainierten Modelle auch tatsächlich zur automatisierten Klassiﬁkation\\neinsetzen könnte.\\n1057 Anwendungen\\nÄhnliche Anwendungsfälle in diesem Bereich sind denkbar, einige Beispiele sind hier\\nabschließend noch kurz genannt:\\n1 Produktbewertungen in Online-Portalen klassiﬁzieren\\n2 Hasskommentare in sozialen Netzwerken identiﬁzieren\\n3 Spam-E-Mails ﬁltern\\n4 Kunden-E-Mails an den richtigen Service weiterleiten\\n5 Kunden-Zufriedenheit bezgl. Produkte & Dienstleistungen messen\\n6 Wahlergebnisse anhand von Stimmungsbilder vorhersagen\\n7 Auswirkungen von Wirtschaftsnachrichten auf Aktienkurse ermitteln\\n1068 Zusammenfassung und Ausblick\\nDigitalisierung bedeutet im engeren Sinn die Transformation von analoge in digitale Da-\\nten. Im weiteren Sinn wird darunter die digitale Revolution verstanden, die unser Arbeits-\\nund Privatleben radikal verändert. Als Treiber für diese Entwicklung sind die technolo-\\ngischen Fortschritte in den folgenden Bereichen zu nennen: Mobile Anwendungen (Ap-\\nps),Cloud Computing ,Big Data , Künstliche Intelligenz (KI), Internet of Things (IoT) sowie\\nIndustrie 4.0. Die KI nimmt dabei eine besondere Rolle ein. Aufgrund der intensiven\\nNutzung und großen Verbreitung mobiler Endgeräte, der Auslagerung von IT-Services\\nanCloud -Anbieter, dem Zusammenschluss vieler Geräte im Internet und dem massiven\\nEinsatz sensorischer Maschinen in Smart Factories werden enorme Mengen an digitalen\\nDaten generiert ( Big Data ), die ohne den Einsatz intelligenter Algorithmen nicht mehr\\nverarbeitet und analysiert werden können. Genau hier setzen moderne Methoden der KI\\nan. Künstliche Neuronale Netzwerke (KNN) und insbes. Deep Learning (DL) bilden die\\ntechnologische Grundlage von vielen selbstlernenden Systemen. Auch aufgrund von Fort-\\nschritten im Bereich der graﬁschen Prozessoren (GPUs) können diese KI-Technologien\\ninzwischen kostengünstig eingesetzt werden und/oder komplexe Probleme der Bild- und\\nSpracherkennung lösen. Ein Beispiel ist das autonome Fahren, bei denen die Videodaten\\nin Echtzeit analysiert und Objekte (Autos, Schilder, Fußgänger usw.) eindeutig erkannt,\\nd.h. klassiﬁziert, werden müssen. Digitale Sprachassistenten wie bspw. Apples Siri oder\\nAmazons Alexa basieren auf den Fortschritten der Spracherkennung. Hierdurch wird\\neine neue Mensch-Maschine-Schnittstelle ermöglicht, wodurch in den nächsten Jahren\\nviele neue Anwendungen und Innovationen entstehen werden.\\nDas Ziel dieser Arbeit war es, typische Anwendungen durchzuführen, bei denen Künst-\\nliche Neuronale Netzwerke bzw. Techniken des Deep Learning zum Einsatz kommen,\\ndie dann als Basis in der Lehre und der angewandten Forschung an der Hochschule\\nfür Technik und Wirtschaft des Saarlandes (htw saar) dienen können. Zunächst wurden\\ndeshalb Grundlagen zu den wesentlichen Konzepten von KNN vorgestellt. Auf die biolo-\\ngischen Analogien wurde eingegangen und die Ideen zu den ersten Netzwerk-Modellen\\nund Lernverfahren inklusive deren Probleme wurden erläutern. Drei bedeutende KNN-\\nArchitekturen wurde jeweils ein eigenes Kapitel gewidmet. Das Multilayer Perceptron\\n(MLP) ist ein vollständig-vernetztes vorwärtsgekoppeltes KNN, welches mit dem po-\\npulären Backpropagation -Algorithmus trainiert werden kann. Lernen bedeutet, dass die\\nParameter des Netzwerks, also die Gewichte der Verbindungen und die Schwellenwerte\\nder Neuronen, angepasst werden. Hierzu werden die bewerteten Differenzen aus Netz-\\nfehlern und tatsächlichen Werten schichtweise rückwärts durch das Netzwerk propagiert.\\nDieses Lernverfahren gehört zum überwachten Lernen und basiert auf der Methode des\\nsteilsten Gradienten. Es handelt sich also um ein Optimierungsproblem mit seinen typi-\\nschen Problemen wie ﬂache Plateaus, Oszillationen, Steckenbleiben in lokalen Minima\\nusw. Netzwerke mit vielen Parametern in vielen verborgenen Schichten können durch\\ndas Training zu einer sehr guten Anpassung kommen, ggf. aber schlecht generalisieren,\\nwenn neue Daten verwendet werden. Man spricht dann von einer Überanpassung ( Over-\\nﬁtting ). Werden die Netzwerke immer größer und tiefer, dann kommt noch das Problem\\nder verschwindenden Gradienten hinzu und das Training gerät ins Stocken.\\n1078 Zusammenfassung und Ausblick\\nTiefe KNN werden verwendet, um große Datenmengen besser verarbeiten zu können.\\nUm die beschriebenen Probleme zu vermindern bzw. zu lösen, werden Techniken des\\nDeep Learning eingesetzt. Hierzu wurden u.a. neuartige KNN-Architekturen entwickelt,\\nNeuronen mit speziellen Aktivierungsfunktionen (ReLU) verwendet, graﬁsche Prozesso-\\nren (GPUs) zur schnellen Matrix-Multiplikation eingesetzt usw. Das Convolutional Neural\\nNetwork ( CNN )ist ein Netzwerk mit dem visuellen Cortex als biologisches Vorbild. Es\\nwird deshalb auch häuﬁg im Bereich Bilderkennung eingesetzt. Durch die Kombination\\nvon sogenannten Convolutional Layer und Pooling Layer werden die Bildinformationen\\nschrittweise abgetastet, um daraus Stufe für Stufe Features zu generieren. Im Fall der Ge-\\nsichtserkennung würden in der ersten Stufe zunächst Kanten im Bild identiﬁziert werden,\\nalso Bereiche in denen es große Helligkeitsunterschiede gibt. In der zweiten Stufe können\\nMerkmale wie Augen, Nase, Mund, Ohren usw. identiﬁziert werden. In der letzten Stufe\\nwerden dann die kompletten Gesichter analysiert. Das CNN generiert diese Features aller-\\ndings allein, d.h. ohne fremde, menschliche Hilfe. Erfolgreiche CNN-Architekturen wie\\nbspw. das LeNet-5, das AlexNet und das GoogleLeNet, die internationale Wettbewerbe\\ngewonnen haben, wurden exemplarisch vorgestellt.\\nRekurrente bzw. rekursive Netzwerke werden dagegen häuﬁg im Bereich Sprach- oder\\nTexterkennung eingesetzt. Das Recurrent Neural Network ( RNN )ist ein Netzwerk mit Rück-\\nkopplungen. In diesem werden die Signale nicht nur von den Eingabeneuronen über die\\nverdeckten Neuronen zu den Ausgabeneuronen transportiert, sondern auch in entgegen-\\ngesetzter Richtung. Das biologische Pendant ist der Neocortex mit den höheren Gehirn-\\nfunktionen wie Motorik oder Sprache, der auch eine wichtige Rolle für das Gedächtnis\\nspielt. Die Eingabedaten werden bei diesen Netzwerken sequenzweise verarbeitet, wobei\\neine Sequenz bspw. einen Satz aus Wörtern darstellen kann. Einheiten mit Verbindungen\\nauf sich selbst, also mit einfachen Rückkopplungen, lassen sich hinsichtlich dieser Sequenz\\nwie ein ausgerolltes Netzwerk betrachten und mit einem Backpropagation -Algorithmus\\ntrainieren, den man nun Backpropagation Through Time ( BPTT )nennt. Um auch hier den\\nProblemen der verschwindenden Gradienten entgegenzuwirken, wurden die Long Short-\\nTerm Memory ( LSTM )Einheiten entwickelt. Diese besitzen eine innere Struktur aus einer\\nZelle und drei Gates , um den Signalﬂuss zu steuern und eine Art Gedächtnis abzubilden.\\nEine Variante davon ist die Gated Recurrent Unit ( GRU ), die etwas einfacher aufgebaut ist,\\naber zu ähnlich erfolgreichen Leistungen fähig ist.\\nNach den ausführlichen Präsentationen dieser erfolgreicher KNN-Modelle, wurden 22\\naktuell verfügbare Open Source Softwarelösungen zum Thema Deep Learning ( DL)vorge-\\nstellt, die auf der Internet-Plattform GitHub vorhanden sind. Neben den Beschreibungen\\nwurden auch die auf GitHub gesammelten statistischen Daten analysiert. Die mit Abstand\\npopulärste DL-Bibliothek ist TensorFlow, die von Mitarbeitern des Google Brain Teams\\nentwickelt wurde. Mathematische Operationen mit Tensoren werden graphenbasiert for-\\nmuliert. Typische Operationen wie Matrix-Multiplikationen, die beim Lernverfahren aus-\\ngeführt werden müssen, lassen sich so einfach formulieren und durch das Verwenden von\\ngraﬁschen Prozessoren auch parallelisiert auf diesen sehr efﬁzient ausführen. TensorFlow\\nbenutzt hierfür bspw. CUDA von Nvidia. Die Bibliothek bietet zwar eine Schnittstelle\\nfür die Programmiersprache Python an, jedoch steht mit der High Level API Keras eine\\nErweiterung zur Verfügung, die sehr viele Bausteine enthält, die leicht eingesetzt und\\nwiederverwendet werden können. Als Backend könnte theoretisch auch Thenao benutzt\\nwerden. Diese von der Universität Montréal entwickelte Lösung wird aber seit Ende 2017\\nnicht mehr weiterentwickelt. Somit wurde die Kombination der Softwarepakete Tensor-\\nFlow und Keras verwendet, um drei typische DL-Anwendungen durchzuführen.\\n108Die erste Anwendung stammt aus dem Bereich Bilderkennung und Objektklassiﬁkation.\\nDie Datenbank MNIST enthält 70.000 Datensätze von Graustufen-Bildern im Format 28\\nx 28 Pixel zu handgeschriebenen Ziffern Null bis Neun. Das Ziel ist es also, ein Modell\\nzu trainieren, das die jeweilige Ziffer automatisch identiﬁzieren kann. Somit stellt diese\\nAnwendung einen Schritt im Prozess Optical Character Recognition ( OCR )dar. Zwei KNN\\nwurden hierzu mit 60.000 Bildern trainiert, ein klassisches MLP mit der Topologie 784-256-\\n128-10 und ein modernes CNN mit einer komplexeren Topologie. Das Training wurde\\nauf der Graﬁkkarte Geforce GTX 1080 Ti von Nvidia ausgeführt. Nach 12 Sekunden und\\n16 Epochen erreichte das MLP eine Genauigkeit von 98 % auf den 10.000 Bildern der\\nTestdaten. Das CNN kam sogar auf eine Genauigkeit von 99,35 %, wobei 20 Epochen in\\n41 Sekunden trainiert wurden. Das sind bereits hervorragende Klassiﬁkationsergebnisse.\\nAuch die zweite Anwendung ist dem Bereich Bilderkennung zuzuordnen. Diesmal stan-\\nden mit der CIFAR-10-Datenbank eine Sammlung von 60.000 Farb-Bildern im Format 32 x\\n32 Pixel zur Verfügung, auf denen zehn Klassen von Objekten zu erkennen sind: Flugzeu-\\nge, Autos, Vögel, Katzen, Rehe, Hunde, Frösche, Pferde, Schiffe und LKWs. Als Modell\\nwurde ein komplexes CNN mit 822.570 freien Parametern verwendet und 47 Epochen\\nin etwas über 6 Minuten trainiert. Die Genauigkeit betrug knapp 82 %. Das ist schon\\nsehr ordentlich, wenn man bedenkt, dass diese Anwendung viel komplizierter als das\\nErkennen der Ziffern ist.\\nAnwendung Nummer Drei ist eine Stimmungserkennung (engl. Sentiment Analysis ), d.h.\\nsie gehört zum Bereich Text Mining . Als Datenbasis werden 50.000 Nutzer-Kritiken zu\\nFilmen der Internetplattform IMDb verwendet. Diese unstrukturierten Text-Rezensionen\\nsind sehr eindeutig hinsichtlich einer positiven oder negativen Kritik. Letztendlich wird\\nbei dieser Analyse also auch wieder eine Klassiﬁkation durchgeführt, diesmal als binäre\\nKlassiﬁkation mit den zwei Klassen positiv und negativ. Zunächst mussten die Daten\\nvorverarbeitet werden. Die Wort-Sequenzen unterschiedlicher Länge der Rezensionen\\nenthielten bereits ganze Zahlen für die unterschiedlichen Wörter, die nach der auftre-\\ntenden Häuﬁgkeit sortiert waren. Jede Sequenz wurde zunächst auf 100 Wörter gekürzt\\nund dann auf einen Vektor der Dimension 16 abgebildet, der kontinuierliche Werte ent-\\nhält ( Word Embedding ). Es wurden dann vier verschiedene KNN mit 25.000 Rezensionen\\ntrainiert und dabei folgende Ergebnisse auf der Testmenge von ebenfalls 25.000 Kritiken\\nerzielt. Mit dem MLP der Topologie 256-16-1 konnte nach 38 Epochen bzw. 10 Sekunden\\neine Genauigkeit von 86 % erzielt werden. Das CNN schaffte nach 56 Epochen in knapp 30\\nSekunden 87 %. Für das Training der LSTM-Einheiten wurden bereits 3 Minuten benötigt,\\nwobei nach den 126 Epochen ebenfalls eine Genauigkeit von 87 % erreicht wurde. Schließ-\\nlich kam das GRU-Netzwerk in der Hälfte der Zeit und 72 Epochen auf eine Genauigkeit\\nvon 87,4 %.\\nInsgesamt kann festgestellt werden, dass mit nur wenigen Zeilen Python-Quelltext ver-\\nschiedene Anwendungen mit Hilfe von mehreren KNN bearbeitet werden konnten, d.h.\\ndie Modelle wurden konﬁguriert, trainiert und evaluiert. Das Training auf der High-End\\nGraﬁkkarte von Nvidia verlief schnell und die Modelle zeigten bereits gute Genauigkei-\\nten. Die Kombination aus TensorFlow und Keras ist somit eine gute Basis für den Einsatz\\nin der Lehre und der angewandten Forschung an der htw saar. Das Ziel dieser Arbeit\\nwurde also vollumfänglich erreicht.\\n1098 Zusammenfassung und Ausblick\\nDie bisher eingesetzten Netzwerke können sicherlich noch bezüglich der Genauigkeit auf\\nden Testdaten verbessert werden. Optimierungstechniken wie Kreuz-Validierung und\\nRegularisierung kamen bislang noch gar nicht zum Einsatz. Hinsichtlich der Topologie\\nund den Trainingseinstellungen lassen sich auch sogenannte Hyperparameterstudien\\ndurchführen, in denen automatisch nach den optimalen Parametern gesucht wird, wobei\\nbspw. die Gittersuche oder evolutionäre Algorithmen verwendet werden.\\nNeben den drei durchgeführten Beispielanwendungen aus den Bereichen Bild- und Tex-\\nterkennung könnten auch noch weitere Anwendungen untersucht werden, um das Wis-\\nsen zu vertiefen. Beispielsweise stellt die Data Science Plattform Kaggle in zahlreichen\\nWettbewerben Daten zur Verfügung, die sich mittels Techniken des Maschinenlernens be-\\narbeiten lassen. Betrachtet man die magischen Quadranten zum Thema Data Science und\\nMaschine Learning des Marktforschungs- und Beratungsunternehmens Gartner (siehe Abb.\\n8.1), so fällt auf, dass keine der in dieser Arbeit präsentierten Softwarelösungen dort zu\\nerkennen ist. Das liegt daran, dass in dieser Arbeit der Fokus auf die Themen KNN und\\nDeep Learning gelegt wurde und es in diesem Bereich momentan nur Softwarelösungen\\ngibt, die dem Anwender bzw. Data Scientist Python oder eine andere Programmiersprache\\nals Schnittstelle zur Verfügung stellen. Es muss also noch programmiert werden.\\nAbbildung 8.1: Gartners magische Quadranten zu Data Science und Maschine Learning [Ido+18]\\nDieData Science Software KNIME Analytics Plattform, die in den magischen Quadranten\\nals eine der führenden Lösungen angesehen wird, wird bereits seit mehreren Jahren in der\\nFakultät für Wirtschaftswissenschaften der htw saar in der Lehre erfolgreich eingesetzt.\\n110Diese Open Source Software, die an der Universität Konstanz entwickelt wurde, bietet die\\nMöglichkeit, graﬁsche Workﬂows zu erstellen, um sehr efﬁzient und benutzerfreundlich\\nAufgaben im Bereich Data Science zu bearbeiten (siehe Abb. 8.2). Der Anwender muss\\ndabei also nicht über Programmierkenntnisse verfügen. Mittlerweile gibt es auch zahl-\\nreiche Erweiterungen für KNIME, sogenannte Extensions , mit denen sich auch andere\\nSoftware-Bibliotheken einbinden und nutzen lassen. Zum Thema Deep Learning werden\\nErweiterungen zu DL4J und Keras angeboten [KNI18]. Diese Erweiterungen sollten zu-\\nkünftig im Hinblick auf den Einsatz in der Lehre intensiv getestet werden.\\nAbbildung 8.2: Workﬂow der KNIME Analytics Platform\\nIn den magischen Quadranten von Gartner fällt außerdem auf, dass die Lösung H2O.ai\\nals besonders visionär angesehen wird. In Kap. 6.6 wurde die DL-Bibliothek Deep Wa-\\ntervorgestellt, die als Erweiterung der H2O-Plattform entwickelt wurde. Als Backend zu\\nH2O lässt sich bspw. auch TensorFlow verwenden. Auch diese Softwarelösung ist inter-\\nessant und sollte ausgiebig getestet werden, um den möglichen Einsatz in der Lehre oder\\nangewandten Forschung zu evaluieren.\\nDas Thema Data Science wird derzeit an der htw saar in den folgenden Master-Modulen\\ngelehrt:\\n- Angewandte Methoden der Informationsbeschaffung (MMF-130)\\n- Angewandte Informatik (DFMMS-222)\\n- Big Data Analysis (MAMS-120)\\n- Data Science (MASCM-141)\\nAußerdem wäre dieses Thema und insbesondere Künstliche Neuronale Netzwerke und\\nDeep Learning aber auch für den Schwerpunkt Wirtschaftsinformatik im Bachelor-Studien-\\ngang Betriebswirtschaft interessant. Im 5. und 6. Semester vertiefen sich die Studierenden\\nin zwei von fünf Schwerpunkten. Vier Module der Vertiefung Wirtschaftsinformatik wer-\\nden derzeit angeboten. Data Science oder KI-Themen sind aber inhaltlich noch nicht Be-\\nstandteil. Zusätzlich könnten diese neuen Themen auch im Rahmen von Projektarbeiten\\n(Modul BBWL-622) oder Abschlussarbeiten durch Studierende bearbeitet werden.\\nProblematisch sind dabei allerdings mehrere Aspekte. Damit das Training von KNN efﬁzi-\\nent ausgeführt werden kann, wird eine High End Graﬁkkarte benötigt. Die in dieser Arbeit\\nverwendete Nvidia-Graﬁkkarte Geforce GTX 1080 Ti kostet ca. 750 Euro netto. Möchte\\nman also ein Labor der Fakultät für Wirtschaftswissenschaften (WiWi) damit ausstatten,\\n1118 Zusammenfassung und Ausblick\\nso wären dies Kosten in Höhe von ca. 30.000 Euro brutto, wenn man 34 Rechner betrachtet\\n(32 Studierenden-PCs, 1 Dozenten-PC, 1 Master-PC). Auf dem DL-Rechner, der in dieser\\nArbeit verwendet wurde, ist Ubuntu als Betriebssystem installiert. Zur Administration\\ndieses Systems fehlt aber in der Fakultät WiWi das notwendige Know-how. Auf fast allen\\nRechnern der Fakultät ist Windows als Betriebssystem installiert. Für diese Rechner gibt\\nes administrative Unterstützung durch das dezentrale IT-Service-Team der Fakultät. Der\\nEinsatz von CUDA, cuDNN, TensorFlow und Keras unter Windows muss noch getestet\\nwerden.\\nEin weiteres Problem besteht darin, dass Python als Programmiersprache benutzt werden\\nmuss. Dies kann bei den Studierenden aber nicht vorausgesetzt werden und Grundlagen\\nder Programmierung lassen sich auch nicht in kurzer Zeit vermitteln. Somit bleibt also\\nnur das Modul BBWL-622, um in einem ersten Pilotprojekt ausgewählten Studierenden\\ndes Schwerpunkts Wirtschaftsinformatik an diese aktuellen Themen heranzuführen. Im\\n5. Semester werden bereits Grundlagen der Programmierung vermittelt, allerdings am\\nBeispiel der objektorientierten Programmiersprache Java. Der Umstieg auf die Skriptspra-\\nche Python ist aber einfacher als Programmierkenntnisse von null aufzubauen. Für dieses\\nPilotprojekt bräuchte man auch nur einen Rechner mit der Nvidia-Graﬁkkarte. D.h. die\\nKosten halten sich in überschaubare Grenzen. Die Installation und Konﬁguration des\\nRechners könnten die Studierenden sogar selbst bewerkstelligen.\\nEine andere Verwendungsmöglichkeit der TensorFlow-Keras-Kombination ist der Einsatz\\nin der angewandten Forschung. Drei Ideen werden im Folgenden kurz skizziert.\\nSoziale Netzwerke: Hasskommentare Rezensionen, Kritiken oder Meinungsäußerun-\\ngen ﬁndet man im Internet bzw. Web 2.0 auf ganz verschiedenen Plattformen: Im sozialen\\nNetzwerk Facebook oder beim Kurznachrichtendienst Twitter schreiben viele Nutzer ihre\\npersönliche Meinung in Nachrichten und Kommentaren, häuﬁg auch zu gesellschaftspo-\\nlitischen Themen. Dabei wird manchmal auch eine Grenze überschritten und Hetze oder\\ngefälschte Meldungen ( Fake News ) verbreitet. Nach dem Netzwerkdurchsetzungsgesetz\\n(NetzDG), welches in Deutschland seit dem 1. Oktober 2017 in Kraft getreten ist, müssen\\ndie großen sozialen Netzwerke mit mehr als zwei Millionen registrierten Nutzern, offen-\\nsichtlich rechtswidrige Inhalte innerhalb von 24 Stunden nach Eingang einer Beschwerde\\nlöschen oder sperren.\\nEine spezielle Form der Sentiment Analysis könnte also auf Kommentare in sozialen Netz-\\nwerken angewendet werden, um Hasskommentare automatisch zu identiﬁzieren. Ein\\nKlassiﬁzierer als selbstlernendes System könnte also im ersten Schritt dabei helfen, um zu\\nerkennen, ob es sich bei dem Text, um einen Hasskommentar handelt oder nicht. Hierzu\\nmüssten zunächst viele Beispiele und Gegenbeispiele gesammelt und manuell klassiﬁ-\\nziert werden, um diese dann als Trainings- und Testdaten verwenden zu können. Doch\\nwas genau ist ein Hasskommentar? Anders formuliert: An wann wird aus einer frei-\\nen Meinungsäußerung ein Hasskommentar? Hier gibt es keine einheitliche Deﬁnition\\noder Richtlinie. Nehmen wir aber an, diese Probleme lassen sich lösen, und genügend\\nDatensetze stehen zum Trainieren bereit, dann könnten ähnliche Verfahren wie bei der\\nKlassiﬁkation der positiven und negativen Filmkritiken der IMDb angewendet werden.\\nDer Gründer von Facebook, Mark Zuckerberg, hat im April 2018 bei seiner Anhörung\\nvor dem US-Kongress zum Datenschutzskandal in Aussicht gestellt, dass die Künstliche\\nIntelligenz in zehn Jahren in der Lage ist, Hate Speech bei Facebook zu erkennen und auto-\\n112matisch zu löschen [Bor18]. Auch Google arbeitet an solchen Lösungen und treibt diese\\nEntwicklungen mit seinem Projekt Perspective voran, um sogenannte toxische Nachrichten\\nzu identiﬁzieren [Goo18]. Eine große Schwierigkeit ist aber immer noch Sarkasmus in den\\nNachrichten, wodurch der die jeweilige Aussage relativiert wird. Ob der Klassiﬁzierer\\ndann auch direkt die potenziellen Kandidaten von Hasskommentaren löschen soll, ist\\neine andere Frage, die geklärt werden muss. Kein Klassiﬁzierer arbeitet perfekt, d.h. es\\nwird immer einige Fehler geben. Entweder wird ein Hasskommentar nicht erkannt oder\\nein normaler Kommentar wird versehentlich als Hasskommentar klassiﬁziert. Wenn die-\\nser automatisch vom KI-System entfernt wird, dann käme dies einer Zensur gleich und\\ngefährdet wiederum die freie Meinungsäußerung in einer Demokratie. Dieses spannen-\\nde Forschungsvorhaben ist also sehr komplex: Es ist technisch sehr anspruchsvoll und\\ngesellschaftspolitisch äußerst brisant.\\nProduktion und Logistik: Predictive Maintenance In der vorausschauenden Wartung\\n(engl. Predictive Maintenance ) geht es darum, vorherzusagen, wann eine Maschine ausfal-\\nlen wird. Das kann bspw. ein Roboter in der Produktion, ein LKW als Transportmittel\\nauf der Straße oder ein Windrad, welches Energie erzeugt, sein. Es gibt also sehr unter-\\nschiedliche Typen von Maschinen, die ausfallen könnten. Durch einen Ausfall kann es\\nim ersten Beispiel zu einem völligen Stillstand der Produktion kommen, im zweiten Bei-\\nspiel werden die Güter nicht rechtzeitig zum Empfänger geliefert und im dritten Beispiel\\nkönnte ein Stromengpass die Folge sein. Mit diesen Maschinenausfällen sind also gewisse\\nRisiken verbunden, die gesteuert werden müssen. Damit Maschinenausfälle überhaupt\\nprognostiziert werden können, müssen zunächst Daten gesammelt werden. Die Sensoren\\nder Maschinen liefern diese Daten. Des Weiteren gibt es Wartungsprotokolle, in denen\\ndokumentiert ist, wann es zu Störungen oder Ausfällen gekommen ist.\\nPrinzipiell gibt es zwei unterschiedliche Ansätze, ein Modell zu erstellen: Den White Box\\nund den Black Box Ansatz. Im ersten Fall wird ein Strukturgleichungsmodell aufgestellt,\\nd.h. kausale Zusammenhänge des Systems werden modelliert. Diese basieren auf phy-\\nsikalische Gesetzmäßigkeiten, also wird z.B. das Schwingungs- und Vibrationsverhalten\\neiner Maschine abgebildet. Anschließend werden Simulationen zum kritischen Verhalten\\nder Maschine auf Basis dieses Modells durchgeführt. Der Nachteil dieses Ansatzes ist,\\ndass für jede Maschine ein eigenes Modell erstellt werden muss, da andere physikalische\\nGesetzmäßigkeiten berücksichtigt werden müssen. Ein weiteres Problem besteht darin,\\ndass Maschinen nicht nur durch Überbeanspruchung und Verschleiß ausfallen können,\\nsondern weil der Mensch sie falsch bedient – absichtlich oder unwissend. Solche Gründe\\nlassen sich in diesen Modellen nicht abbilden.\\nDer Black Box Ansatz benötigt dagegen nur die Eingabedaten (z.B. Sensordaten) und\\ndie Ausgabedaten (z.B. Maschinenzustand). Mit Hilfe des maschinellen Lernens wird\\nnun das Verhalten der Maschine nachgebildet, ohne die innere Struktur und die physi-\\nkalischen Gesetzmäßigkeiten zu kennen. Auch menschliche Anwendungsfehler werden\\nindirekt durch die Eingabedaten erfasst und sind im Modell berücksichtigt. Insbesondere\\nKNN und Deep Learning können in diesen Anwendungen eingesetzt werden. Predictive\\nMaintenance lässt sich sogar als Klassiﬁkationsaufgabe formulieren: Fällt die Maschine\\nin der nächsten aus oder nicht? In der Fakultät für Wirtschaftswissenschaften der htw\\nsaar verfügen die Kollegen des Clusters Logistik (Prof. Dr. Thomas Bousonville, Prof. Dr.\\nSteffen Hütter, Prof. Dr. Thomas Korne, Prof. Dr. Teresa Melo) über das entsprechende\\nFachwissen in diesem Bereich. Somit wäre eine Zusammenarbeit mit diesen Kollegen in\\nder angewandten Forschung zu diesem Thema möglich.\\n1138 Zusammenfassung und Ausblick\\nWirtschaftsinformatik: IT-Security IT-Sicherheit (engl. IT-Security ) nimmt in einer di-\\ngitalen Welt einen sehr hohen Stellenwert ein. Eine wichtige Aufgabe ist es, potenzielle\\nHacker-Angriffe auf ein System zu erkennen, um schnell Gegenmaßnahmen einleiten zu\\nkönnen. Ein sogenanntes Intrusion Detection System ( IDS)wird eingesetzt, um die Log-\\nDaten des Systems und des Netzwerkverkehrs zu analysieren, d.h. nach Mustern in diesen\\nDaten zu suchen, die auf einen Angriff hindeuten. Letztendlich ist es also wieder eine bi-\\nnäre Klassiﬁkation: Angriff oder kein Angriff. Mit dem Einsatz von KNN und Techniken\\ndesDeep Learning können selbstlernende IDS auf graﬁschen Prozessoren implementiert\\nwerden, die in Echtzeit ein System analysieren und ggf. Warnungen (engl. Alerts ) verschi-\\ncken. Fachexperte für solche Systeme ist der Kollege Prof. Dr. Christian Liebig von der\\nFakultät für Wirtschaftswissenschaften, der ebenfalls im Schwerpunkt Wirtschaftsinfor-\\nmatik in der Lehre eingesetzt ist. Ein gemeinsames Forschungsprojekt zu diesem Thema\\nkönnte insbes. auch den mittelständischen Unternehmen im Saarland helfen, neben einer\\nFirewall zukünftig noch eine weitere effektive IT-Sicherheitslösung einzusetzen.\\n114Quellenverzeichnis\\n[AES85] D. H. Ackley, Hinton G. E. und T. J. Sejnowski. „A Learning Algorithm for\\nBoltzmann Machines“. In: Cognitive Science 9 (1985), S. 147–169.\\n[AR+16] R. Al-Rfou, G. Alain, A. Almahairi, C. Angermueller u. a. „Theano: A Python\\nFramework for Fast Computation of Mathematical Expressions“. In: CoRR\\nabs/1605.02688 (2016). U R L : http://arxiv.org/abs/1605.02688.\\n[And70] J. A. Anderson. „Two Models for Memory Organization“. In: Mathematical\\nBioscience 8 (1970), S. 137–160.\\n[And72] J. A. Anderson. „A Simple Neural Network Generating an Interactive Me-\\nmory“. In: Mathematical Bioscience 14 (1972), S. 197–220.\\n[Apa18a] Apache. Deep Learning – The Straight Dope . 2018. U R L : http://gluon.mxnet.io\\n(besucht am 08. 03. 2018).\\n[Apa18b] Apache. Gluon . 2018. U R L : https : / / mxnet . incubator. apache . org / api /\\npython/gluon/gluon.html (besucht am 08. 03. 2018).\\n[Apa18c] Apache. MXNet: A Flexible and Efﬁcient Library for Deep Learning . 2018. U R L :\\nhttps://mxnet.apache.org (besucht am 08. 03. 2018).\\n[Apa18d] Apache. SINGA . 2018. U R L : https://singa.apache.org (besucht am 08. 03. 2018).\\n[Aph15a] Aphex34. Max Pooling with 2x2 Filter and Stride = 2 . 16. Dez. 2015. U R L :\\nhttps://commons.wikimedia.org/wiki/File:Max_pooling.png (besucht am\\n21. 04. 2018).\\n[Aph15b] Aphex34. Typical CNN Architecture . 16. Dez. 2015. U R L : https://commons.\\nwikimedia.org/wiki/File:Typical_cnn.png (besucht am 21. 04. 2018).\\n[Art18] Artelnics. OpenNN: High Performance library for Advanced Analytics . 2018.\\nU R L : http://www.opennn.net (besucht am 08. 03. 2018).\\n[BLG+18] F. Bastien, P . Lamblin, I. Goodfellow u. a. Theano . 2018. U R L : https://github.\\ncom/Theano/Theano (besucht am 08. 03. 2018).\\n[Bai18] Baidu. P Arallel Distributed Deep LEarning . 2018. U R L : http://www.paddlepaddle.\\norg (besucht am 08. 03. 2018).\\n[Bas+17] C. Basoglu u. a. The Microsoft Cognitive Toolkit . 22. Jan. 2017. U R L : https :\\n//docs.microsoft.com/en-us/cognitive-toolkit (besucht am 08. 03. 2018).\\n[Bat+18] E. Battenberg, S. Dieleman, D. Nouri, E. Olson, A. van den Oord, C. Raf-\\nfel, J. Schlüter und S. K. Sønderby. Lasagne . 2018. U R L : http : / / lasagne .\\nreadthedocs.io (besucht am 08. 03. 2018).\\n[Bea15] F. Beaufays. The Neural Networks behind Google Voice Transcription . 11. Aug.\\n2015. U R L : https : / / research . googleblog . com / 2015 / 08 / the - neural -\\nnetworks-behind-google-voice.html (besucht am 28. 04. 2018).\\n[Ben+17] Y. Bengio, F. Bastien, P . Lamblin, I. Goodfellow, R. Pascanu, N. Léonard u. a.\\nTheano . 21. Nov. 2017. U R L : http://deeplearning.net/software/theano\\n(besucht am 08. 03. 2018).\\n115Quellenverzeichnis\\n[Bla+18] A. Black, A. Gibson, M. Warrick, M. Pumperla, J. Long, S. Audet, E. Wright\\nu. a. Deep Learning for Java, Scala & Clojure on Hadoop & Spark With GPUs .\\n2018. U R L : https://github.com/deeplearning4j/deeplearning4j (besucht\\nam 08. 03. 2018).\\n[Bor18] T. Borgböhmer. Wie künstliche Intelligenz gegen Hass im Netz funktioniert: eine\\nmoderne Technik mit vielen Fragezeichen . 13. Apr. 2018. U R L : http://meedia.\\nde / 2018 / 04 / 13 / wie - kuenstliche - intelligenz - gegen - hass - im - netz -\\nfunktioniert-eine-moderne-technik-mit-vielen-fragezeichen (besucht am\\n04. 05. 2018).\\n[Bri18] S. Brin. 2017 Founders’ Letter . 27. Apr. 2018. U R L : https://abc.xyz/investor/\\nfounders-letters/2017/index.html (besucht am 01. 05. 2018).\\n[Bro16a] J. Brownlee. Predict Sentiment From Movie Reviews Using Deep Learning . 4. Juli\\n2016. U R L : https://machinelearningmastery.com/predict-sentiment-movie-\\nreviews-using-deep-learning (besucht am 17. 04. 2018).\\n[Bro16b] J. Brownlee. Sequence Classiﬁcation with LSTM Recurrent Neural Networks in\\nPython with Keras . 26. Juli 2016. U R L : https://machinelearningmastery.com/\\nsequence - classiﬁcation - lstm - recurrent - neural - networks - python - keras\\n(besucht am 17. 04. 2018).\\n[Bro17] A Bronshtein. Train/Test Split and Cross Validation in Python . 17. Mai 2017.\\nU R L : https://towardsdatascience.com/train-test-split-and-cross-validation-\\nin-python-80b61beca4b6 (besucht am 04. 05. 2018).\\n[CMS12] D. C. Ciresan, U. Meier und J. Schmidhuber. „Multi-Column Deep Neural\\nNetworks for Image Classiﬁcation“. In: CoRR abs/1202.2745 (2012). U R L :\\nhttps://arxiv.org/abs/1202.2745.\\n[CRZ+18] F. Chollet, F. Rahman, O. Zabluda u. a. Keras: Deep Learning for Humans . 2018.\\nU R L : https://github.com/keras-team/keras (besucht am 08. 03. 2018).\\n[CYL+14] C.-Y. Cheng-Yuan Liou, W.-C. Cheng, J.-W. Liou und D.-R. Liou. „Autoenco-\\nder for Words“. In: Neurocomputing 139 (2014), S. 84–96.\\n[Can+18] A. Candel, Q. Kou, M. Stensmo, M. Dymczyk, F. Milo u. a. Deepwater: Deep\\nLearning in H2O using Native GPU Backends . 2018. U R L : https://github.com/\\nh2oai/deepwater (besucht am 08. 03. 2018).\\n[Che+15] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang und\\nZ. Zhang. „MXNet: A Flexible and Efﬁcient Machine Learning Library for\\nHeterogeneous Distributed Systems“. In: CoRR abs/1512.01274 (2015). U R L :\\nhttp://arxiv.org/abs/1512.01274.\\n[Chi+18] S. Chintala, R. Collobert, K. Kavukcuoglu, K. Zhou, C. Farabet u. a. Torch .\\n2018. U R L : https://github.com/torch/torch7 (besucht am 08. 03. 2018).\\n[Cho+14] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk\\nund Y. Bengio. „Learning Phrase Representations using RNN Encoder–Decoder\\nfor Statistical Machine Translation“. In: CoRR abs/1406.1078 (2014). U R L :\\nhttps://arxiv.org/abs/1406.1078.\\n[Cho+18a] F. Chollet u. a. Datasets: IMDb Movie Reviews Sentiment Classiﬁcation . 2018.\\nU R L : https://keras.io/datasets (besucht am 17. 04. 2018).\\n[Cho+18b] F. Chollet u. a. Keras: The Python Deep Learning library . 2018. U R L : https :\\n//keras.io (besucht am 08. 03. 2018).\\n116Quellenverzeichnis\\n[Chu16] K. Chung. Generating Recommendations at Amazon Scale with Apache Spark and\\nAmazon DSSTNE . 9. Juli 2016. U R L : https://aws.amazon.com/de/blogs/\\nbig- data/generating- recommendations- at- amazon- scale- with- apache-\\nspark-and-amazon-dsstne (besucht am 08. 03. 2018).\\n[Cir+10] D. C. Ciresan, U. Meier, L. M. Gambardella und J. Schmidhuber. „Deep Big\\nSimple Neural Nets Excel on Handwritten Digit Recognition“. In: CoRR\\nabs/1003.0358 (2010). U R L : https://arxiv.org/abs/1003.0358.\\n[Col+11] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu und P . Kuksa.\\n„Natural Language Processing (almost) from Scratch“. In: CoRR abs/1103.0398\\n(2011). U R L : https://arxiv.org/abs/1103.0398.\\n[Col+18] R. Collobert, C. Farabet, K. Kavukcuoglu, S. Chintala u. a. Torch: A Scientiﬁc\\nComputing Framework for LuaJIT . 2018. U R L : http://torch.ch (besucht am\\n08. 03. 2018).\\n[DB+18] A. Damien, W. Ballard u. a. Deep Learning Library Featuring a Higher-Level API\\nfor TensorFlow . 2018. U R L : https://github.com/tﬂearn/tﬂearn (besucht am\\n08. 03. 2018).\\n[Dam+18] A. Damien u. a. TFLearn . 2018. U R L : http://tﬂearn.org (besucht am 08. 03. 2018).\\n[Dav14] S. Davies. Hawking warns on Rise of the Machines . 2. Dez. 2014. U R L : https:\\n//www.ft.com/content/9943bee8-7a25-11e4-8958-00144feabdc0 (besucht\\nam 01. 05. 2018).\\n[Efr16] A. Efrati. Apples Machines can Learn too . 13. Juni 2016. U R L : https://www.\\ntheinformation.com/articles/apples-machines-can-learn-too (besucht am\\n28. 04. 2018).\\n[Elm90] J. L. Elman. „Finding Structure in Time“. In: Cognitive Science 14 (1990), S. 179–\\n211. U R L : https://crl.ucsd.edu/~elman/Papers/fsit.pdf.\\n[Ert16] W. Ertel. Grundkurs Künstliche Intelligenz . 4. Auﬂ. Wiesbaden: Springer View-\\neg Fachmedien, 2016.\\n[FPS91] W. J. Frawley und G. Piatetsky-Shapiro. Knowledge Discovery in Databases .\\n1. Auﬂ. Menlo Park, California, USA: The MIT Press, 1991.\\n[Fay+96] U. M. Fayyad, G. Piatetsky-Shapiro, P . Smyth und R. Uthurusamy. Advances\\nin Knowledge Discovery and Data Mining . 1. Auﬂ. Menlo Park, California, USA:\\nThe MIT Press, 1996.\\n[Fuk80] K. Fukushima. „Neocognitron: A Self-Organized Neural Network Model\\nfor a Mechanism of Pattern Recognition Unaffected by Shift in Position“. In:\\nBiological Cybernetics 36 (1980), S. 193–202.\\n[Fü96] K. Füser. Neuronale Netze in der Finanzwirtschaft . 1. Auﬂ. Wiesbaden: Gabler,\\n1996.\\n[GB10] X. Glorot und Y. Bengio. „Understanding the Difﬁculty of Training Deep\\nFeedforward Neural Networks“. In: Proceedings of the Thirteenth International\\nConference on Artiﬁcial Intelligence and Statistics . Hrsg. von Y. W. Teh und M.\\nTitterington. Bd. 9. Proceedings of Machine Learning Research. Chia Laguna\\nResort, Sardinia, Italy: PMLR, 2010, S. 249–256. U R L : http://proceedings.\\nmlr.press/v9/glorot10a/glorot10a.pdf.\\n[GBC16] I. Goodfellow, Y. Bengio und A. Courville. Deep Learning . 1. Auﬂ. Cambridge,\\nMassachusettts, USA: The MIT Press, 2016.\\n117Quellenverzeichnis\\n[GJ79] M. Garey und D. Johnson. Computers and Intractability: A Guide to the Theory\\nof NP-Completeness . 1. Auﬂ. New York: Freeman, 1979.\\n[GNP+18] A. Gibson, C. Nicholson, J. Patterson u. a. DL4J: Deep Learning for Java . 2018.\\nU R L : https://deeplearning4j.org (besucht am 08. 03. 2018).\\n[GS00] F. Gers und J. Schmidhuber. „Recurrent Nets that Time and Count“. In: Procee-\\ndings of the International Joint Conference on Neural Networks . Bd. 3. Como, Italy,\\nFeb. 2000, S. 189–194. U R L : ftp://ftp.idsia.ch/pub/juergen/TimeCount-\\nIJCNN2000.pdf.\\n[GSC99] F. A. Gers, J. Schmidhuber und F. Cummins. „Learning to Forget: Continual\\nPrediction with LSTM“. In: Neural Computation 12 (1999), S. 2451–2471.\\n[Git18] GitHub. GitHub . 2018. U R L : https://github.com (besucht am 08. 03. 2018).\\n[Gom+18] F. P . Gomez u. a. OpenNN: Open Neural Networks Library . 2018. U R L : https:\\n//github.com/Artelnics/OpenNN (besucht am 08. 03. 2018).\\n[Goo+14] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,\\nA. Courville und Y. Bengio. „Generative Adversarial Networks“. In: CoRR\\nabs/1406.2661 (2014). U R L : https://arxiv.org/abs/1406.2661.\\n[Goo18] Google. Perspective . 2018. U R L : http : / / perspectiveapi . com (besucht am\\n04. 05. 2018).\\n[Gre+15] K. Greff, R. K. Srivastava, J. Koutník, B. R. Steunebrink und J. Schmidhuber.\\n„LSTM: A Search Space Odyssey“. In: CoRR abs/1503.04069 (2015). U R L :\\nhttps://arxiv.org/abs/1503.04069.\\n[Gro76] S. Grossberg. „Adaptive Pattern Classiﬁcation and Universal Recording I +\\nII“. In: Biological Cybernetics 23 (1976), S. 187–202.\\n[Gé17] Aurélien Géron. Hands -On Machine Learning with Scikit -Learn and TensorFlow .\\n1. Auﬂ. Sebastopol (CA), USA: O’Reilly, 2017.\\n[HFZ92] F. Herget, W. Finnoff und H. G. Zimmermann. A Comaparison of Weight Elimi-\\nnation Methods for Reducing Complexity in Neural Networks . Techn. Ber. Mün-\\nchen: Siemens AG, Corporate Research und Development, 1992.\\n[HN87a] R. Hecht-Nielsen. „Counterpropagation Networks“. In: Applied Optics 26\\n(1987), S. 4979–4984.\\n[HN87b] R. Hecht-Nielsen. „Kolmogorov’s Mapping Neural Network Existence Theo-\\nrem“. In: Proceedings of the First IEEE International Conference on Neural Net-\\nworks . Bd. III. San Diego (CA), 1987, S. 11–14.\\n[HOT16] G. E. Hinton, S. Osindero und Y.-W. Teh. „A Fast Learning Algorithm for\\nDeep Belief Nets“. In: Neural Computation 18 (2016), S. 1527–1554. U R L : https:\\n//arxiv.org/abs/1211.5063.\\n[HS97] S. Hochreiter und J. Schmidhuber. „Long Short-Term Memory“. In: Neural\\nComputaion 9.8 (1997), S. 1735–1780. U R L : http : / / www. bioinf . jku . at /\\npublications/older/2604.pdf.\\n[HSS14] G. E. Hinton, N. Srivastava und K. Swersky. Neural Networks for Machine\\nLearning – Lecture 6a: Overview of Mini-Batch Gradient Descent . 6. Feb. 2014.\\nU R L : https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_\\nlec6.pdf (besucht am 28. 04. 2018).\\n[HSS17] J. Hu, L. Shen und G. Sun. „Squeeze-and-Excitation Networks“. In: CoRR\\nabs/1709.01507 (2017). U R L : https://arxiv.org/abs/1709.01507.\\n118Quellenverzeichnis\\n[Hah+00] R. H. R. Hahnloser, R. Sarpeshkar, M. A. Mahowald, R. J. Douglas und H. S.\\nSeung. „Digital Selection and Analogue Ampliﬁcation Coexist in a Cortex-\\ninspired Silicon Circuit“. In: Nature 405 (2000), S. 947–951. U R L : http://www.\\nbioguider.com/ebook/biology/pdf/hahnloser_n947.pdf.\\n[He+15] K. He, X. Zhang, S. Ren und J. Sun. „Deep Residual Learning for Image\\nRecognition“. In: CoRR abs/1512.03385 (2015). U R L : https://arxiv.org/abs/\\n1512.03385.\\n[Heb49] D. O. Hebb. The Organization of Behavior . 1. Auﬂ. New York: John Wiley &\\nSons, 1949.\\n[Hoc91] S. Hochreiter. „Untersuchungen zu dynamischen neuronalen Netzen“. Di-\\nplomarbeit. München: Institut für Informatik, Technische Universität Mün-\\nchen, 1991.\\n[Hop82] J. J. Hopﬁeld. „Neural Networks and Physical Systems with Emergent Col-\\nlective Computational Abilities“. In: Proceedings of the National Academy of\\nSciences 79 (1982), S. 2554–2558.\\n[IMD18a] IMDb. About IMDb . 2018. U R L : https://www.imdb.com/pressroom/about\\n(besucht am 01. 05. 2018).\\n[IMD18b] IMDb. Statistics . 2018. U R L : https://www.imdb.com/pressroom/stats\\n(besucht am 01. 05. 2018).\\n[IS15] S. Ioffe und C. Szegedy. „Batch Normalization: Accelerating Deep Network\\nTraining by Reducing Internal Covariate Shift“. In: CoRR abs/1502.03167\\n(2015). U R L : https://arxiv.org/abs/1502.03167.\\n[Ido+18] C. J. Idoine, P . Krensky, E. Brethenoux, J. Hare, S. Sicular und S. Vashisth.\\nMagic Quadrant for Data Science and Machine-Learning Platforms . 22. Feb. 2018.\\nU R L : https://www.gartner.com/doc/reprints?id=1-4RMUF5K&ct=180222\\n(besucht am 04. 05. 2018).\\n[Ima14] ImageNet. ImageNet Large Scale Visual Recognition Challenge 2012 . 2. Sep. 2014.\\nU R L : http://www.image-net.org/challenges/LSVRC/2012 (besucht am\\n21. 04. 2018).\\n[Ima16] ImageNet. ImageNet Large Scale Visual Recognition Challenge 2016 . 26. Sep.\\n2016. U R L : http://www.image-net.org/challenges/LSVRC/2016 (besucht\\nam 21. 04. 2018).\\n[Ima17] ImageNet. ImageNet Large Scale Visual Recognition Challenge 2017 . 26. Juli 2017.\\nU R L : http://www.image-net.org/challenges/LSVRC/2017 (besucht am\\n21. 04. 2018).\\n[JP+18] Rejith J., T. Penman u. a. Deep Scalable Sparse Tensor Network Engine (DSSTNE)\\n– An Amazon Developed Library for Building Deep Learning (DL) Machine Lear-\\nning (ML) Models . 2018. U R L : https://github.com/amzn/amazon-dsstne\\n(besucht am 08. 03. 2018).\\n[JS18] Y. Jia und E. Shelhamer. Caffe . 2018. U R L : http://caffe.berkeleyvision.org\\n(besucht am 08. 03. 2018).\\n[JZS15] R. Jozefowicz, W. Zaremba und I. Sutskever. „An Empirical Exploration of\\nRecurrent Network Architectures“. In: Proceedings of the 32Nd International\\nConference on International Conference on Machine Learning . Bd. 37. ICML’15.\\nLille, France: JMLR.org, 2015, S. 2342–2350. U R L : http://proceedings.mlr.\\npress/v37/jozefowicz15.pdf.\\n119Quellenverzeichnis\\n[Jia+14] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadar-\\nrama und T. Darrell. „Caffe: Convolutional Architecture for Fast Feature\\nEmbedding“. In: CoRR abs/1408.5093 (2014). U R L : http://arxiv.org/abs/\\n1408.5093.\\n[Jia+18] Y. Jia, B. Wasti, P . Noordhuis, L. Yeager, S. Layton, D. Dzhulgakov u. a. Caffe2\\n– A Lightweight, Modular, and Scalable Deep Learning Framework . 2018. U R L :\\nhttps://github.com/caffe2/caffe2 (besucht am 08. 03. 2018).\\n[Jor86] M. I. Jordan. Serial Order: A Parallel Distributed Processing Approach . Techn. Ber.\\nICS Report 8604. San Diego (CA): Institute for Cognitive Science, University\\nof California, 1986.\\n[Jor97] M. I. Jordan. „Serial Order: A Parallel Distributed Processing Approach“. In:\\nAdvances in Psychology 121 (1997), S. 471–495.\\n[KB14] D. P . Kingma und J. Ba. „Adam: A Method for Stochastic Optimization“. In:\\nCoRR abs/1412.6980 (2014). U R L : https://arxiv.org/abs/1412.6980.\\n[KNI18] KNIME. Deep Learning . 2018. U R L : https://www.knime.com/nodeguide/\\nanalytics/deep-learning (besucht am 04. 05. 2018).\\n[KSH12] A. Krizhevsky, I. Sutskever und G. E. Hinton. „ImageNet Classiﬁcation with\\nDeep Convolutional Neural Networks“. In: Advances in Neural Information\\nProcessing Systems 25 . Hrsg. von F. Pereira, C. J. C. Burges, L. Bottou und K. Q.\\nWeinberger. Curran Associates, Inc., 2012, S. 1097–1105. U R L : http://papers.\\nnips.cc/paper/4824- imagenet- classiﬁcation- with- deep- convolutional-\\nneural-networks.pdf.\\n[Kag18a] Kaggle. CIFAR-10 . 2018. U R L : https://www.kaggle.com/c/cifar-10 (besucht\\nam 17. 04. 2018).\\n[Kag18b] Kaggle. Digit Recognizer: Learn Computer Vision Fundamentals with the Famous\\nMNIST Data . 2018. U R L : https://www.kaggle.com/c/digit- recognizer\\n(besucht am 17. 04. 2018).\\n[Kag18c] Kaggle. Sentiment Analysis on IMDb Movie Reviews . 2018. U R L : https://www.\\nkaggle.com/c/sentiment-analysis-on-imdb-movie-reviews (besucht am\\n17. 04. 2018).\\n[Kag18d] Kaggle. The Home of Data Science & Machine Learning . 2018. U R L : https://\\nwww.kaggle.com (besucht am 21. 04. 2018).\\n[Kar15] A. Karpathy. The Unreasonable Effectiveness of Recurrent Neural Networks . 21. Mai\\n2015. U R L : http://karpathy.github.io/2015/05/21/rnn-effectiveness (be-\\nsucht am 24. 04. 2018).\\n[Kar18] A. Karpathy. CS231n: Convolutional Neural Networks for Visual Recognition .\\n2018. U R L : http : / / cs231n . github . io / neural - networks - 3 (besucht am\\n28. 04. 2018).\\n[Kha16] P . Khaitan. Chat Smarter with Allo . 18. Mai 2016. U R L : https : / / research .\\ngoogleblog . com / 2016 / 05 / chat - smarter - with - allo . html (besucht am\\n28. 04. 2018).\\n[Kin+18] D. E. King u. a. Dlib: A Toolkit for Making Real World Machine Learning and Data\\nAnalysis Applications in C++ . 2018. U R L : https://github.com/davisking/dlib\\n(besucht am 08. 03. 2018).\\n120Quellenverzeichnis\\n[Kin09] D. E. King. „Dlib-ml: A Machine Learning Toolkit“. In: Journal of Machine\\nLearning Research 10 (2009), S. 1755–1758. U R L : http : / / www. jmlr. org /\\npapers/volume10/king09a/king09a.pdf.\\n[Kin18] D. E. King. Dlib C++ Library . 22. Jan. 2018. U R L : http://dlib.net (besucht am\\n08. 03. 2018).\\n[Koh72] T. Kohonen. „Correlation Matrix Memories“. In: IEEE Transactions on Compu-\\nters21 (1972), S. 353–359.\\n[Koh82] T. Kohonen. „Self-Organized Formation of Topologically Correct Feature\\nMaps“. In: Biological Cybernetics 43 (1982), S. 59–69.\\n[Kol57] A. N. Kolmogorov. „On the Representation of Continuous Functions of Ma-\\nny Variables by Superposition of Continuous Functions of One Variable and\\nAddition“. In: Doklady Akademii Nauk SSR 114 (1957), S. 953–956.\\n[Kos87] B. Kosko. „Adaptive Bidirectional Associative Memories“. In: Applied Optics\\n26 (1987), S. 4947–4960.\\n[Kri09] A. Krizhevsky. „Learning Multiple Layers of Features from Tiny Images“.\\nDiplomarbeit. Toronto, Kanada: Department of Computer Science, Univer-\\nsity of Toronto, 2009. U R L : http://www.cs.toronto.edu/~kriz/learning-\\nfeatures-2009-TR.pdf.\\n[Kri10] A. Krizhevsky. Convolutional Deep Belief Networks on CIFAR-10 . Techn. Ber. Au-\\ngust. Toronto, Kanada: Department of Computer Science, 2010. U R L : https:\\n//www.cs.toronto.edu/~kriz/conv-cifar10-aug2010.pdf.\\n[Kri18] A. Krizhevsky. The CIFAR-10 Dataset . 2018. U R L : http://www.cs.toronto.\\nedu/~kriz/cifar.html (besucht am 17. 04. 2018).\\n[Kru+15] R. Kruse, C. Borgelt, C. Braune, F. Klawonn, C. Moewes und M. Steinbrecher.\\nComputational Intelligence - Eine methodische Einführung in Künstliche Neuro-\\nnale Netze, Evolutionäre Algorithmen, Fuzzy-Systeme und Bayes-Netze . 2. Auﬂ.\\nWiesbaden: Springer Vieweg Fachmedien, 2015.\\n[Kur12] R. Kurzweil. How to Create a Mind: The Secret of Human Thought Revealed .\\n1. Auﬂ. New York City, USA: Viking Penguin, 2012.\\n[LCB18] Y. LeCun, C. Cortes und C. J. C. Burges. The MNIST Database of Handwritten\\nDigits . 2018. U R L : http://yann.lecun.com/exdb/mnist/index.html (besucht\\nam 17. 04. 2018).\\n[LDS89] Y. LeCun, J. S. Denker und S. A. Solla. „Optimal Brain Damage“. In: Advance\\nin Neural Information Processing Systems 2 . Hrsg. von R. P . Lippmann, J. E.\\nMoody und D. S. Touretzky. San Francisco (CA), USA: Morgan Kaufmann,\\n1989, S. 598–605. U R L : http://papers.nips.cc/paper/250-optimal-brain-\\ndamage.pdf.\\n[LH88] M. Livingstone und D. Hubel. „Segregation of Form, Color, Movement, and\\nDepth: Anatomy, Physiology, and Perception“. In: Science 240 (1988), S. 740–\\n749. U R L : http://www.hms.harvard.edu/bss/neuro/bornlab/nb204/\\npapers/livingstone-hubel-segregation-science1988.pdf.\\n[Lam17] P . Lamblin. MILA and the Future of Theano . 28. Sep. 2017. U R L : https : / /\\ngroups.google.com/forum/#!topic/theano-users/7Poq8BZutbY (besucht\\nam 08. 03. 2018).\\n121Quellenverzeichnis\\n[LeC+98] Y. LeCun, L. Bottou, Y. Bengio und P . Haffner. „Gradient-Based Learning\\nApplied to Document Recognition“. In: Proceedings of the IEEE . Institute of\\nElectrical und Electronics Engineers, Inc., 1998. U R L : http://yann.lecun.\\ncom/exdb/publis/pdf/lecun-98.pdf.\\n[LeC86] Y. LeCun. „Learning Processes in an Asymmetric Threshold Network“. In:\\nDisordered Systems and Biological Organization . Hrsg. von E. Bienenstock, F.\\nFogelman-Soulié und G. Weisbuch. Berlin: Springer, 1986.\\n[Lee+11] H. Lee, R. Grosse, R. Ranganath und A. Y. Ng. „Unsupervised Learning of\\nHierarchical Representations with Convolutional Deep Belief Networks“. In:\\nCommunications of the ACM 54.10 (2011), S. 95–103. U R L : https://www.cs.\\nprinceton.edu/~rajeshr/papers/cacm2011-researchHighlights-convDBN.\\npdf.\\n[Ley12] A. Leyh. Convolutional Neural Network . 26. Sep. 2012. U R L : https://www.\\ndasgehirn.info/entdecken/meilensteine/den-rasen-mit-der-nagelschere-\\nschneiden (besucht am 21. 04. 2018).\\n[MP43] W. S. McCulloch und W. Pitts. „A Logical Calculus of the Ideas Immanent in\\nNervous Activity“. In: Bulletin of Mathematical Biophysics 5 (1943), S. 115–133.\\n[MP69] M. Minsky und S. Papert. Perceptrons . 1. Auﬂ. Cambridge (MA): MIT Press,\\n1969.\\n[Mad16] Mads00. Neural Pathway Diagram . 6. Juni 2016. U R L : https : / / commons .\\nwikimedia . org / wiki / File : Neural _ pathway _ diagram . svg (besucht am\\n21. 04. 2018).\\n[Mat18] Mathworks. Convolutional Neural Network . 2018. U R L : https://de.mathworks.\\ncom/discovery/convolutional-neural-network.html (besucht am 17. 04. 2018).\\n[Mer16] Mercyse. Three Type of Feedback in RNN . 4. März 2016. U R L : https://commons.\\nwikimedia.org/wiki/File:Neuronal-Networks-Feedback.png (besucht am\\n24. 04. 2018).\\n[Mou16] A. Moujahid. A Practical Introduction to Deep Learning with Caffe and Python .\\n26. Juni 2016. U R L : http://adilmoujahid.com/posts/2016/06/introduction-\\ndeep-learning-python-caffe/ (besucht am 28. 04. 2018).\\n[Mus17] E. Musk. Competition for AI . 4. Sep. 2017. U R L : https : / / twitter . com /\\nelonmusk/status/904638455761612800 (besucht am 01. 05. 2018).\\n[Mwk04] Mwka. Modell einer Spracherkennung nach Waibel . 12. Okt. 2004. U R L : https:\\n/ / commons . wikimedia . org / wiki / File : Spracherkennung - modell . png\\n(besucht am 28. 04. 2018).\\n[Net18] Preferred Networks. Chainer: A Powerful, Flexible, and Intuitive Framework for\\nNeural Networks . 2018. U R L : https://chainer.org (besucht am 08. 03. 2018).\\n[Ola15] C. Olah. Understanding LSTM Networks . 27. Aug. 2015. U R L : http://colah.\\ngithub.io/posts/2015-08-Understanding-LSTMs (besucht am 24. 04. 2018).\\n[Ooi+15] B. C. Ooi, K.-L. Tan, S. Wang, W. Wang, Q. Cai, G. Chen, J. Gao, Z. Luo, A. K.\\nH. Tung, Y. Wang, Z. Xie, M. Zhang und K. Zheng. „SINGA: A Distributed\\nDeep Learning Platform“. In: ACM Multimedia . 2015.\\n[PMB12] P . Pascanu, T. Mikolov und Y. Bengio. „On the Difﬁculty of Training Recur-\\nrent Neural Networks“. In: CoRR abs/1211.5063 (2012). U R L : https://arxiv.\\norg/abs/1211.5063.\\n122Quellenverzeichnis\\n[Par+18] A. Park, S. Leishman, A. Thomas, U. Köster u. a. Neon: Reference Deep Learning\\nFramework . 2018. U R L : https://github.com/NervanaSystems/neon (besucht\\nam 08. 03. 2018).\\n[Par85] D. Parker. Learning Logic . Techn. Ber. TR-87. MIT, Cambridge (MA): Center\\nfor Computational Research in Economics und Management Science, 1985.\\n[Pas+18] A. Paszke, S. Chintala, G. Chanan, S. Gross, E. Z. Yang, Z. DeVito, T. Killeen\\nu. a. PyTorch: Tensors and Dynamic Neural Networks in Python with Strong GPU\\nAcceleration . 2018. U R L : https://github.com/pytorch/pytorch (besucht am\\n08. 03. 2018).\\n[Pha+18] W. Phan, M. Stensmo, M. Dymczyk, A. Candel und Q. Kou. Deep Learning\\nwith Deep Water . Techn. Ber. Mountain View (CA), USA: H2O.ai, 2018. U R L :\\nhttp://docs.h2o.ai/h2o/latest-stable/h2o-docs/booklets/DeepWaterBooklet.\\npdf.\\n[PyT18] PyTorch. PyTorch: Tensors and Dynamic Neural Networks in Python with Strong\\nGPU Acceleration . 2018. U R L : http://pytorch.org (besucht am 08. 03. 2018).\\n[RB92] M. Riedmiller und H. Braun. „Rprop – A Fast Adaptive Learning Algorithm“.\\nIn:Proceedings of the 7th International Symposium of Computer and Information\\nScience (ISCIS) . Antalya, Turkey, 1992, S. 279–286.\\n[RB93] M. Riedmiller und H. Braun. „A Direct Adaptive Method for Faster Back-\\npropagation Learning: The RPROP Algorithm“. In: Proceedings of the IEEE\\nInternational Conference on Neural Networks . IEEE Press, 1993, S. 586–591. U R L :\\nhttp : / / www. cs . cmu . edu / afs / cs / user / bhiksha / WWW / courses /\\ndeeplearning/Fall.2016/pdfs/Rprop.pdf.\\n[RHW86a] D. E. Rummelhart, G. E. Hinton und R. J. Williams. „Learning Internal Re-\\npresentations by Error Propagation“. In: Parallel Distibuted Processing: Explo-\\nrations in the Microstructures of Cognition . Hrsg. von D. E. Rummelhardt und\\nJ. L. McCelland. Cambridge (MA): MIT Press, 1986.\\n[RHW86b] D. E. Rummelhart, G. E. Hinton und R. J. Williams. „Learning Representati-\\nons by Back-Propagating Errors“. In: Nature 323 (1986), S. 533–536.\\n[Raz12] Razorbliss. Hidden Markov Model with Output . 22. Jan. 2012. U R L : https://\\ncommons.wikimedia.org/wiki/File:HiddenMarkovModel.svg (besucht am\\n24. 04. 2018).\\n[Rea+18] E. Real, A. Aggarwal, Y. Huang und Q. V . Le. „Regularized Evolution for\\nImage Classiﬁer Architecture Search“. In: CoRR abs/1802.01548 (2018). U R L :\\nhttps://arxiv.org/abs/1802.01548.\\n[Red+18] J. Redmon u. a. Darknet: Convolutional Neural Networks . 2018. U R L : https :\\n//github.com/pjreddie/darknet (besucht am 08. 03. 2018).\\n[Red18] J. Redmon. Darknet: Open Source Neural Networks in C . 2018. U R L : https :\\n//pjreddie.com/darknet (besucht am 08. 03. 2018).\\n[Roc+18] S. Rochel u. a. Gluon: A Clear, Concise, Simple yet Powerful and Efﬁcient API\\nfor Deep Learning . 2018. U R L : https://github.com/gluon-api/gluon-api\\n(besucht am 08. 03. 2018).\\n[Ros58] F. Rosenblatt. „The Perceptron: A Probabilistic Model for Information Stora-\\nge and Organization in the Brain“. In: Psychological Review 65 (1958), S. 386–\\n408.\\n123Quellenverzeichnis\\n[Rud16] S. Ruder. „An Overview of Gradient Descent Optimization Algorithms“. In:\\nCoRR abs/1609.04747 (2016). U R L : https://arxiv.org/abs/1609.04747.\\n[Rus+15] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A.\\nKarpathy, A. Khosla, M. Bernstein, A. C. Berg und L. Fei-Fei. „ImageNet Lar-\\nge Scale Visual Recognition Challenge“. In: International Journal of Computer\\nVision (IJCV) 115.3 (2015), S. 211–252. U R L : https://arxiv.org/abs/1409.0575.\\n[SSP03] P . Y. Simard, D. Steinkraus und J. C. Platt. „Best Practices for Convolutional\\nNeural Networks Applied to Visual Document Analysis“. In: Proceedings\\nof the Seventh International Conference on Document Analysis and Recognition\\n(ICDAR 2003) . Institute of Electrical und Electronics Engineers, Inc., 2003.\\nU R L : https://www.microsoft.com/en- us/research/publication/best-\\npractices-for-convolutional-neural-networks-applied-to-visual-document-\\nanalysis/.\\n[Sch+18] J. Schlüter, S. Dieleman, C. Raffel, E. Olson u. a. Lasagne: Lightweight Library\\nto Build and Train Neural Networks in Theano . 2018. U R L : https://github.com/\\nLasagne/Lasagne (besucht am 08. 03. 2018).\\n[Sch14] J. Schmidhuber. Deep Learning in Neural Networks: An Overview . Techn. Ber.\\nIDSIA-03-14. Manno-Lugano, Switzerland: Istituto Dalle Molle di Studi sull’\\nIntelligenza Artiﬁciale, University of Lugano & SUPSI, 8. Okt. 2014. U R L :\\nhttps://arxiv.org/abs/1404.7828.\\n[Sch17] J. Schmidhuber. Our Impact on the World’s Most Valuable Public Companies .\\n17. Aug. 2017. U R L : http://people.idsia.ch/~juergen/impact-on-most-\\nvaluable-companies.html (besucht am 28. 04. 2018).\\n[Sei+18] F. Seide, W. Richert, M. Hillebrand, A. Agarwal, J. B. Faddoul, Z. Wang, W.\\nDarling u. a. Microsoft Cognitive Toolkit (CNTK) . 2018. U R L : https://github.\\ncom/Microsoft/CNTK (besucht am 08. 03. 2018).\\n[She+18] E. Shelhamer, J. Donahue, Y. Jia, J. Long, S. Guadarrama u. a. Caffe: A Fast\\nOpen Framework for Deep Learning . 2018. U R L : https://github.com/BVLC/\\ncaffe (besucht am 08. 03. 2018).\\n[Shi+15] X. Shi, Z. Chen, H. Wang, D.-Y. Yeung, W. Wong und W. Woo. „Convolutional\\nLSTM Network: A Machine Learning Approach for Precipitation Nowcas-\\nting“. In: CoRR abs/1506.04214 (2015). U R L : https://arxiv.org/abs/1506.\\n04214.\\n[Sou18] Facebook Open Source. Caffe2 – A New Lightweight, Modular, and Scalable Deep\\nLearning Framework . 2018. U R L : https://caffe2.ai (besucht am 08. 03. 2018).\\n[Ste+18] B. Steiner, S. Cai, V . Vasudevan, D. Murray, G. Gulsoy u. a. TensorFlow: Compu-\\ntation using Data Flow Graphs for Scalable Machine Learning . 2018. U R L : https:\\n//github.com/tensorﬂow/tensorﬂow (besucht am 08. 03. 2018).\\n[Ste17] J. Steppan. Sample Images from MNIST Test Dataset . 14. Dez. 2017. U R L : https:\\n//commons.wikimedia.org/wiki/File:MnistExamples.png (besucht am\\n17. 04. 2018).\\n[Ste93] R. Stein. „Selecting Data for Neural Networks“. In: AI Expert 2 (1993), S. 42–\\n47.\\n[Sys18] Nervana Systems. Neon . 2018. U R L : http://neon.nervanasys.com (besucht\\nam 08. 03. 2018).\\n124Quellenverzeichnis\\n[Sze+14] C. Szegedy, W. Liu, Y. Jia, P . Sermanet, S. Reed, D. Anguelov, D. Erhan, V .\\nVanhoucke und A. Rabinovich. „Going Deeper with Convolutions“. In: CoRR\\nabs/1409.4842 (2014). U R L : https://arxiv.org/abs/1409.4842.\\n[Ten18] TensorFlow. An Open-Source Software Library for Machine Intelligence . 2018.\\nU R L : https://www.tensorﬂow.org (besucht am 08. 03. 2018).\\n[Toe07] M. W. Toews. Normal Distribution Curve that illustrates Standard Deviations .\\n7. Apr. 2007. U R L : https://commons.wikimedia.org/wiki/File:Standard_\\ndeviation_diagram.svg (besucht am 28. 04. 2018).\\n[Tok+15] S. Tokui, K. Oono, S. Hido und J. Clayton. „Chainer: a Next-Generation Open\\nSource Framework for Deep Learning“. In: Proceedings of Workshop on Machi-\\nne Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on\\nNeural Information Processing Systems (NIPS) . 2015. U R L : http://learningsys.\\norg/papers/LearningSys_2015_paper_33.pdf.\\n[Unn+18] Y. Unno, S. Tokui, R. Okuta, M. Takagi, S. Saito u. a. Chainer: A Flexible Frame-\\nwork of Neural Networks for Deep Learning . 2018. U R L : https://github.com/\\nchainer/chainer (besucht am 08. 03. 2018).\\n[Vin+08] P . Vincent, H. Larochelle, Y. Bengio und P . A. Manzagol. „Extracting and\\nComposing Robust Features with Denoising Autoencoders“. In: Proceedings\\nof the Twenty-ﬁfth International Conference on Machine Learning (ICML‘08) . Hel-\\nsinki, Finland, 2008, S. 1096–1103. U R L : http : / / www. cs . toronto . edu /\\n~larocheh/publications/icml-2008-denoising-autoencoders.pdf.\\n[Vog16] W. Vogels. Bringing the Magic of Amazon AI and Alexa to Apps on AWS . 30. Nov.\\n2016. U R L : https://www.allthingsdistributed.com/2016/11/amazon-ai-\\nand-alexa-for-all-aws-apps.html (besucht am 28. 04. 2018).\\n[WH60] B. Widrow und M. E. Hoff. Adaptive Switching Circuits . 1. Auﬂ. New York:\\nIRE WESCON Convention Record, 1960.\\n[WL90] B. Widrow und M. A. Lehr. „30 Years of Adaptive Neural Networks: Percep-\\ntron, Madaline and Backpropagation“. In: Proceedings of the IEEE 78 (1990),\\nS. 1415–1441.\\n[WRH91] A. S. Weigend, D. E. Rumelhart und B. A. Huberman. „Generalization by\\nWeight-Elimination with Application to Forecasting“. In: Advances in Neural\\nInformation Processing Systems 3 . Hrsg. von R. P . Lippmann, J. E. Moody und\\nD. S. Touretzky. San Francisco (CA), USA: Morgan-Kaufmann, 1991, S. 875–\\n882. U R L : http://papers.nips.cc/paper/323-generalization-by-weight-\\nelimination-with-application-to-forecasting.pdf.\\n[WX+18] W. Wang, Z. Xie u. a. Apache Singa (Incubating) . 2018. U R L : https://github.\\ncom/apache/incubator-singa (besucht am 08. 03. 2018).\\n[Wan+15] W. Wang, G. Chen, T. T. A Dinh, J. Gao, B. C. Ooi, K.-L. Tan und S. Wang.\\n„SINGA: Putting Deep Learning in the Hands of Multimedia Users“. In: ACM\\nMultimedia . 2015.\\n[Wer74] P . J. Werbos. „Beyond Regression: New Tools for Prediction and Analysis in\\nthe Behavioral Sciences“. Diss. Harvard University, 1974.\\n[Wer88] P . J. Werbos. „Generalization of Backpropagation with Application to a Re-\\ncurrent Gas Market Model“. In: Neural Networks 1.4 (1988), S. 339–356.\\n[Wik18a] Wikipedia. AlphaGo gegen Lee Sedol . 2018. U R L : https://de.wikipedia.org/\\nwiki/AlphaGo_gegen_Lee_Sedol (besucht am 17. 01. 2018).\\n125Quellenverzeichnis\\n[Wik18b] Wikipedia. CIFAR-10 . 2018. U R L : https://en.wikipedia.org/wiki/CIFAR-10\\n(besucht am 17. 04. 2018).\\n[Wik18c] Wikipedia. Comparison of Deep Learning Software . 2018. U R L : https://en.\\nwikipedia.org/wiki/Comparison_of_deep_learning_software (besucht am\\n08. 03. 2018).\\n[Wik18d] Wikipedia. Git. 2018. U R L : https://de.wikipedia.org/wiki/Git (besucht am\\n08. 03. 2018).\\n[Wik18e] Wikipedia. GitHub . 2018. U R L : https://de.wikipedia.org/wiki/GitHub\\n(besucht am 08. 03. 2018).\\n[Wik18f] Wikipedia. Long Short-Term Memory . 2018. U R L : https://en.wikipedia.org/\\nwiki/Long_short-term_memory (besucht am 17. 04. 2018).\\n[Wik18g] Wikipedia. Loss Functions for Classiﬁcation . 2018. U R L : https://en.wikipedia.\\norg/wiki/Loss_functions_for_classiﬁcation (besucht am 28. 04. 2018).\\n[Wik18h] Wikipedia. MNIST Database . 2018. U R L : https://en.wikipedia.org/wiki/\\nMNIST_database (besucht am 17. 04. 2018).\\n[Wik18i] Wikipedia. Python . 2018. U R L : https://de.wikipedia.org/wiki/Python_\\n(Programmiersprache) (besucht am 08. 03. 2018).\\n[Wik18j] Wikipedia. Recurrent Neural Network . 2018. U R L : https://en.wikipedia.org/\\nwiki/Recurrent_neural_network (besucht am 17. 04. 2018).\\n[Wis18] Spektrum der Wissenschaft. Lexikon der Neurowissenschaft: Hypersäule . 2018.\\nU R L : https://www.spektrum.de/lexikon/neurowissenschaft/hypersaeule/\\n5808 (besucht am 21. 04. 2018).\\n[Xie+18] E. J. Xie, T. Chen, M. Li, B. Xu, C. Zhang, Y. Liu u. a. Apache MXNet (incubating)\\nfor Deep Learning . 2018. U R L : https://github.com/apache/incubator-mxnet\\n(besucht am 08. 03. 2018).\\n[Yan+18] Y. Yang, Q. Longfei, T. Luo, Q. Jun, J. Feng u. a. P Arallel Distributed Deep LEar-\\nning . 2018. U R L : https://github.com/PaddlePaddle/Paddle (besucht am\\n08. 03. 2018).\\n[ZHF92] H. G. Zimmermann, F. Herget und W. Finnoff. Neuron Pruning and Merging\\nMethods for Use in Conjunction with Weight Elimination . Techn. Ber. München:\\nSiemens AG, Corporate Research und Development, 1992.\\n126Kolophon\\nDieses Dokument wurde mit der L ATEX-Vorlage für Abschlussarbeiten an der htw saar der\\nFakultät für Wirtschaftswissenschaften im Bereich Wirtschaftsinformatik erstellt (Version\\n1.0). Die Vorlage wurde von Stefan Selle erstellt und basiert weitestgehend auf der entspre-\\nchenden Vorlage der Fakultät für Ingenieurswissenschaften, die von Yves Hary, André\\nMiede, Thomas Kretschmer, Helmut G. Folz und Martina Lehser entwickelt wurde.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Funktion aus Aufgabe 1 um PDF zu lesen\n",
    "def tf_read(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "        for page_num in range(num_pages):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def main():\n",
    "# text wird in liste mit corpus gespeichert\n",
    "    corpus = []\n",
    "\n",
    "# PDF files sammeln\n",
    "    pdf_files = [\"Einführung in Autoencoder und Convolutional Neural Networks.pdf\", \"Grundlagen_neuronaleNetzwerke.pdf\", \"Kuenstliche_neuronale_netzwerke.pdf\"]\n",
    "\n",
    "# Text extrahieren\n",
    "    for pdf_file in pdf_files:\n",
    "        text = tf_read(pdf_file)\n",
    "        corpus.append(text)\n",
    "    \n",
    "    print(corpus)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "#Nltk-Bibliothek verwenden\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "#tokenized_corpus = []\n",
    "\n",
    "#for text in corpus:\n",
    "#    tokens = word_tokenize(text)\n",
    "#    tokenized_corpus.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3298680a",
   "metadata": {},
   "source": [
    "### 3. Funktion idf(Wort, Korpus)\n",
    "Erstellen Sie eine Funktion idf(Wort, Korpus)\n",
    "\n",
    "Die Funktion nimt zwei Argumente entgegen Wort und Korpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd105d80",
   "metadata": {},
   "source": [
    "### 4. Erstellen Sie eine Funktion tf_idf(Wort, Dokument, Korpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d19a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
